{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dfply import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "   print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:\\\\ProjectData\\\\ERAU-REU\\\\Project-Drone-Behavior\\\\behavior-captures\\\\behavior-signal-multiple-processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot = df.plot(legend=False)\n",
    "#plot.figure.savefig(\"E:\\\\ProjectData\\\\ERAU-REU\\\\Project-Drone-Behavior\\\\plots\\\\allplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot = videoDf.plot(legend=False)\n",
    "#plot.figure.savefig(\"E:\\\\ProjectData\\\\ERAU-REU\\\\Project-Drone-Behavior\\\\plots\\\\allVideoPlot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df >>= mutate(drone_present = case_when([df.behavior == 'surround','yes'],\n",
    "[df.behavior == 'straight','yes'],\n",
    "[df.behavior == 'noise','no']))\n",
    "presence_labs = df['drone_present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = np.array(presence_labs)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = df.drop(['behavior','signal','multiple','drone_present'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(name='test')\n",
    "model.add(Dense(64, activation='relu', input_shape=(1280,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"test\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                81984     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 84,130\n",
      "Trainable params: 84,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(1280,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2048 samples, validate on 512 samples\n",
      "Epoch 1/100\n",
      "2048/2048 [==============================] - 1s 501us/sample - loss: 0.6902 - accuracy: 0.4712 - val_loss: 0.6842 - val_accuracy: 0.6113\n",
      "Epoch 2/100\n",
      "2048/2048 [==============================] - 0s 103us/sample - loss: 0.6787 - accuracy: 0.6206 - val_loss: 0.6741 - val_accuracy: 0.6260\n",
      "Epoch 3/100\n",
      "2048/2048 [==============================] - 0s 97us/sample - loss: 0.6694 - accuracy: 0.6252 - val_loss: 0.6649 - val_accuracy: 0.6289\n",
      "Epoch 4/100\n",
      "2048/2048 [==============================] - 0s 111us/sample - loss: 0.6605 - accuracy: 0.6262 - val_loss: 0.6555 - val_accuracy: 0.6289\n",
      "Epoch 5/100\n",
      "2048/2048 [==============================] - 0s 108us/sample - loss: 0.6505 - accuracy: 0.6262 - val_loss: 0.6442 - val_accuracy: 0.6289\n",
      "Epoch 6/100\n",
      "2048/2048 [==============================] - 0s 110us/sample - loss: 0.6399 - accuracy: 0.6265 - val_loss: 0.6340 - val_accuracy: 0.6289\n",
      "Epoch 7/100\n",
      "2048/2048 [==============================] - 0s 97us/sample - loss: 0.6304 - accuracy: 0.6267 - val_loss: 0.6247 - val_accuracy: 0.6289\n",
      "Epoch 8/100\n",
      "2048/2048 [==============================] - 0s 110us/sample - loss: 0.6213 - accuracy: 0.6267 - val_loss: 0.6159 - val_accuracy: 0.6289\n",
      "Epoch 9/100\n",
      "2048/2048 [==============================] - 0s 160us/sample - loss: 0.6126 - accuracy: 0.6267 - val_loss: 0.6075 - val_accuracy: 0.6289\n",
      "Epoch 10/100\n",
      "2048/2048 [==============================] - 0s 128us/sample - loss: 0.6043 - accuracy: 0.6267 - val_loss: 0.5996 - val_accuracy: 0.6289\n",
      "Epoch 11/100\n",
      "2048/2048 [==============================] - 0s 100us/sample - loss: 0.5966 - accuracy: 0.6270 - val_loss: 0.5923 - val_accuracy: 0.6289\n",
      "Epoch 12/100\n",
      "2048/2048 [==============================] - 0s 171us/sample - loss: 0.5893 - accuracy: 0.6270 - val_loss: 0.5854 - val_accuracy: 0.6289\n",
      "Epoch 13/100\n",
      "2048/2048 [==============================] - 0s 132us/sample - loss: 0.5824 - accuracy: 0.6267 - val_loss: 0.5788 - val_accuracy: 0.6289\n",
      "Epoch 14/100\n",
      "2048/2048 [==============================] - 0s 148us/sample - loss: 0.5757 - accuracy: 0.6270 - val_loss: 0.5723 - val_accuracy: 0.6289\n",
      "Epoch 15/100\n",
      "2048/2048 [==============================] - 0s 152us/sample - loss: 0.5692 - accuracy: 0.6270 - val_loss: 0.5662 - val_accuracy: 0.6289\n",
      "Epoch 16/100\n",
      "2048/2048 [==============================] - 0s 158us/sample - loss: 0.5631 - accuracy: 0.6409 - val_loss: 0.5604 - val_accuracy: 0.6855\n",
      "Epoch 17/100\n",
      "2048/2048 [==============================] - 0s 175us/sample - loss: 0.5574 - accuracy: 0.7336 - val_loss: 0.5549 - val_accuracy: 0.7861\n",
      "Epoch 18/100\n",
      "2048/2048 [==============================] - 0s 177us/sample - loss: 0.5518 - accuracy: 0.8130 - val_loss: 0.5497 - val_accuracy: 0.8525\n",
      "Epoch 19/100\n",
      "2048/2048 [==============================] - 0s 132us/sample - loss: 0.5466 - accuracy: 0.8591 - val_loss: 0.5447 - val_accuracy: 0.8623\n",
      "Epoch 20/100\n",
      "2048/2048 [==============================] - 0s 132us/sample - loss: 0.5419 - accuracy: 0.8667 - val_loss: 0.5403 - val_accuracy: 0.8721\n",
      "Epoch 21/100\n",
      "2048/2048 [==============================] - 0s 163us/sample - loss: 0.5375 - accuracy: 0.8711 - val_loss: 0.5362 - val_accuracy: 0.8760\n",
      "Epoch 22/100\n",
      "2048/2048 [==============================] - 0s 165us/sample - loss: 0.5335 - accuracy: 0.8733 - val_loss: 0.5323 - val_accuracy: 0.8760\n",
      "Epoch 23/100\n",
      "2048/2048 [==============================] - 0s 168us/sample - loss: 0.5297 - accuracy: 0.8743 - val_loss: 0.5286 - val_accuracy: 0.8779\n",
      "Epoch 24/100\n",
      "2048/2048 [==============================] - 0s 182us/sample - loss: 0.5261 - accuracy: 0.8750 - val_loss: 0.5248 - val_accuracy: 0.8789\n",
      "Epoch 25/100\n",
      "2048/2048 [==============================] - 0s 181us/sample - loss: 0.5224 - accuracy: 0.8752 - val_loss: 0.5212 - val_accuracy: 0.8789\n",
      "Epoch 26/100\n",
      "2048/2048 [==============================] - 0s 161us/sample - loss: 0.5186 - accuracy: 0.8752 - val_loss: 0.5176 - val_accuracy: 0.8789\n",
      "Epoch 27/100\n",
      "2048/2048 [==============================] - 0s 124us/sample - loss: 0.5151 - accuracy: 0.8755 - val_loss: 0.5140 - val_accuracy: 0.8789\n",
      "Epoch 28/100\n",
      "2048/2048 [==============================] - 0s 135us/sample - loss: 0.5115 - accuracy: 0.8755 - val_loss: 0.5104 - val_accuracy: 0.8789\n",
      "Epoch 29/100\n",
      "2048/2048 [==============================] - 0s 121us/sample - loss: 0.5081 - accuracy: 0.8755 - val_loss: 0.5068 - val_accuracy: 0.8789\n",
      "Epoch 30/100\n",
      "2048/2048 [==============================] - 0s 139us/sample - loss: 0.5043 - accuracy: 0.8755 - val_loss: 0.5032 - val_accuracy: 0.8789\n",
      "Epoch 31/100\n",
      "2048/2048 [==============================] - 0s 122us/sample - loss: 0.5007 - accuracy: 0.8755 - val_loss: 0.4994 - val_accuracy: 0.8789\n",
      "Epoch 32/100\n",
      "2048/2048 [==============================] - 0s 124us/sample - loss: 0.4969 - accuracy: 0.8755 - val_loss: 0.4957 - val_accuracy: 0.8789\n",
      "Epoch 33/100\n",
      "2048/2048 [==============================] - 0s 162us/sample - loss: 0.4932 - accuracy: 0.8755 - val_loss: 0.4920 - val_accuracy: 0.8789\n",
      "Epoch 34/100\n",
      "2048/2048 [==============================] - 0s 159us/sample - loss: 0.4894 - accuracy: 0.8755 - val_loss: 0.4881 - val_accuracy: 0.8789\n",
      "Epoch 35/100\n",
      "2048/2048 [==============================] - 0s 148us/sample - loss: 0.4855 - accuracy: 0.8755 - val_loss: 0.4842 - val_accuracy: 0.8789\n",
      "Epoch 36/100\n",
      "2048/2048 [==============================] - 0s 179us/sample - loss: 0.4816 - accuracy: 0.8755 - val_loss: 0.4802 - val_accuracy: 0.8789\n",
      "Epoch 37/100\n",
      "2048/2048 [==============================] - 0s 141us/sample - loss: 0.4776 - accuracy: 0.8755 - val_loss: 0.4760 - val_accuracy: 0.8789\n",
      "Epoch 38/100\n",
      "2048/2048 [==============================] - 0s 119us/sample - loss: 0.4736 - accuracy: 0.8755 - val_loss: 0.4719 - val_accuracy: 0.8789\n",
      "Epoch 39/100\n",
      "2048/2048 [==============================] - 0s 127us/sample - loss: 0.4695 - accuracy: 0.8755 - val_loss: 0.4677 - val_accuracy: 0.8789\n",
      "Epoch 40/100\n",
      "2048/2048 [==============================] - 0s 134us/sample - loss: 0.4654 - accuracy: 0.8755 - val_loss: 0.4635 - val_accuracy: 0.8789\n",
      "Epoch 41/100\n",
      "2048/2048 [==============================] - 0s 120us/sample - loss: 0.4613 - accuracy: 0.8755 - val_loss: 0.4594 - val_accuracy: 0.8789\n",
      "Epoch 42/100\n",
      "2048/2048 [==============================] - 0s 113us/sample - loss: 0.4572 - accuracy: 0.8755 - val_loss: 0.4552 - val_accuracy: 0.8789\n",
      "Epoch 43/100\n",
      "2048/2048 [==============================] - 0s 119us/sample - loss: 0.4529 - accuracy: 0.8755 - val_loss: 0.4510 - val_accuracy: 0.8789\n",
      "Epoch 44/100\n",
      "2048/2048 [==============================] - 0s 121us/sample - loss: 0.4488 - accuracy: 0.8755 - val_loss: 0.4468 - val_accuracy: 0.8789\n",
      "Epoch 45/100\n",
      "2048/2048 [==============================] - 0s 120us/sample - loss: 0.4446 - accuracy: 0.8755 - val_loss: 0.4426 - val_accuracy: 0.8789\n",
      "Epoch 46/100\n",
      "2048/2048 [==============================] - 0s 133us/sample - loss: 0.4406 - accuracy: 0.8755 - val_loss: 0.4384 - val_accuracy: 0.8789\n",
      "Epoch 47/100\n",
      "2048/2048 [==============================] - 0s 173us/sample - loss: 0.4364 - accuracy: 0.8755 - val_loss: 0.4343 - val_accuracy: 0.8789\n",
      "Epoch 48/100\n",
      "2048/2048 [==============================] - 0s 163us/sample - loss: 0.4326 - accuracy: 0.8755 - val_loss: 0.4301 - val_accuracy: 0.8789\n",
      "Epoch 49/100\n",
      "2048/2048 [==============================] - 0s 128us/sample - loss: 0.4284 - accuracy: 0.8755 - val_loss: 0.4260 - val_accuracy: 0.8789\n",
      "Epoch 50/100\n",
      "2048/2048 [==============================] - 0s 122us/sample - loss: 0.4245 - accuracy: 0.8755 - val_loss: 0.4220 - val_accuracy: 0.8789\n",
      "Epoch 51/100\n",
      "2048/2048 [==============================] - 0s 162us/sample - loss: 0.4206 - accuracy: 0.8755 - val_loss: 0.4180 - val_accuracy: 0.8789\n",
      "Epoch 52/100\n",
      "2048/2048 [==============================] - 0s 138us/sample - loss: 0.4167 - accuracy: 0.8755 - val_loss: 0.4141 - val_accuracy: 0.8789\n",
      "Epoch 53/100\n",
      "2048/2048 [==============================] - 0s 131us/sample - loss: 0.4129 - accuracy: 0.8752 - val_loss: 0.4103 - val_accuracy: 0.8789\n",
      "Epoch 54/100\n",
      "2048/2048 [==============================] - 0s 118us/sample - loss: 0.4091 - accuracy: 0.8755 - val_loss: 0.4065 - val_accuracy: 0.8789\n",
      "Epoch 55/100\n",
      "2048/2048 [==============================] - 0s 172us/sample - loss: 0.4055 - accuracy: 0.8752 - val_loss: 0.4028 - val_accuracy: 0.8789\n",
      "Epoch 56/100\n",
      "2048/2048 [==============================] - 0s 149us/sample - loss: 0.4020 - accuracy: 0.8752 - val_loss: 0.3992 - val_accuracy: 0.8789\n",
      "Epoch 57/100\n",
      "2048/2048 [==============================] - 0s 169us/sample - loss: 0.3984 - accuracy: 0.8752 - val_loss: 0.3957 - val_accuracy: 0.8789\n",
      "Epoch 58/100\n",
      "2048/2048 [==============================] - 0s 165us/sample - loss: 0.3950 - accuracy: 0.8755 - val_loss: 0.3923 - val_accuracy: 0.8789\n",
      "Epoch 59/100\n",
      "2048/2048 [==============================] - 0s 161us/sample - loss: 0.3917 - accuracy: 0.8755 - val_loss: 0.3891 - val_accuracy: 0.8789\n",
      "Epoch 60/100\n",
      "2048/2048 [==============================] - 0s 164us/sample - loss: 0.3886 - accuracy: 0.8752 - val_loss: 0.3860 - val_accuracy: 0.8789\n",
      "Epoch 61/100\n",
      "2048/2048 [==============================] - 0s 172us/sample - loss: 0.3853 - accuracy: 0.8752 - val_loss: 0.3831 - val_accuracy: 0.8789\n",
      "Epoch 62/100\n",
      "2048/2048 [==============================] - 0s 147us/sample - loss: 0.3824 - accuracy: 0.8752 - val_loss: 0.3803 - val_accuracy: 0.8789\n",
      "Epoch 63/100\n",
      "2048/2048 [==============================] - 0s 138us/sample - loss: 0.3795 - accuracy: 0.8755 - val_loss: 0.3777 - val_accuracy: 0.8789\n",
      "Epoch 64/100\n",
      "2048/2048 [==============================] - 0s 183us/sample - loss: 0.3767 - accuracy: 0.8752 - val_loss: 0.3752 - val_accuracy: 0.8789\n",
      "Epoch 65/100\n",
      "2048/2048 [==============================] - 0s 117us/sample - loss: 0.3741 - accuracy: 0.8752 - val_loss: 0.3730 - val_accuracy: 0.8789\n",
      "Epoch 66/100\n",
      "2048/2048 [==============================] - 0s 98us/sample - loss: 0.3719 - accuracy: 0.8757 - val_loss: 0.3707 - val_accuracy: 0.8789\n",
      "Epoch 67/100\n",
      "2048/2048 [==============================] - 0s 108us/sample - loss: 0.3694 - accuracy: 0.8757 - val_loss: 0.3686 - val_accuracy: 0.8789\n",
      "Epoch 68/100\n",
      "2048/2048 [==============================] - 0s 94us/sample - loss: 0.3673 - accuracy: 0.8755 - val_loss: 0.3666 - val_accuracy: 0.8779\n",
      "Epoch 69/100\n",
      "2048/2048 [==============================] - 0s 105us/sample - loss: 0.3650 - accuracy: 0.8755 - val_loss: 0.3648 - val_accuracy: 0.8779\n",
      "Epoch 70/100\n",
      "2048/2048 [==============================] - 0s 101us/sample - loss: 0.3631 - accuracy: 0.8755 - val_loss: 0.3631 - val_accuracy: 0.8779\n",
      "Epoch 71/100\n",
      "2048/2048 [==============================] - 0s 147us/sample - loss: 0.3610 - accuracy: 0.8755 - val_loss: 0.3616 - val_accuracy: 0.8779\n",
      "Epoch 72/100\n",
      "2048/2048 [==============================] - 0s 157us/sample - loss: 0.3596 - accuracy: 0.8757 - val_loss: 0.3598 - val_accuracy: 0.8779\n",
      "Epoch 73/100\n",
      "2048/2048 [==============================] - 0s 163us/sample - loss: 0.3576 - accuracy: 0.8760 - val_loss: 0.3583 - val_accuracy: 0.8779\n",
      "Epoch 74/100\n",
      "2048/2048 [==============================] - 0s 169us/sample - loss: 0.3559 - accuracy: 0.8757 - val_loss: 0.3569 - val_accuracy: 0.8779\n",
      "Epoch 75/100\n",
      "2048/2048 [==============================] - 0s 170us/sample - loss: 0.3545 - accuracy: 0.8757 - val_loss: 0.3557 - val_accuracy: 0.8760\n",
      "Epoch 76/100\n",
      "2048/2048 [==============================] - 0s 178us/sample - loss: 0.3528 - accuracy: 0.8762 - val_loss: 0.3544 - val_accuracy: 0.8760\n",
      "Epoch 77/100\n",
      "2048/2048 [==============================] - 0s 185us/sample - loss: 0.3514 - accuracy: 0.8757 - val_loss: 0.3532 - val_accuracy: 0.8760\n",
      "Epoch 78/100\n",
      "2048/2048 [==============================] - 0s 178us/sample - loss: 0.3501 - accuracy: 0.8762 - val_loss: 0.3521 - val_accuracy: 0.8760\n",
      "Epoch 79/100\n",
      "2048/2048 [==============================] - 0s 174us/sample - loss: 0.3485 - accuracy: 0.8760 - val_loss: 0.3508 - val_accuracy: 0.8779\n",
      "Epoch 80/100\n",
      "2048/2048 [==============================] - 0s 174us/sample - loss: 0.3475 - accuracy: 0.8762 - val_loss: 0.3498 - val_accuracy: 0.8770\n",
      "Epoch 81/100\n",
      "2048/2048 [==============================] - 0s 142us/sample - loss: 0.3460 - accuracy: 0.8765 - val_loss: 0.3493 - val_accuracy: 0.8760\n",
      "Epoch 82/100\n",
      "2048/2048 [==============================] - 0s 160us/sample - loss: 0.3448 - accuracy: 0.8762 - val_loss: 0.3479 - val_accuracy: 0.8760\n",
      "Epoch 83/100\n",
      "2048/2048 [==============================] - 0s 153us/sample - loss: 0.3438 - accuracy: 0.8762 - val_loss: 0.3470 - val_accuracy: 0.8760\n",
      "Epoch 84/100\n",
      "2048/2048 [==============================] - 0s 172us/sample - loss: 0.3426 - accuracy: 0.8765 - val_loss: 0.3466 - val_accuracy: 0.8760\n",
      "Epoch 85/100\n",
      "2048/2048 [==============================] - 0s 142us/sample - loss: 0.3414 - accuracy: 0.8767 - val_loss: 0.3453 - val_accuracy: 0.8770\n",
      "Epoch 86/100\n",
      "2048/2048 [==============================] - 0s 174us/sample - loss: 0.3407 - accuracy: 0.8765 - val_loss: 0.3449 - val_accuracy: 0.8760\n",
      "Epoch 87/100\n",
      "2048/2048 [==============================] - 0s 160us/sample - loss: 0.3396 - accuracy: 0.8767 - val_loss: 0.3440 - val_accuracy: 0.8760\n",
      "Epoch 88/100\n",
      "2048/2048 [==============================] - 0s 143us/sample - loss: 0.3383 - accuracy: 0.8765 - val_loss: 0.3429 - val_accuracy: 0.8779\n",
      "Epoch 89/100\n",
      "2048/2048 [==============================] - 0s 168us/sample - loss: 0.3375 - accuracy: 0.8770 - val_loss: 0.3430 - val_accuracy: 0.8740\n",
      "Epoch 90/100\n",
      "2048/2048 [==============================] - 0s 174us/sample - loss: 0.3367 - accuracy: 0.8770 - val_loss: 0.3423 - val_accuracy: 0.8750\n",
      "Epoch 91/100\n",
      "2048/2048 [==============================] - 0s 146us/sample - loss: 0.3359 - accuracy: 0.8770 - val_loss: 0.3412 - val_accuracy: 0.8770\n",
      "Epoch 92/100\n",
      "2048/2048 [==============================] - 0s 160us/sample - loss: 0.3349 - accuracy: 0.8770 - val_loss: 0.3404 - val_accuracy: 0.8779\n",
      "Epoch 93/100\n",
      "2048/2048 [==============================] - 0s 137us/sample - loss: 0.3342 - accuracy: 0.8770 - val_loss: 0.3404 - val_accuracy: 0.8770\n",
      "Epoch 94/100\n",
      "2048/2048 [==============================] - 0s 154us/sample - loss: 0.3336 - accuracy: 0.8772 - val_loss: 0.3391 - val_accuracy: 0.8779\n",
      "Epoch 95/100\n",
      "2048/2048 [==============================] - 0s 166us/sample - loss: 0.3323 - accuracy: 0.8770 - val_loss: 0.3387 - val_accuracy: 0.8779\n",
      "Epoch 96/100\n",
      "2048/2048 [==============================] - 0s 164us/sample - loss: 0.3316 - accuracy: 0.8772 - val_loss: 0.3389 - val_accuracy: 0.8760\n",
      "Epoch 97/100\n",
      "2048/2048 [==============================] - 0s 178us/sample - loss: 0.3311 - accuracy: 0.8767 - val_loss: 0.3377 - val_accuracy: 0.8779\n",
      "Epoch 98/100\n",
      "2048/2048 [==============================] - 0s 171us/sample - loss: 0.3302 - accuracy: 0.8777 - val_loss: 0.3383 - val_accuracy: 0.8730\n",
      "Epoch 99/100\n",
      "2048/2048 [==============================] - 0s 153us/sample - loss: 0.3298 - accuracy: 0.8772 - val_loss: 0.3371 - val_accuracy: 0.8779\n",
      "Epoch 100/100\n",
      "2048/2048 [==============================] - 0s 151us/sample - loss: 0.3291 - accuracy: 0.8774 - val_loss: 0.3368 - val_accuracy: 0.8779\n"
     ]
    }
   ],
   "source": [
    "fit1 = model.fit(x = x_train,\n",
    "        y = y_train, \n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        validation_split = .2,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(x_test)\n",
    "prediction_df = pd.DataFrame(prediction, columns=['prob_present','prob_not_present']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>present</th>\n",
       "      <th>not_present</th>\n",
       "      <th>prob_present</th>\n",
       "      <th>prob_not_present</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.969571</td>\n",
       "      <td>0.027838</td>\n",
       "      <td>present</td>\n",
       "      <td>present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.963166</td>\n",
       "      <td>0.034037</td>\n",
       "      <td>present</td>\n",
       "      <td>present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.196372</td>\n",
       "      <td>0.815164</td>\n",
       "      <td>not_present</td>\n",
       "      <td>not_present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124108</td>\n",
       "      <td>0.845129</td>\n",
       "      <td>present</td>\n",
       "      <td>not_present</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.967007</td>\n",
       "      <td>0.030316</td>\n",
       "      <td>present</td>\n",
       "      <td>present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.169884</td>\n",
       "      <td>0.832999</td>\n",
       "      <td>not_present</td>\n",
       "      <td>not_present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.172048</td>\n",
       "      <td>0.843130</td>\n",
       "      <td>not_present</td>\n",
       "      <td>not_present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182468</td>\n",
       "      <td>0.822879</td>\n",
       "      <td>present</td>\n",
       "      <td>not_present</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.197012</td>\n",
       "      <td>0.807713</td>\n",
       "      <td>not_present</td>\n",
       "      <td>not_present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.173204</td>\n",
       "      <td>0.836918</td>\n",
       "      <td>not_present</td>\n",
       "      <td>not_present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     present  not_present  prob_present  prob_not_present       actual  \\\n",
       "0        1.0          0.0      0.969571          0.027838      present   \n",
       "1        1.0          0.0      0.963166          0.034037      present   \n",
       "2        0.0          1.0      0.196372          0.815164  not_present   \n",
       "3        1.0          0.0      0.124108          0.845129      present   \n",
       "4        1.0          0.0      0.967007          0.030316      present   \n",
       "..       ...          ...           ...               ...          ...   \n",
       "635      0.0          1.0      0.169884          0.832999  not_present   \n",
       "636      0.0          1.0      0.172048          0.843130  not_present   \n",
       "637      1.0          0.0      0.182468          0.822879      present   \n",
       "638      0.0          1.0      0.197012          0.807713  not_present   \n",
       "639      0.0          1.0      0.173204          0.836918  not_present   \n",
       "\n",
       "       predicted correct  \n",
       "0        present     yes  \n",
       "1        present     yes  \n",
       "2    not_present     yes  \n",
       "3    not_present      no  \n",
       "4        present     yes  \n",
       "..           ...     ...  \n",
       "635  not_present     yes  \n",
       "636  not_present     yes  \n",
       "637  not_present      no  \n",
       "638  not_present     yes  \n",
       "639  not_present     yes  \n",
       "\n",
       "[640 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = pd.DataFrame(y_test,columns=['present','not_present'])\n",
    "\n",
    "y_pred = y_test >> bind_cols(prediction_df)\n",
    "\n",
    "y_pred >>= mutate(actual = case_when([y_pred.present == 1, 'present'],\n",
    "[y_pred.not_present == 1, 'not_present']),\n",
    "predicted = case_when([y_pred.prob_present > y_pred.prob_not_present, 'present'],\n",
    "[y_pred.prob_present < y_pred.prob_not_present, 'not_present']))\n",
    "\n",
    "y_pred >>= mutate(correct = case_when([y_pred.actual == y_pred.predicted, 'yes'],\n",
    "[y_pred.actual != y_pred.predicted, 'no']))\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"test\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 18,594\n",
      "Trainable params: 18,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2048 samples, validate on 512 samples\n",
      "Epoch 1/100\n",
      "2048/2048 [==============================] - 1s 443us/sample - loss: 0.6844 - accuracy: 0.5554 - val_loss: 0.6807 - val_accuracy: 0.5928\n",
      "Epoch 2/100\n",
      "2048/2048 [==============================] - 0s 117us/sample - loss: 0.6751 - accuracy: 0.5994 - val_loss: 0.6739 - val_accuracy: 0.5938\n",
      "Epoch 3/100\n",
      "2048/2048 [==============================] - 0s 146us/sample - loss: 0.6686 - accuracy: 0.6001 - val_loss: 0.6692 - val_accuracy: 0.5938\n",
      "Epoch 4/100\n",
      "2048/2048 [==============================] - 0s 103us/sample - loss: 0.6637 - accuracy: 0.6001 - val_loss: 0.6656 - val_accuracy: 0.5938\n",
      "Epoch 5/100\n",
      "2048/2048 [==============================] - 0s 95us/sample - loss: 0.6597 - accuracy: 0.6001 - val_loss: 0.6628 - val_accuracy: 0.5938\n",
      "Epoch 6/100\n",
      "2048/2048 [==============================] - 0s 131us/sample - loss: 0.6563 - accuracy: 0.6001 - val_loss: 0.6605 - val_accuracy: 0.5938\n",
      "Epoch 7/100\n",
      "2048/2048 [==============================] - 0s 93us/sample - loss: 0.6533 - accuracy: 0.6001 - val_loss: 0.6584 - val_accuracy: 0.5938\n",
      "Epoch 8/100\n",
      "2048/2048 [==============================] - 0s 122us/sample - loss: 0.6507 - accuracy: 0.6001 - val_loss: 0.6567 - val_accuracy: 0.5938\n",
      "Epoch 9/100\n",
      "2048/2048 [==============================] - 0s 113us/sample - loss: 0.6484 - accuracy: 0.6001 - val_loss: 0.6552 - val_accuracy: 0.5938\n",
      "Epoch 10/100\n",
      "2048/2048 [==============================] - 0s 92us/sample - loss: 0.6462 - accuracy: 0.6001 - val_loss: 0.6538 - val_accuracy: 0.5938\n",
      "Epoch 11/100\n",
      "2048/2048 [==============================] - 0s 109us/sample - loss: 0.6441 - accuracy: 0.6001 - val_loss: 0.6525 - val_accuracy: 0.5938\n",
      "Epoch 12/100\n",
      "2048/2048 [==============================] - 0s 143us/sample - loss: 0.6421 - accuracy: 0.6001 - val_loss: 0.6512 - val_accuracy: 0.5938\n",
      "Epoch 13/100\n",
      "2048/2048 [==============================] - 0s 109us/sample - loss: 0.6401 - accuracy: 0.6001 - val_loss: 0.6499 - val_accuracy: 0.5938\n",
      "Epoch 14/100\n",
      "2048/2048 [==============================] - 0s 93us/sample - loss: 0.6382 - accuracy: 0.6001 - val_loss: 0.6487 - val_accuracy: 0.5938\n",
      "Epoch 15/100\n",
      "2048/2048 [==============================] - 0s 136us/sample - loss: 0.6361 - accuracy: 0.6001 - val_loss: 0.6473 - val_accuracy: 0.5938\n",
      "Epoch 16/100\n",
      "2048/2048 [==============================] - 0s 102us/sample - loss: 0.6340 - accuracy: 0.6001 - val_loss: 0.6459 - val_accuracy: 0.5938\n",
      "Epoch 17/100\n",
      "2048/2048 [==============================] - 0s 114us/sample - loss: 0.6318 - accuracy: 0.6001 - val_loss: 0.6445 - val_accuracy: 0.5938\n",
      "Epoch 18/100\n",
      "2048/2048 [==============================] - 0s 120us/sample - loss: 0.6297 - accuracy: 0.6001 - val_loss: 0.6431 - val_accuracy: 0.5938\n",
      "Epoch 19/100\n",
      "2048/2048 [==============================] - 0s 104us/sample - loss: 0.6274 - accuracy: 0.6001 - val_loss: 0.6418 - val_accuracy: 0.5938\n",
      "Epoch 20/100\n",
      "2048/2048 [==============================] - 0s 99us/sample - loss: 0.6253 - accuracy: 0.6001 - val_loss: 0.6405 - val_accuracy: 0.5938\n",
      "Epoch 21/100\n",
      "2048/2048 [==============================] - 0s 152us/sample - loss: 0.6230 - accuracy: 0.6001 - val_loss: 0.6390 - val_accuracy: 0.5938\n",
      "Epoch 22/100\n",
      "2048/2048 [==============================] - 0s 99us/sample - loss: 0.6208 - accuracy: 0.6001 - val_loss: 0.6376 - val_accuracy: 0.5938\n",
      "Epoch 23/100\n",
      "2048/2048 [==============================] - 0s 98us/sample - loss: 0.6185 - accuracy: 0.6001 - val_loss: 0.6362 - val_accuracy: 0.5938\n",
      "Epoch 24/100\n",
      "2048/2048 [==============================] - 0s 101us/sample - loss: 0.6161 - accuracy: 0.6001 - val_loss: 0.6347 - val_accuracy: 0.5938\n",
      "Epoch 25/100\n",
      "2048/2048 [==============================] - 0s 93us/sample - loss: 0.6138 - accuracy: 0.6001 - val_loss: 0.6334 - val_accuracy: 0.5938\n",
      "Epoch 26/100\n",
      "2048/2048 [==============================] - 0s 104us/sample - loss: 0.6114 - accuracy: 0.6001 - val_loss: 0.6319 - val_accuracy: 0.5938\n",
      "Epoch 27/100\n",
      "2048/2048 [==============================] - 0s 102us/sample - loss: 0.6090 - accuracy: 0.6001 - val_loss: 0.6304 - val_accuracy: 0.5938\n",
      "Epoch 28/100\n",
      "2048/2048 [==============================] - 0s 93us/sample - loss: 0.6065 - accuracy: 0.6006 - val_loss: 0.6290 - val_accuracy: 0.5938\n",
      "Epoch 29/100\n",
      "2048/2048 [==============================] - 0s 97us/sample - loss: 0.6042 - accuracy: 0.6194 - val_loss: 0.6277 - val_accuracy: 0.6230\n",
      "Epoch 30/100\n",
      "2048/2048 [==============================] - 0s 103us/sample - loss: 0.6019 - accuracy: 0.6536 - val_loss: 0.6265 - val_accuracy: 0.7041\n",
      "Epoch 31/100\n",
      "2048/2048 [==============================] - 0s 97us/sample - loss: 0.5997 - accuracy: 0.7583 - val_loss: 0.6255 - val_accuracy: 0.7500\n",
      "Epoch 32/100\n",
      "2048/2048 [==============================] - 0s 103us/sample - loss: 0.5977 - accuracy: 0.7913 - val_loss: 0.6244 - val_accuracy: 0.8027\n",
      "Epoch 33/100\n",
      "2048/2048 [==============================] - 0s 90us/sample - loss: 0.5957 - accuracy: 0.8237 - val_loss: 0.6235 - val_accuracy: 0.8271\n",
      "Epoch 34/100\n",
      "2048/2048 [==============================] - 0s 99us/sample - loss: 0.5938 - accuracy: 0.8494 - val_loss: 0.6226 - val_accuracy: 0.8369\n",
      "Epoch 35/100\n",
      "2048/2048 [==============================] - 0s 100us/sample - loss: 0.5920 - accuracy: 0.8574 - val_loss: 0.6217 - val_accuracy: 0.8447\n",
      "Epoch 36/100\n",
      "2048/2048 [==============================] - 0s 152us/sample - loss: 0.5902 - accuracy: 0.8652 - val_loss: 0.6209 - val_accuracy: 0.8477\n",
      "Epoch 37/100\n",
      "2048/2048 [==============================] - 0s 155us/sample - loss: 0.5886 - accuracy: 0.8723 - val_loss: 0.6200 - val_accuracy: 0.8516\n",
      "Epoch 38/100\n",
      "2048/2048 [==============================] - 0s 100us/sample - loss: 0.5869 - accuracy: 0.8743 - val_loss: 0.6192 - val_accuracy: 0.8525\n",
      "Epoch 39/100\n",
      "2048/2048 [==============================] - 0s 92us/sample - loss: 0.5852 - accuracy: 0.8774 - val_loss: 0.6184 - val_accuracy: 0.8525\n",
      "Epoch 40/100\n",
      "2048/2048 [==============================] - 0s 153us/sample - loss: 0.5837 - accuracy: 0.8782 - val_loss: 0.6175 - val_accuracy: 0.8535\n",
      "Epoch 41/100\n",
      "2048/2048 [==============================] - 0s 102us/sample - loss: 0.5821 - accuracy: 0.8801 - val_loss: 0.6167 - val_accuracy: 0.8535\n",
      "Epoch 42/100\n",
      "2048/2048 [==============================] - 0s 105us/sample - loss: 0.5805 - accuracy: 0.8804 - val_loss: 0.6159 - val_accuracy: 0.8535\n",
      "Epoch 43/100\n",
      "2048/2048 [==============================] - 0s 99us/sample - loss: 0.5789 - accuracy: 0.8804 - val_loss: 0.6149 - val_accuracy: 0.8555\n",
      "Epoch 44/100\n",
      "2048/2048 [==============================] - 0s 117us/sample - loss: 0.5775 - accuracy: 0.8804 - val_loss: 0.6141 - val_accuracy: 0.8555\n",
      "Epoch 45/100\n",
      "2048/2048 [==============================] - 0s 110us/sample - loss: 0.5759 - accuracy: 0.8804 - val_loss: 0.6133 - val_accuracy: 0.8555\n",
      "Epoch 46/100\n",
      "2048/2048 [==============================] - 0s 90us/sample - loss: 0.5744 - accuracy: 0.8804 - val_loss: 0.6122 - val_accuracy: 0.8555\n",
      "Epoch 47/100\n",
      "2048/2048 [==============================] - 0s 100us/sample - loss: 0.5729 - accuracy: 0.8804 - val_loss: 0.6114 - val_accuracy: 0.8555\n",
      "Epoch 48/100\n",
      "2048/2048 [==============================] - 0s 98us/sample - loss: 0.5714 - accuracy: 0.8804 - val_loss: 0.6103 - val_accuracy: 0.8555\n",
      "Epoch 49/100\n",
      "2048/2048 [==============================] - 0s 96us/sample - loss: 0.5699 - accuracy: 0.8804 - val_loss: 0.6094 - val_accuracy: 0.8555\n",
      "Epoch 50/100\n",
      "2048/2048 [==============================] - 0s 113us/sample - loss: 0.5684 - accuracy: 0.8804 - val_loss: 0.6082 - val_accuracy: 0.8555\n",
      "Epoch 51/100\n",
      "2048/2048 [==============================] - 0s 97us/sample - loss: 0.5668 - accuracy: 0.8804 - val_loss: 0.6072 - val_accuracy: 0.8555\n",
      "Epoch 52/100\n",
      "2048/2048 [==============================] - 0s 93us/sample - loss: 0.5653 - accuracy: 0.8804 - val_loss: 0.6061 - val_accuracy: 0.8555\n",
      "Epoch 53/100\n",
      "2048/2048 [==============================] - 0s 102us/sample - loss: 0.5637 - accuracy: 0.8804 - val_loss: 0.6048 - val_accuracy: 0.8555\n",
      "Epoch 54/100\n",
      "2048/2048 [==============================] - 0s 102us/sample - loss: 0.5622 - accuracy: 0.8804 - val_loss: 0.6036 - val_accuracy: 0.8555\n",
      "Epoch 55/100\n",
      "2048/2048 [==============================] - 0s 101us/sample - loss: 0.5604 - accuracy: 0.8804 - val_loss: 0.6024 - val_accuracy: 0.8555\n",
      "Epoch 56/100\n",
      "2048/2048 [==============================] - 0s 98us/sample - loss: 0.5589 - accuracy: 0.8804 - val_loss: 0.6006 - val_accuracy: 0.8555\n",
      "Epoch 57/100\n",
      "2048/2048 [==============================] - 0s 101us/sample - loss: 0.5572 - accuracy: 0.8804 - val_loss: 0.5992 - val_accuracy: 0.8555\n",
      "Epoch 58/100\n",
      "2048/2048 [==============================] - 0s 89us/sample - loss: 0.5555 - accuracy: 0.8804 - val_loss: 0.5978 - val_accuracy: 0.8555\n",
      "Epoch 59/100\n",
      "2048/2048 [==============================] - 0s 106us/sample - loss: 0.5537 - accuracy: 0.8804 - val_loss: 0.5962 - val_accuracy: 0.8555\n",
      "Epoch 60/100\n",
      "2048/2048 [==============================] - 0s 93us/sample - loss: 0.5518 - accuracy: 0.8804 - val_loss: 0.5945 - val_accuracy: 0.8555\n",
      "Epoch 61/100\n",
      "2048/2048 [==============================] - 0s 93us/sample - loss: 0.5500 - accuracy: 0.8804 - val_loss: 0.5930 - val_accuracy: 0.8555\n",
      "Epoch 62/100\n",
      "2048/2048 [==============================] - 0s 102us/sample - loss: 0.5480 - accuracy: 0.8804 - val_loss: 0.5911 - val_accuracy: 0.8555\n",
      "Epoch 63/100\n",
      "2048/2048 [==============================] - 0s 112us/sample - loss: 0.5462 - accuracy: 0.8804 - val_loss: 0.5894 - val_accuracy: 0.8555\n",
      "Epoch 64/100\n",
      "2048/2048 [==============================] - 0s 105us/sample - loss: 0.5443 - accuracy: 0.8804 - val_loss: 0.5877 - val_accuracy: 0.8555\n",
      "Epoch 65/100\n",
      "2048/2048 [==============================] - 0s 164us/sample - loss: 0.5423 - accuracy: 0.8804 - val_loss: 0.5856 - val_accuracy: 0.8555\n",
      "Epoch 66/100\n",
      "2048/2048 [==============================] - 0s 111us/sample - loss: 0.5402 - accuracy: 0.8804 - val_loss: 0.5837 - val_accuracy: 0.8555\n",
      "Epoch 67/100\n",
      "2048/2048 [==============================] - 0s 100us/sample - loss: 0.5381 - accuracy: 0.8804 - val_loss: 0.5818 - val_accuracy: 0.8555\n",
      "Epoch 68/100\n",
      "2048/2048 [==============================] - 0s 124us/sample - loss: 0.5360 - accuracy: 0.8804 - val_loss: 0.5797 - val_accuracy: 0.8555\n",
      "Epoch 69/100\n",
      "2048/2048 [==============================] - 0s 107us/sample - loss: 0.5337 - accuracy: 0.8804 - val_loss: 0.5774 - val_accuracy: 0.8555\n",
      "Epoch 70/100\n",
      "2048/2048 [==============================] - 0s 90us/sample - loss: 0.5314 - accuracy: 0.8804 - val_loss: 0.5749 - val_accuracy: 0.8555\n",
      "Epoch 71/100\n",
      "2048/2048 [==============================] - 0s 125us/sample - loss: 0.5290 - accuracy: 0.8804 - val_loss: 0.5725 - val_accuracy: 0.8555\n",
      "Epoch 72/100\n",
      "2048/2048 [==============================] - 0s 116us/sample - loss: 0.5266 - accuracy: 0.8804 - val_loss: 0.5699 - val_accuracy: 0.8555\n",
      "Epoch 73/100\n",
      "2048/2048 [==============================] - 0s 103us/sample - loss: 0.5239 - accuracy: 0.8804 - val_loss: 0.5670 - val_accuracy: 0.8555\n",
      "Epoch 74/100\n",
      "2048/2048 [==============================] - 0s 101us/sample - loss: 0.5211 - accuracy: 0.8804 - val_loss: 0.5641 - val_accuracy: 0.8555\n",
      "Epoch 75/100\n",
      "2048/2048 [==============================] - 0s 147us/sample - loss: 0.5186 - accuracy: 0.8804 - val_loss: 0.5612 - val_accuracy: 0.8555\n",
      "Epoch 76/100\n",
      "2048/2048 [==============================] - 0s 116us/sample - loss: 0.5157 - accuracy: 0.8804 - val_loss: 0.5584 - val_accuracy: 0.8555\n",
      "Epoch 77/100\n",
      "2048/2048 [==============================] - 0s 110us/sample - loss: 0.5128 - accuracy: 0.8804 - val_loss: 0.5549 - val_accuracy: 0.8555\n",
      "Epoch 78/100\n",
      "2048/2048 [==============================] - 0s 90us/sample - loss: 0.5098 - accuracy: 0.8804 - val_loss: 0.5518 - val_accuracy: 0.8555\n",
      "Epoch 79/100\n",
      "2048/2048 [==============================] - 0s 99us/sample - loss: 0.5067 - accuracy: 0.8804 - val_loss: 0.5487 - val_accuracy: 0.8555\n",
      "Epoch 80/100\n",
      "2048/2048 [==============================] - 0s 105us/sample - loss: 0.5036 - accuracy: 0.8804 - val_loss: 0.5455 - val_accuracy: 0.8555\n",
      "Epoch 81/100\n",
      "2048/2048 [==============================] - 0s 143us/sample - loss: 0.5003 - accuracy: 0.8804 - val_loss: 0.5418 - val_accuracy: 0.8555\n",
      "Epoch 82/100\n",
      "2048/2048 [==============================] - 0s 110us/sample - loss: 0.4969 - accuracy: 0.8804 - val_loss: 0.5387 - val_accuracy: 0.8555\n",
      "Epoch 83/100\n",
      "2048/2048 [==============================] - 0s 93us/sample - loss: 0.4936 - accuracy: 0.8804 - val_loss: 0.5348 - val_accuracy: 0.8555\n",
      "Epoch 84/100\n",
      "2048/2048 [==============================] - 0s 107us/sample - loss: 0.4899 - accuracy: 0.8804 - val_loss: 0.5305 - val_accuracy: 0.8555\n",
      "Epoch 85/100\n",
      "2048/2048 [==============================] - 0s 145us/sample - loss: 0.4864 - accuracy: 0.8804 - val_loss: 0.5264 - val_accuracy: 0.8555\n",
      "Epoch 86/100\n",
      "2048/2048 [==============================] - 0s 113us/sample - loss: 0.4828 - accuracy: 0.8804 - val_loss: 0.5222 - val_accuracy: 0.8555\n",
      "Epoch 87/100\n",
      "2048/2048 [==============================] - 0s 98us/sample - loss: 0.4787 - accuracy: 0.8804 - val_loss: 0.5183 - val_accuracy: 0.8555\n",
      "Epoch 88/100\n",
      "2048/2048 [==============================] - 0s 94us/sample - loss: 0.4751 - accuracy: 0.8804 - val_loss: 0.5142 - val_accuracy: 0.8555\n",
      "Epoch 89/100\n",
      "2048/2048 [==============================] - 0s 112us/sample - loss: 0.4711 - accuracy: 0.8804 - val_loss: 0.5099 - val_accuracy: 0.8555\n",
      "Epoch 90/100\n",
      "2048/2048 [==============================] - 0s 128us/sample - loss: 0.4670 - accuracy: 0.8804 - val_loss: 0.5047 - val_accuracy: 0.8555\n",
      "Epoch 91/100\n",
      "2048/2048 [==============================] - 0s 99us/sample - loss: 0.4630 - accuracy: 0.8804 - val_loss: 0.5006 - val_accuracy: 0.8555\n",
      "Epoch 92/100\n",
      "2048/2048 [==============================] - 0s 99us/sample - loss: 0.4587 - accuracy: 0.8804 - val_loss: 0.4961 - val_accuracy: 0.8555\n",
      "Epoch 93/100\n",
      "2048/2048 [==============================] - 0s 91us/sample - loss: 0.4545 - accuracy: 0.8804 - val_loss: 0.4912 - val_accuracy: 0.8555\n",
      "Epoch 94/100\n",
      "2048/2048 [==============================] - 0s 97us/sample - loss: 0.4500 - accuracy: 0.8804 - val_loss: 0.4872 - val_accuracy: 0.8555\n",
      "Epoch 95/100\n",
      "2048/2048 [==============================] - 0s 109us/sample - loss: 0.4459 - accuracy: 0.8804 - val_loss: 0.4820 - val_accuracy: 0.8555\n",
      "Epoch 96/100\n",
      "2048/2048 [==============================] - 0s 130us/sample - loss: 0.4414 - accuracy: 0.8804 - val_loss: 0.4773 - val_accuracy: 0.8555\n",
      "Epoch 97/100\n",
      "2048/2048 [==============================] - 0s 119us/sample - loss: 0.4373 - accuracy: 0.8804 - val_loss: 0.4723 - val_accuracy: 0.8555\n",
      "Epoch 98/100\n",
      "2048/2048 [==============================] - 0s 102us/sample - loss: 0.4327 - accuracy: 0.8804 - val_loss: 0.4672 - val_accuracy: 0.8555\n",
      "Epoch 99/100\n",
      "2048/2048 [==============================] - 0s 93us/sample - loss: 0.4285 - accuracy: 0.8804 - val_loss: 0.4626 - val_accuracy: 0.8555\n",
      "Epoch 100/100\n",
      "2048/2048 [==============================] - 0s 136us/sample - loss: 0.4244 - accuracy: 0.8804 - val_loss: 0.4583 - val_accuracy: 0.8555\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>present</th>\n",
       "      <th>not_present</th>\n",
       "      <th>prob_present</th>\n",
       "      <th>prob_not_present</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.275959</td>\n",
       "      <td>0.729775</td>\n",
       "      <td>not_present</td>\n",
       "      <td>not_present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.805861</td>\n",
       "      <td>0.212950</td>\n",
       "      <td>present</td>\n",
       "      <td>present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>0.218235</td>\n",
       "      <td>present</td>\n",
       "      <td>present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.198758</td>\n",
       "      <td>0.792078</td>\n",
       "      <td>not_present</td>\n",
       "      <td>not_present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.811502</td>\n",
       "      <td>0.209422</td>\n",
       "      <td>present</td>\n",
       "      <td>present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.305182</td>\n",
       "      <td>0.706242</td>\n",
       "      <td>not_present</td>\n",
       "      <td>not_present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792277</td>\n",
       "      <td>0.228555</td>\n",
       "      <td>present</td>\n",
       "      <td>present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.795923</td>\n",
       "      <td>0.224302</td>\n",
       "      <td>present</td>\n",
       "      <td>present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.782478</td>\n",
       "      <td>0.239123</td>\n",
       "      <td>present</td>\n",
       "      <td>present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.190934</td>\n",
       "      <td>0.819620</td>\n",
       "      <td>not_present</td>\n",
       "      <td>not_present</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     present  not_present  prob_present  prob_not_present       actual  \\\n",
       "0        0.0          1.0      0.275959          0.729775  not_present   \n",
       "1        1.0          0.0      0.805861          0.212950      present   \n",
       "2        1.0          0.0      0.802632          0.218235      present   \n",
       "3        0.0          1.0      0.198758          0.792078  not_present   \n",
       "4        1.0          0.0      0.811502          0.209422      present   \n",
       "..       ...          ...           ...               ...          ...   \n",
       "635      0.0          1.0      0.305182          0.706242  not_present   \n",
       "636      1.0          0.0      0.792277          0.228555      present   \n",
       "637      1.0          0.0      0.795923          0.224302      present   \n",
       "638      1.0          0.0      0.782478          0.239123      present   \n",
       "639      0.0          1.0      0.190934          0.819620  not_present   \n",
       "\n",
       "       predicted correct  \n",
       "0    not_present     yes  \n",
       "1        present     yes  \n",
       "2        present     yes  \n",
       "3    not_present     yes  \n",
       "4        present     yes  \n",
       "..           ...     ...  \n",
       "635  not_present     yes  \n",
       "636      present     yes  \n",
       "637      present     yes  \n",
       "638      present     yes  \n",
       "639  not_present     yes  \n",
       "\n",
       "[640 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_df = df[df.columns[0:256]]\n",
    "small_df >>= bind_cols(df.drone_present)\n",
    "\n",
    "presence_labs = small_df['drone_present']\n",
    "\n",
    "values = np.array(presence_labs)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "x = small_df.drop(['drone_present'],axis=1).values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "model = keras.Sequential(name='test')\n",
    "model.add(Dense(64, activation='relu', input_shape=(256,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(256,))\n",
    "model.summary()\n",
    "\n",
    "fit1 = model.fit(x = x_train,\n",
    "        y = y_train, \n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        validation_split = .2,\n",
    "        verbose=True)\n",
    "\n",
    "prediction = model.predict(x_test)\n",
    "prediction_df = pd.DataFrame(prediction, columns=['prob_present','prob_not_present']) \n",
    "\n",
    "y_test = pd.DataFrame(y_test,columns=['present','not_present'])\n",
    "\n",
    "y_pred = y_test >> bind_cols(prediction_df)\n",
    "\n",
    "y_pred >>= mutate(actual = case_when([y_pred.present == 1, 'present'],\n",
    "[y_pred.not_present == 1, 'not_present']),\n",
    "predicted = case_when([y_pred.prob_present > y_pred.prob_not_present, 'present'],\n",
    "[y_pred.prob_present < y_pred.prob_not_present, 'not_present']))\n",
    "\n",
    "y_pred >>= mutate(correct = case_when([y_pred.actual == y_pred.predicted, 'yes'],\n",
    "[y_pred.actual != y_pred.predicted, 'no']))\n",
    "y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20ec30b4408>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       " <defs>\r\n",
       "  <style type=\"text/css\">\r\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\r\n",
       "  </style>\r\n",
       " </defs>\r\n",
       " <g id=\"figure_1\">\r\n",
       "  <g id=\"patch_1\">\r\n",
       "   <path d=\"M 0 248.518125 \r\n",
       "L 372.103125 248.518125 \r\n",
       "L 372.103125 0 \r\n",
       "L 0 0 \r\n",
       "z\r\n",
       "\" style=\"fill:none;\"/>\r\n",
       "  </g>\r\n",
       "  <g id=\"axes_1\">\r\n",
       "   <g id=\"patch_2\">\r\n",
       "    <path d=\"M 30.103125 224.64 \r\n",
       "L 364.903125 224.64 \r\n",
       "L 364.903125 7.2 \r\n",
       "L 30.103125 7.2 \r\n",
       "z\r\n",
       "\" style=\"fill:#ffffff;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_1\">\r\n",
       "    <g id=\"xtick_1\">\r\n",
       "     <g id=\"line2d_1\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L 0 3.5 \r\n",
       "\" id=\"mcc690f1ae0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#mcc690f1ae0\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_1\">\r\n",
       "      <!-- 0 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 31.78125 66.40625 \r\n",
       "Q 24.171875 66.40625 20.328125 58.90625 \r\n",
       "Q 16.5 51.421875 16.5 36.375 \r\n",
       "Q 16.5 21.390625 20.328125 13.890625 \r\n",
       "Q 24.171875 6.390625 31.78125 6.390625 \r\n",
       "Q 39.453125 6.390625 43.28125 13.890625 \r\n",
       "Q 47.125 21.390625 47.125 36.375 \r\n",
       "Q 47.125 51.421875 43.28125 58.90625 \r\n",
       "Q 39.453125 66.40625 31.78125 66.40625 \r\n",
       "z\r\n",
       "M 31.78125 74.21875 \r\n",
       "Q 44.046875 74.21875 50.515625 64.515625 \r\n",
       "Q 56.984375 54.828125 56.984375 36.375 \r\n",
       "Q 56.984375 17.96875 50.515625 8.265625 \r\n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \r\n",
       "Q 6.59375 17.96875 6.59375 36.375 \r\n",
       "Q 6.59375 54.828125 13.0625 64.515625 \r\n",
       "Q 19.53125 74.21875 31.78125 74.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-48\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_2\">\r\n",
       "     <g id=\"line2d_2\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"92.915308\" xlink:href=\"#mcc690f1ae0\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_2\">\r\n",
       "      <!-- 200 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 19.1875 8.296875 \r\n",
       "L 53.609375 8.296875 \r\n",
       "L 53.609375 0 \r\n",
       "L 7.328125 0 \r\n",
       "L 7.328125 8.296875 \r\n",
       "Q 12.9375 14.109375 22.625 23.890625 \r\n",
       "Q 32.328125 33.6875 34.8125 36.53125 \r\n",
       "Q 39.546875 41.84375 41.421875 45.53125 \r\n",
       "Q 43.3125 49.21875 43.3125 52.78125 \r\n",
       "Q 43.3125 58.59375 39.234375 62.25 \r\n",
       "Q 35.15625 65.921875 28.609375 65.921875 \r\n",
       "Q 23.96875 65.921875 18.8125 64.3125 \r\n",
       "Q 13.671875 62.703125 7.8125 59.421875 \r\n",
       "L 7.8125 69.390625 \r\n",
       "Q 13.765625 71.78125 18.9375 73 \r\n",
       "Q 24.125 74.21875 28.421875 74.21875 \r\n",
       "Q 39.75 74.21875 46.484375 68.546875 \r\n",
       "Q 53.21875 62.890625 53.21875 53.421875 \r\n",
       "Q 53.21875 48.921875 51.53125 44.890625 \r\n",
       "Q 49.859375 40.875 45.40625 35.40625 \r\n",
       "Q 44.1875 33.984375 37.640625 27.21875 \r\n",
       "Q 31.109375 20.453125 19.1875 8.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-50\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(83.371558 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_3\">\r\n",
       "     <g id=\"line2d_3\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"140.509309\" xlink:href=\"#mcc690f1ae0\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_3\">\r\n",
       "      <!-- 400 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 37.796875 64.3125 \r\n",
       "L 12.890625 25.390625 \r\n",
       "L 37.796875 25.390625 \r\n",
       "z\r\n",
       "M 35.203125 72.90625 \r\n",
       "L 47.609375 72.90625 \r\n",
       "L 47.609375 25.390625 \r\n",
       "L 58.015625 25.390625 \r\n",
       "L 58.015625 17.1875 \r\n",
       "L 47.609375 17.1875 \r\n",
       "L 47.609375 0 \r\n",
       "L 37.796875 0 \r\n",
       "L 37.796875 17.1875 \r\n",
       "L 4.890625 17.1875 \r\n",
       "L 4.890625 26.703125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-52\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(130.965559 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_4\">\r\n",
       "     <g id=\"line2d_4\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"188.10331\" xlink:href=\"#mcc690f1ae0\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_4\">\r\n",
       "      <!-- 600 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 33.015625 40.375 \r\n",
       "Q 26.375 40.375 22.484375 35.828125 \r\n",
       "Q 18.609375 31.296875 18.609375 23.390625 \r\n",
       "Q 18.609375 15.53125 22.484375 10.953125 \r\n",
       "Q 26.375 6.390625 33.015625 6.390625 \r\n",
       "Q 39.65625 6.390625 43.53125 10.953125 \r\n",
       "Q 47.40625 15.53125 47.40625 23.390625 \r\n",
       "Q 47.40625 31.296875 43.53125 35.828125 \r\n",
       "Q 39.65625 40.375 33.015625 40.375 \r\n",
       "z\r\n",
       "M 52.59375 71.296875 \r\n",
       "L 52.59375 62.3125 \r\n",
       "Q 48.875 64.0625 45.09375 64.984375 \r\n",
       "Q 41.3125 65.921875 37.59375 65.921875 \r\n",
       "Q 27.828125 65.921875 22.671875 59.328125 \r\n",
       "Q 17.53125 52.734375 16.796875 39.40625 \r\n",
       "Q 19.671875 43.65625 24.015625 45.921875 \r\n",
       "Q 28.375 48.1875 33.59375 48.1875 \r\n",
       "Q 44.578125 48.1875 50.953125 41.515625 \r\n",
       "Q 57.328125 34.859375 57.328125 23.390625 \r\n",
       "Q 57.328125 12.15625 50.6875 5.359375 \r\n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \r\n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \r\n",
       "Q 6.984375 17.96875 6.984375 36.375 \r\n",
       "Q 6.984375 53.65625 15.1875 63.9375 \r\n",
       "Q 23.390625 74.21875 37.203125 74.21875 \r\n",
       "Q 40.921875 74.21875 44.703125 73.484375 \r\n",
       "Q 48.484375 72.75 52.59375 71.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-54\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(178.55956 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_5\">\r\n",
       "     <g id=\"line2d_5\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.697311\" xlink:href=\"#mcc690f1ae0\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_5\">\r\n",
       "      <!-- 800 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 31.78125 34.625 \r\n",
       "Q 24.75 34.625 20.71875 30.859375 \r\n",
       "Q 16.703125 27.09375 16.703125 20.515625 \r\n",
       "Q 16.703125 13.921875 20.71875 10.15625 \r\n",
       "Q 24.75 6.390625 31.78125 6.390625 \r\n",
       "Q 38.8125 6.390625 42.859375 10.171875 \r\n",
       "Q 46.921875 13.96875 46.921875 20.515625 \r\n",
       "Q 46.921875 27.09375 42.890625 30.859375 \r\n",
       "Q 38.875 34.625 31.78125 34.625 \r\n",
       "z\r\n",
       "M 21.921875 38.8125 \r\n",
       "Q 15.578125 40.375 12.03125 44.71875 \r\n",
       "Q 8.5 49.078125 8.5 55.328125 \r\n",
       "Q 8.5 64.0625 14.71875 69.140625 \r\n",
       "Q 20.953125 74.21875 31.78125 74.21875 \r\n",
       "Q 42.671875 74.21875 48.875 69.140625 \r\n",
       "Q 55.078125 64.0625 55.078125 55.328125 \r\n",
       "Q 55.078125 49.078125 51.53125 44.71875 \r\n",
       "Q 48 40.375 41.703125 38.8125 \r\n",
       "Q 48.828125 37.15625 52.796875 32.3125 \r\n",
       "Q 56.78125 27.484375 56.78125 20.515625 \r\n",
       "Q 56.78125 9.90625 50.3125 4.234375 \r\n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.734375 -1.421875 13.25 4.234375 \r\n",
       "Q 6.78125 9.90625 6.78125 20.515625 \r\n",
       "Q 6.78125 27.484375 10.78125 32.3125 \r\n",
       "Q 14.796875 37.15625 21.921875 38.8125 \r\n",
       "z\r\n",
       "M 18.3125 54.390625 \r\n",
       "Q 18.3125 48.734375 21.84375 45.5625 \r\n",
       "Q 25.390625 42.390625 31.78125 42.390625 \r\n",
       "Q 38.140625 42.390625 41.71875 45.5625 \r\n",
       "Q 45.3125 48.734375 45.3125 54.390625 \r\n",
       "Q 45.3125 60.0625 41.71875 63.234375 \r\n",
       "Q 38.140625 66.40625 31.78125 66.40625 \r\n",
       "Q 25.390625 66.40625 21.84375 63.234375 \r\n",
       "Q 18.3125 60.0625 18.3125 54.390625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-56\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(226.153561 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-56\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_6\">\r\n",
       "     <g id=\"line2d_6\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"283.291312\" xlink:href=\"#mcc690f1ae0\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_6\">\r\n",
       "      <!-- 1000 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 12.40625 8.296875 \r\n",
       "L 28.515625 8.296875 \r\n",
       "L 28.515625 63.921875 \r\n",
       "L 10.984375 60.40625 \r\n",
       "L 10.984375 69.390625 \r\n",
       "L 28.421875 72.90625 \r\n",
       "L 38.28125 72.90625 \r\n",
       "L 38.28125 8.296875 \r\n",
       "L 54.390625 8.296875 \r\n",
       "L 54.390625 0 \r\n",
       "L 12.40625 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-49\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(270.566312 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_7\">\r\n",
       "     <g id=\"line2d_7\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.885313\" xlink:href=\"#mcc690f1ae0\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_7\">\r\n",
       "      <!-- 1200 -->\r\n",
       "      <g transform=\"translate(318.160313 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_2\">\r\n",
       "    <g id=\"ytick_1\">\r\n",
       "     <g id=\"line2d_8\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L -3.5 0 \r\n",
       "\" id=\"m8a43d56f7c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8a43d56f7c\" y=\"214.756364\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_8\">\r\n",
       "      <!-- 0.0 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 10.6875 12.40625 \r\n",
       "L 21 12.40625 \r\n",
       "L 21 0 \r\n",
       "L 10.6875 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-46\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 218.555582)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_2\">\r\n",
       "     <g id=\"line2d_9\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8a43d56f7c\" y=\"175.221818\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_9\">\r\n",
       "      <!-- 0.2 -->\r\n",
       "      <g transform=\"translate(7.2 179.021037)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_3\">\r\n",
       "     <g id=\"line2d_10\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8a43d56f7c\" y=\"135.687273\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_10\">\r\n",
       "      <!-- 0.4 -->\r\n",
       "      <g transform=\"translate(7.2 139.486491)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_4\">\r\n",
       "     <g id=\"line2d_11\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8a43d56f7c\" y=\"96.152727\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_11\">\r\n",
       "      <!-- 0.6 -->\r\n",
       "      <g transform=\"translate(7.2 99.951946)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_5\">\r\n",
       "     <g id=\"line2d_12\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8a43d56f7c\" y=\"56.618182\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_12\">\r\n",
       "      <!-- 0.8 -->\r\n",
       "      <g transform=\"translate(7.2 60.417401)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_6\">\r\n",
       "     <g id=\"line2d_13\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m8a43d56f7c\" y=\"17.083636\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_13\">\r\n",
       "      <!-- 1.0 -->\r\n",
       "      <g transform=\"translate(7.2 20.882855)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"line2d_14\">\r\n",
       "    <path clip-path=\"url(#p4054ec7d2b)\" d=\"M 45.321307 214.756364 \r\n",
       "L 45.559277 113.508733 \r\n",
       "L 45.797247 79.388497 \r\n",
       "L 46.273187 71.92962 \r\n",
       "L 46.987097 66.237567 \r\n",
       "L 47.938977 62.255097 \r\n",
       "L 49.366797 57.206053 \r\n",
       "L 50.794617 52.92477 \r\n",
       "L 52.460407 48.675048 \r\n",
       "L 53.888227 45.580635 \r\n",
       "L 55.078077 43.332008 \r\n",
       "L 56.505897 40.989603 \r\n",
       "L 58.171687 38.674201 \r\n",
       "L 60.075447 36.50869 \r\n",
       "L 61.979207 34.711286 \r\n",
       "L 63.644997 33.424294 \r\n",
       "L 66.024697 31.889906 \r\n",
       "L 68.642367 30.51 \r\n",
       "L 71.260037 29.390483 \r\n",
       "L 74.591617 28.212807 \r\n",
       "L 78.637108 27.039868 \r\n",
       "L 83.158538 25.973822 \r\n",
       "L 88.869818 24.855155 \r\n",
       "L 96.008918 23.699791 \r\n",
       "L 104.099898 22.621194 \r\n",
       "L 113.856668 21.548011 \r\n",
       "L 125.517198 20.502238 \r\n",
       "L 138.367579 19.587302 \r\n",
       "L 152.645779 18.800246 \r\n",
       "L 168.589769 18.155472 \r\n",
       "L 186.43752 17.665932 \r\n",
       "L 207.14091 17.330019 \r\n",
       "L 233.079641 17.149185 \r\n",
       "L 279.007852 17.084911 \r\n",
       "L 349.684943 17.083636 \r\n",
       "L 349.684943 17.083636 \r\n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_3\">\r\n",
       "    <path d=\"M 30.103125 224.64 \r\n",
       "L 30.103125 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_4\">\r\n",
       "    <path d=\"M 364.903125 224.64 \r\n",
       "L 364.903125 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_5\">\r\n",
       "    <path d=\"M 30.103125 224.64 \r\n",
       "L 364.903125 224.64 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_6\">\r\n",
       "    <path d=\"M 30.103125 7.2 \r\n",
       "L 364.903125 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"legend_1\">\r\n",
       "    <g id=\"patch_7\">\r\n",
       "     <path d=\"M 271.35 219.64 \r\n",
       "L 357.903125 219.64 \r\n",
       "Q 359.903125 219.64 359.903125 217.64 \r\n",
       "L 359.903125 203.68375 \r\n",
       "Q 359.903125 201.68375 357.903125 201.68375 \r\n",
       "L 271.35 201.68375 \r\n",
       "Q 269.35 201.68375 269.35 203.68375 \r\n",
       "L 269.35 217.64 \r\n",
       "Q 269.35 219.64 271.35 219.64 \r\n",
       "z\r\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n",
       "    </g>\r\n",
       "    <g id=\"line2d_15\">\r\n",
       "     <path d=\"M 273.35 209.782187 \r\n",
       "L 293.35 209.782187 \r\n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n",
       "    </g>\r\n",
       "    <g id=\"line2d_16\"/>\r\n",
       "    <g id=\"text_14\">\r\n",
       "     <!-- ratio_sums -->\r\n",
       "     <defs>\r\n",
       "      <path d=\"M 41.109375 46.296875 \r\n",
       "Q 39.59375 47.171875 37.8125 47.578125 \r\n",
       "Q 36.03125 48 33.890625 48 \r\n",
       "Q 26.265625 48 22.1875 43.046875 \r\n",
       "Q 18.109375 38.09375 18.109375 28.8125 \r\n",
       "L 18.109375 0 \r\n",
       "L 9.078125 0 \r\n",
       "L 9.078125 54.6875 \r\n",
       "L 18.109375 54.6875 \r\n",
       "L 18.109375 46.1875 \r\n",
       "Q 20.953125 51.171875 25.484375 53.578125 \r\n",
       "Q 30.03125 56 36.53125 56 \r\n",
       "Q 37.453125 56 38.578125 55.875 \r\n",
       "Q 39.703125 55.765625 41.0625 55.515625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-114\"/>\r\n",
       "      <path d=\"M 34.28125 27.484375 \r\n",
       "Q 23.390625 27.484375 19.1875 25 \r\n",
       "Q 14.984375 22.515625 14.984375 16.5 \r\n",
       "Q 14.984375 11.71875 18.140625 8.90625 \r\n",
       "Q 21.296875 6.109375 26.703125 6.109375 \r\n",
       "Q 34.1875 6.109375 38.703125 11.40625 \r\n",
       "Q 43.21875 16.703125 43.21875 25.484375 \r\n",
       "L 43.21875 27.484375 \r\n",
       "z\r\n",
       "M 52.203125 31.203125 \r\n",
       "L 52.203125 0 \r\n",
       "L 43.21875 0 \r\n",
       "L 43.21875 8.296875 \r\n",
       "Q 40.140625 3.328125 35.546875 0.953125 \r\n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \r\n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \r\n",
       "Q 6 8.015625 6 15.921875 \r\n",
       "Q 6 25.140625 12.171875 29.828125 \r\n",
       "Q 18.359375 34.515625 30.609375 34.515625 \r\n",
       "L 43.21875 34.515625 \r\n",
       "L 43.21875 35.40625 \r\n",
       "Q 43.21875 41.609375 39.140625 45 \r\n",
       "Q 35.0625 48.390625 27.6875 48.390625 \r\n",
       "Q 23 48.390625 18.546875 47.265625 \r\n",
       "Q 14.109375 46.140625 10.015625 43.890625 \r\n",
       "L 10.015625 52.203125 \r\n",
       "Q 14.9375 54.109375 19.578125 55.046875 \r\n",
       "Q 24.21875 56 28.609375 56 \r\n",
       "Q 40.484375 56 46.34375 49.84375 \r\n",
       "Q 52.203125 43.703125 52.203125 31.203125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-97\"/>\r\n",
       "      <path d=\"M 18.3125 70.21875 \r\n",
       "L 18.3125 54.6875 \r\n",
       "L 36.8125 54.6875 \r\n",
       "L 36.8125 47.703125 \r\n",
       "L 18.3125 47.703125 \r\n",
       "L 18.3125 18.015625 \r\n",
       "Q 18.3125 11.328125 20.140625 9.421875 \r\n",
       "Q 21.96875 7.515625 27.59375 7.515625 \r\n",
       "L 36.8125 7.515625 \r\n",
       "L 36.8125 0 \r\n",
       "L 27.59375 0 \r\n",
       "Q 17.1875 0 13.234375 3.875 \r\n",
       "Q 9.28125 7.765625 9.28125 18.015625 \r\n",
       "L 9.28125 47.703125 \r\n",
       "L 2.6875 47.703125 \r\n",
       "L 2.6875 54.6875 \r\n",
       "L 9.28125 54.6875 \r\n",
       "L 9.28125 70.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-116\"/>\r\n",
       "      <path d=\"M 9.421875 54.6875 \r\n",
       "L 18.40625 54.6875 \r\n",
       "L 18.40625 0 \r\n",
       "L 9.421875 0 \r\n",
       "z\r\n",
       "M 9.421875 75.984375 \r\n",
       "L 18.40625 75.984375 \r\n",
       "L 18.40625 64.59375 \r\n",
       "L 9.421875 64.59375 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-105\"/>\r\n",
       "      <path d=\"M 30.609375 48.390625 \r\n",
       "Q 23.390625 48.390625 19.1875 42.75 \r\n",
       "Q 14.984375 37.109375 14.984375 27.296875 \r\n",
       "Q 14.984375 17.484375 19.15625 11.84375 \r\n",
       "Q 23.34375 6.203125 30.609375 6.203125 \r\n",
       "Q 37.796875 6.203125 41.984375 11.859375 \r\n",
       "Q 46.1875 17.53125 46.1875 27.296875 \r\n",
       "Q 46.1875 37.015625 41.984375 42.703125 \r\n",
       "Q 37.796875 48.390625 30.609375 48.390625 \r\n",
       "z\r\n",
       "M 30.609375 56 \r\n",
       "Q 42.328125 56 49.015625 48.375 \r\n",
       "Q 55.71875 40.765625 55.71875 27.296875 \r\n",
       "Q 55.71875 13.875 49.015625 6.21875 \r\n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \r\n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \r\n",
       "Q 5.515625 13.875 5.515625 27.296875 \r\n",
       "Q 5.515625 40.765625 12.171875 48.375 \r\n",
       "Q 18.84375 56 30.609375 56 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-111\"/>\r\n",
       "      <path d=\"M 50.984375 -16.609375 \r\n",
       "L 50.984375 -23.578125 \r\n",
       "L -0.984375 -23.578125 \r\n",
       "L -0.984375 -16.609375 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-95\"/>\r\n",
       "      <path d=\"M 44.28125 53.078125 \r\n",
       "L 44.28125 44.578125 \r\n",
       "Q 40.484375 46.53125 36.375 47.5 \r\n",
       "Q 32.28125 48.484375 27.875 48.484375 \r\n",
       "Q 21.1875 48.484375 17.84375 46.4375 \r\n",
       "Q 14.5 44.390625 14.5 40.28125 \r\n",
       "Q 14.5 37.15625 16.890625 35.375 \r\n",
       "Q 19.28125 33.59375 26.515625 31.984375 \r\n",
       "L 29.59375 31.296875 \r\n",
       "Q 39.15625 29.25 43.1875 25.515625 \r\n",
       "Q 47.21875 21.78125 47.21875 15.09375 \r\n",
       "Q 47.21875 7.46875 41.1875 3.015625 \r\n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \r\n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \r\n",
       "Q 10.6875 0.296875 5.421875 2 \r\n",
       "L 5.421875 11.28125 \r\n",
       "Q 10.40625 8.6875 15.234375 7.390625 \r\n",
       "Q 20.0625 6.109375 24.8125 6.109375 \r\n",
       "Q 31.15625 6.109375 34.5625 8.28125 \r\n",
       "Q 37.984375 10.453125 37.984375 14.40625 \r\n",
       "Q 37.984375 18.0625 35.515625 20.015625 \r\n",
       "Q 33.0625 21.96875 24.703125 23.78125 \r\n",
       "L 21.578125 24.515625 \r\n",
       "Q 13.234375 26.265625 9.515625 29.90625 \r\n",
       "Q 5.8125 33.546875 5.8125 39.890625 \r\n",
       "Q 5.8125 47.609375 11.28125 51.796875 \r\n",
       "Q 16.75 56 26.8125 56 \r\n",
       "Q 31.78125 56 36.171875 55.265625 \r\n",
       "Q 40.578125 54.546875 44.28125 53.078125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-115\"/>\r\n",
       "      <path d=\"M 8.5 21.578125 \r\n",
       "L 8.5 54.6875 \r\n",
       "L 17.484375 54.6875 \r\n",
       "L 17.484375 21.921875 \r\n",
       "Q 17.484375 14.15625 20.5 10.265625 \r\n",
       "Q 23.53125 6.390625 29.59375 6.390625 \r\n",
       "Q 36.859375 6.390625 41.078125 11.03125 \r\n",
       "Q 45.3125 15.671875 45.3125 23.6875 \r\n",
       "L 45.3125 54.6875 \r\n",
       "L 54.296875 54.6875 \r\n",
       "L 54.296875 0 \r\n",
       "L 45.3125 0 \r\n",
       "L 45.3125 8.40625 \r\n",
       "Q 42.046875 3.421875 37.71875 1 \r\n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \r\n",
       "Q 18.265625 -1.421875 13.375 4.4375 \r\n",
       "Q 8.5 10.296875 8.5 21.578125 \r\n",
       "z\r\n",
       "M 31.109375 56 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-117\"/>\r\n",
       "      <path d=\"M 52 44.1875 \r\n",
       "Q 55.375 50.25 60.0625 53.125 \r\n",
       "Q 64.75 56 71.09375 56 \r\n",
       "Q 79.640625 56 84.28125 50.015625 \r\n",
       "Q 88.921875 44.046875 88.921875 33.015625 \r\n",
       "L 88.921875 0 \r\n",
       "L 79.890625 0 \r\n",
       "L 79.890625 32.71875 \r\n",
       "Q 79.890625 40.578125 77.09375 44.375 \r\n",
       "Q 74.3125 48.1875 68.609375 48.1875 \r\n",
       "Q 61.625 48.1875 57.5625 43.546875 \r\n",
       "Q 53.515625 38.921875 53.515625 30.90625 \r\n",
       "L 53.515625 0 \r\n",
       "L 44.484375 0 \r\n",
       "L 44.484375 32.71875 \r\n",
       "Q 44.484375 40.625 41.703125 44.40625 \r\n",
       "Q 38.921875 48.1875 33.109375 48.1875 \r\n",
       "Q 26.21875 48.1875 22.15625 43.53125 \r\n",
       "Q 18.109375 38.875 18.109375 30.90625 \r\n",
       "L 18.109375 0 \r\n",
       "L 9.078125 0 \r\n",
       "L 9.078125 54.6875 \r\n",
       "L 18.109375 54.6875 \r\n",
       "L 18.109375 46.1875 \r\n",
       "Q 21.1875 51.21875 25.484375 53.609375 \r\n",
       "Q 29.78125 56 35.6875 56 \r\n",
       "Q 41.65625 56 45.828125 52.96875 \r\n",
       "Q 50 49.953125 52 44.1875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-109\"/>\r\n",
       "     </defs>\r\n",
       "     <g transform=\"translate(301.35 213.282187)scale(0.1 -0.1)\">\r\n",
       "      <use xlink:href=\"#DejaVuSans-114\"/>\r\n",
       "      <use x=\"41.113281\" xlink:href=\"#DejaVuSans-97\"/>\r\n",
       "      <use x=\"102.392578\" xlink:href=\"#DejaVuSans-116\"/>\r\n",
       "      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\r\n",
       "      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-111\"/>\r\n",
       "      <use x=\"230.566406\" xlink:href=\"#DejaVuSans-95\"/>\r\n",
       "      <use x=\"280.566406\" xlink:href=\"#DejaVuSans-115\"/>\r\n",
       "      <use x=\"332.666016\" xlink:href=\"#DejaVuSans-117\"/>\r\n",
       "      <use x=\"396.044922\" xlink:href=\"#DejaVuSans-109\"/>\r\n",
       "      <use x=\"493.457031\" xlink:href=\"#DejaVuSans-115\"/>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "  </g>\r\n",
       " </g>\r\n",
       " <defs>\r\n",
       "  <clipPath id=\"p4054ec7d2b\">\r\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n",
       "  </clipPath>\r\n",
       " </defs>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = df.drop(['behavior','signal','multiple','drone_present'],axis=1).values\n",
    "\n",
    "pca = decomposition.PCA(n_components=1280)\n",
    "pca.fit(x)\n",
    "pca_x = pca.transform(x)\n",
    "\n",
    "variance = []\n",
    "for i in range(1280):\n",
    "    variance.append(sum(pca.explained_variance_ratio_[0:i]))\n",
    "\n",
    "variance = pd.DataFrame(variance,columns=['ratio_sums'])\n",
    "variance.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presence_labs = df['drone_present']\n",
    "\n",
    "values = np.array(presence_labs)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "x = df.drop(['behavior','signal','multiple','drone_present'],axis=1).values\n",
    "\n",
    "pca = decomposition.PCA(n_components=200)\n",
    "pca.fit(x)\n",
    "x = pca.transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "model = keras.Sequential(name='test')\n",
    "model.add(Dense(64, activation='relu', input_shape=(200,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(200,))\n",
    "model.summary()\n",
    "\n",
    "fit1 = model.fit(x = x_train,\n",
    "        y = y_train, \n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        validation_split = .2,\n",
    "        verbose=True)\n",
    "\n",
    "prediction = model.predict(x_test)\n",
    "prediction_df = pd.DataFrame(prediction, columns=['prob_present','prob_not_present']) \n",
    "\n",
    "y_test = pd.DataFrame(y_test,columns=['present','not_present'])\n",
    "\n",
    "y_pred = y_test >> bind_cols(prediction_df)\n",
    "\n",
    "y_pred >>= mutate(actual = case_when([y_pred.present == 1, 'present'],\n",
    "[y_pred.not_present == 1, 'not_present']),\n",
    "predicted = case_when([y_pred.prob_present > y_pred.prob_not_present, 'present'],\n",
    "[y_pred.prob_present < y_pred.prob_not_present, 'not_present']))\n",
    "\n",
    "y_pred >>= mutate(correct = case_when([y_pred.actual == y_pred.predicted, 'yes'],\n",
    "[y_pred.actual != y_pred.predicted, 'no']))\n",
    "y_pred\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
