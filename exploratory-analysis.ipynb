{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.1.0\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dfply import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:\\\\ProjectData\\\\ERAU-REU\\\\Project-Drone-Behavior\\\\behavior-captures\\\\behavior-signal-multiple-processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot = df.plot(legend=False)\n",
    "#plot.figure.savefig(\"E:\\\\ProjectData\\\\ERAU-REU\\\\Project-Drone-Behavior\\\\plots\\\\allplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot = videoDf.plot(legend=False)\n",
    "#plot.figure.savefig(\"E:\\\\ProjectData\\\\ERAU-REU\\\\Project-Drone-Behavior\\\\plots\\\\allVideoPlot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df >>= mutate(drone_present = case_when([df.behavior == 'surround','yes'],\n",
    "[df.behavior == 'straight','yes'],\n",
    "[df.behavior == 'noise','no']))\n",
    "presence_labs = df['drone_present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(3200, 2)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "values = np.array(presence_labs)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = df.drop(['behavior','signal','multiple','drone_present'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(name='test')\n",
    "model.add(Dense(64, activation='relu', input_shape=(1280,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 64)                81984     \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_2 (Dense)              (None, 2)                 66        \n=================================================================\nTotal params: 84,130\nTrainable params: 84,130\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(1280,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 2048 samples, validate on 512 samples\nEpoch 1/100\n2048/2048 [==============================] - 1s 351us/sample - loss: 0.6851 - accuracy: 0.4390 - val_loss: 0.6788 - val_accuracy: 0.4512\nEpoch 2/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.6729 - accuracy: 0.5508 - val_loss: 0.6647 - val_accuracy: 0.6309\nEpoch 3/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.6595 - accuracy: 0.6233 - val_loss: 0.6507 - val_accuracy: 0.6406\nEpoch 4/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.6474 - accuracy: 0.6238 - val_loss: 0.6388 - val_accuracy: 0.6426\nEpoch 5/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.6369 - accuracy: 0.6238 - val_loss: 0.6285 - val_accuracy: 0.6426\nEpoch 6/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.6277 - accuracy: 0.6243 - val_loss: 0.6195 - val_accuracy: 0.6426\nEpoch 7/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.6192 - accuracy: 0.6243 - val_loss: 0.6108 - val_accuracy: 0.6426\nEpoch 8/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.6107 - accuracy: 0.6243 - val_loss: 0.6026 - val_accuracy: 0.6426\nEpoch 9/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.6029 - accuracy: 0.6243 - val_loss: 0.5951 - val_accuracy: 0.6426\nEpoch 10/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.5955 - accuracy: 0.6252 - val_loss: 0.5879 - val_accuracy: 0.6426\nEpoch 11/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.5885 - accuracy: 0.6455 - val_loss: 0.5813 - val_accuracy: 0.6963\nEpoch 12/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.5819 - accuracy: 0.7068 - val_loss: 0.5750 - val_accuracy: 0.7383\nEpoch 13/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.5755 - accuracy: 0.7368 - val_loss: 0.5689 - val_accuracy: 0.7510\nEpoch 14/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.5695 - accuracy: 0.7473 - val_loss: 0.5633 - val_accuracy: 0.7559\nEpoch 15/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.5638 - accuracy: 0.7490 - val_loss: 0.5580 - val_accuracy: 0.7578\nEpoch 16/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.5584 - accuracy: 0.7498 - val_loss: 0.5530 - val_accuracy: 0.7578\nEpoch 17/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.5534 - accuracy: 0.7703 - val_loss: 0.5482 - val_accuracy: 0.8008\nEpoch 18/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.5486 - accuracy: 0.8140 - val_loss: 0.5439 - val_accuracy: 0.8467\nEpoch 19/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.5438 - accuracy: 0.8496 - val_loss: 0.5396 - val_accuracy: 0.8623\nEpoch 20/100\n2048/2048 [==============================] - 0s 64us/sample - loss: 0.5393 - accuracy: 0.8655 - val_loss: 0.5352 - val_accuracy: 0.8652\nEpoch 21/100\n2048/2048 [==============================] - 0s 61us/sample - loss: 0.5348 - accuracy: 0.8723 - val_loss: 0.5311 - val_accuracy: 0.8682\nEpoch 22/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.5306 - accuracy: 0.8738 - val_loss: 0.5271 - val_accuracy: 0.8721\nEpoch 23/100\n2048/2048 [==============================] - 0s 61us/sample - loss: 0.5261 - accuracy: 0.8748 - val_loss: 0.5232 - val_accuracy: 0.8730\nEpoch 24/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.5221 - accuracy: 0.8750 - val_loss: 0.5193 - val_accuracy: 0.8730\nEpoch 25/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.5177 - accuracy: 0.8752 - val_loss: 0.5154 - val_accuracy: 0.8730\nEpoch 26/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.5135 - accuracy: 0.8752 - val_loss: 0.5117 - val_accuracy: 0.8750\nEpoch 27/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.5094 - accuracy: 0.8752 - val_loss: 0.5078 - val_accuracy: 0.8750\nEpoch 28/100\n2048/2048 [==============================] - 0s 65us/sample - loss: 0.5052 - accuracy: 0.8755 - val_loss: 0.5039 - val_accuracy: 0.8750\nEpoch 29/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.5010 - accuracy: 0.8752 - val_loss: 0.5000 - val_accuracy: 0.8750\nEpoch 30/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4965 - accuracy: 0.8752 - val_loss: 0.4958 - val_accuracy: 0.8750\nEpoch 31/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.4922 - accuracy: 0.8752 - val_loss: 0.4917 - val_accuracy: 0.8750\nEpoch 32/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4878 - accuracy: 0.8752 - val_loss: 0.4873 - val_accuracy: 0.8750\nEpoch 33/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4832 - accuracy: 0.8752 - val_loss: 0.4830 - val_accuracy: 0.8750\nEpoch 34/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4787 - accuracy: 0.8752 - val_loss: 0.4787 - val_accuracy: 0.8750\nEpoch 35/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.4740 - accuracy: 0.8752 - val_loss: 0.4743 - val_accuracy: 0.8750\nEpoch 36/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4692 - accuracy: 0.8752 - val_loss: 0.4702 - val_accuracy: 0.8750\nEpoch 37/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4647 - accuracy: 0.8752 - val_loss: 0.4656 - val_accuracy: 0.8750\nEpoch 38/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.4598 - accuracy: 0.8752 - val_loss: 0.4610 - val_accuracy: 0.8750\nEpoch 39/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4552 - accuracy: 0.8752 - val_loss: 0.4565 - val_accuracy: 0.8750\nEpoch 40/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4506 - accuracy: 0.8752 - val_loss: 0.4521 - val_accuracy: 0.8750\nEpoch 41/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.4459 - accuracy: 0.8752 - val_loss: 0.4478 - val_accuracy: 0.8750\nEpoch 42/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4411 - accuracy: 0.8752 - val_loss: 0.4434 - val_accuracy: 0.8750\nEpoch 43/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4365 - accuracy: 0.8752 - val_loss: 0.4390 - val_accuracy: 0.8750\nEpoch 44/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.4319 - accuracy: 0.8752 - val_loss: 0.4346 - val_accuracy: 0.8750\nEpoch 45/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4274 - accuracy: 0.8752 - val_loss: 0.4305 - val_accuracy: 0.8750\nEpoch 46/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4228 - accuracy: 0.8752 - val_loss: 0.4261 - val_accuracy: 0.8750\nEpoch 47/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.4183 - accuracy: 0.8752 - val_loss: 0.4221 - val_accuracy: 0.8750\nEpoch 48/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4140 - accuracy: 0.8752 - val_loss: 0.4180 - val_accuracy: 0.8750\nEpoch 49/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4098 - accuracy: 0.8752 - val_loss: 0.4140 - val_accuracy: 0.8750\nEpoch 50/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.4057 - accuracy: 0.8752 - val_loss: 0.4103 - val_accuracy: 0.8750\nEpoch 51/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.4021 - accuracy: 0.8752 - val_loss: 0.4067 - val_accuracy: 0.8750\nEpoch 52/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3978 - accuracy: 0.8750 - val_loss: 0.4030 - val_accuracy: 0.8750\nEpoch 53/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3946 - accuracy: 0.8750 - val_loss: 0.3997 - val_accuracy: 0.8750\nEpoch 54/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3911 - accuracy: 0.8752 - val_loss: 0.3964 - val_accuracy: 0.8750\nEpoch 55/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3879 - accuracy: 0.8752 - val_loss: 0.3933 - val_accuracy: 0.8750\nEpoch 56/100\n2048/2048 [==============================] - 0s 61us/sample - loss: 0.3843 - accuracy: 0.8748 - val_loss: 0.3906 - val_accuracy: 0.8750\nEpoch 57/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3818 - accuracy: 0.8750 - val_loss: 0.3877 - val_accuracy: 0.8750\nEpoch 58/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3790 - accuracy: 0.8752 - val_loss: 0.3851 - val_accuracy: 0.8750\nEpoch 59/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3763 - accuracy: 0.8748 - val_loss: 0.3826 - val_accuracy: 0.8740\nEpoch 60/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3736 - accuracy: 0.8748 - val_loss: 0.3803 - val_accuracy: 0.8750\nEpoch 61/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3712 - accuracy: 0.8752 - val_loss: 0.3780 - val_accuracy: 0.8740\nEpoch 62/100\n2048/2048 [==============================] - 0s 64us/sample - loss: 0.3689 - accuracy: 0.8755 - val_loss: 0.3761 - val_accuracy: 0.8750\nEpoch 63/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3671 - accuracy: 0.8750 - val_loss: 0.3741 - val_accuracy: 0.8750\nEpoch 64/100\n2048/2048 [==============================] - 0s 61us/sample - loss: 0.3648 - accuracy: 0.8748 - val_loss: 0.3722 - val_accuracy: 0.8750\nEpoch 65/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3627 - accuracy: 0.8752 - val_loss: 0.3707 - val_accuracy: 0.8730\nEpoch 66/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3608 - accuracy: 0.8755 - val_loss: 0.3689 - val_accuracy: 0.8750\nEpoch 67/100\n2048/2048 [==============================] - 0s 71us/sample - loss: 0.3592 - accuracy: 0.8755 - val_loss: 0.3673 - val_accuracy: 0.8750\nEpoch 68/100\n2048/2048 [==============================] - 0s 69us/sample - loss: 0.3573 - accuracy: 0.8755 - val_loss: 0.3656 - val_accuracy: 0.8740\nEpoch 69/100\n2048/2048 [==============================] - 0s 66us/sample - loss: 0.3557 - accuracy: 0.8755 - val_loss: 0.3641 - val_accuracy: 0.8730\nEpoch 70/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3540 - accuracy: 0.8745 - val_loss: 0.3629 - val_accuracy: 0.8740\nEpoch 71/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.3523 - accuracy: 0.8752 - val_loss: 0.3615 - val_accuracy: 0.8740\nEpoch 72/100\n2048/2048 [==============================] - 0s 64us/sample - loss: 0.3510 - accuracy: 0.8757 - val_loss: 0.3602 - val_accuracy: 0.8740\nEpoch 73/100\n2048/2048 [==============================] - 0s 72us/sample - loss: 0.3494 - accuracy: 0.8760 - val_loss: 0.3597 - val_accuracy: 0.8750\nEpoch 74/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.3487 - accuracy: 0.8760 - val_loss: 0.3578 - val_accuracy: 0.8740\nEpoch 75/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3472 - accuracy: 0.8745 - val_loss: 0.3567 - val_accuracy: 0.8740\nEpoch 76/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.3457 - accuracy: 0.8757 - val_loss: 0.3558 - val_accuracy: 0.8740\nEpoch 77/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3448 - accuracy: 0.8757 - val_loss: 0.3548 - val_accuracy: 0.8740\nEpoch 78/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3436 - accuracy: 0.8752 - val_loss: 0.3538 - val_accuracy: 0.8740\nEpoch 79/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.3425 - accuracy: 0.8755 - val_loss: 0.3529 - val_accuracy: 0.8730\nEpoch 80/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3415 - accuracy: 0.8760 - val_loss: 0.3523 - val_accuracy: 0.8740\nEpoch 81/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3405 - accuracy: 0.8762 - val_loss: 0.3515 - val_accuracy: 0.8740\nEpoch 82/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.3394 - accuracy: 0.8757 - val_loss: 0.3511 - val_accuracy: 0.8750\nEpoch 83/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3386 - accuracy: 0.8760 - val_loss: 0.3497 - val_accuracy: 0.8740\nEpoch 84/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3373 - accuracy: 0.8760 - val_loss: 0.3490 - val_accuracy: 0.8721\nEpoch 85/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3366 - accuracy: 0.8755 - val_loss: 0.3485 - val_accuracy: 0.8740\nEpoch 86/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3358 - accuracy: 0.8760 - val_loss: 0.3477 - val_accuracy: 0.8721\nEpoch 87/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3353 - accuracy: 0.8767 - val_loss: 0.3469 - val_accuracy: 0.8730\nEpoch 88/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.3340 - accuracy: 0.8760 - val_loss: 0.3469 - val_accuracy: 0.8750\nEpoch 89/100\n2048/2048 [==============================] - 0s 72us/sample - loss: 0.3336 - accuracy: 0.8762 - val_loss: 0.3457 - val_accuracy: 0.8740\nEpoch 90/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3327 - accuracy: 0.8760 - val_loss: 0.3454 - val_accuracy: 0.8711\nEpoch 91/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3319 - accuracy: 0.8765 - val_loss: 0.3448 - val_accuracy: 0.8740\nEpoch 92/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3312 - accuracy: 0.8774 - val_loss: 0.3446 - val_accuracy: 0.8750\nEpoch 93/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3309 - accuracy: 0.8757 - val_loss: 0.3437 - val_accuracy: 0.8721\nEpoch 94/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3302 - accuracy: 0.8765 - val_loss: 0.3432 - val_accuracy: 0.8740\nEpoch 95/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.3294 - accuracy: 0.8765 - val_loss: 0.3429 - val_accuracy: 0.8740\nEpoch 96/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3289 - accuracy: 0.8760 - val_loss: 0.3433 - val_accuracy: 0.8750\nEpoch 97/100\n2048/2048 [==============================] - 0s 66us/sample - loss: 0.3286 - accuracy: 0.8760 - val_loss: 0.3419 - val_accuracy: 0.8740\nEpoch 98/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.3276 - accuracy: 0.8767 - val_loss: 0.3415 - val_accuracy: 0.8721\nEpoch 99/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3269 - accuracy: 0.8765 - val_loss: 0.3411 - val_accuracy: 0.8730\nEpoch 100/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3264 - accuracy: 0.8765 - val_loss: 0.3415 - val_accuracy: 0.8721\n"
    }
   ],
   "source": [
    "fit1 = model.fit(x = x_train,\n",
    "        y = y_train, \n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        validation_split = .2,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(x_test)\n",
    "prediction_df = pd.DataFrame(prediction, columns=['prob_present','prob_not_present']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     present  not_present  prob_present  prob_not_present       actual  \\\n0        0.0          1.0      0.193071          0.805527  not_present   \n1        1.0          0.0      0.221193          0.751699      present   \n2        0.0          1.0      0.259591          0.707430  not_present   \n3        0.0          1.0      0.165224          0.824095  not_present   \n4        1.0          0.0      0.191563          0.808546      present   \n..       ...          ...           ...               ...          ...   \n635      1.0          0.0      0.991199          0.009219      present   \n636      0.0          1.0      0.195044          0.800591  not_present   \n637      0.0          1.0      0.173823          0.824452  not_present   \n638      1.0          0.0      0.973547          0.021875      present   \n639      1.0          0.0      0.973195          0.022034      present   \n\n       predicted correct  \n0    not_present     yes  \n1    not_present      no  \n2    not_present     yes  \n3    not_present     yes  \n4    not_present      no  \n..           ...     ...  \n635      present     yes  \n636  not_present     yes  \n637  not_present     yes  \n638      present     yes  \n639      present     yes  \n\n[640 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>present</th>\n      <th>not_present</th>\n      <th>prob_present</th>\n      <th>prob_not_present</th>\n      <th>actual</th>\n      <th>predicted</th>\n      <th>correct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.193071</td>\n      <td>0.805527</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.221193</td>\n      <td>0.751699</td>\n      <td>present</td>\n      <td>not_present</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.259591</td>\n      <td>0.707430</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.165224</td>\n      <td>0.824095</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.191563</td>\n      <td>0.808546</td>\n      <td>present</td>\n      <td>not_present</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>635</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.991199</td>\n      <td>0.009219</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>636</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.195044</td>\n      <td>0.800591</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.173823</td>\n      <td>0.824452</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>638</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.973547</td>\n      <td>0.021875</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>639</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.973195</td>\n      <td>0.022034</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n    </tr>\n  </tbody>\n</table>\n<p>640 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "y_test = pd.DataFrame(y_test,columns=['present','not_present'])\n",
    "\n",
    "y_pred = y_test >> bind_cols(prediction_df)\n",
    "\n",
    "y_pred >>= mutate(actual = case_when([y_pred.present == 1, 'present'],\n",
    "[y_pred.not_present == 1, 'not_present']),\n",
    "predicted = case_when([y_pred.prob_present > y_pred.prob_not_present, 'present'],\n",
    "[y_pred.prob_present < y_pred.prob_not_present, 'not_present']))\n",
    "\n",
    "y_pred >>= mutate(correct = case_when([y_pred.actual == y_pred.predicted, 'yes'],\n",
    "[y_pred.actual != y_pred.predicted, 'no']))\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      imf_1_feature_1  imf_1_feature_2  imf_1_feature_3  imf_1_feature_4  \\\n0            0.048540         0.104127         0.036859         0.035103   \n1            0.019087         0.015032         0.016774         0.098352   \n2            0.105886         0.028141         0.012870         0.016689   \n3            0.055652         0.020754         0.084426         0.068263   \n4            0.055740         0.048381         0.059448         0.049503   \n...               ...              ...              ...              ...   \n3195         0.032114         0.018314         0.009516         0.031732   \n3196         0.007821         0.023563         0.013839         0.021590   \n3197         0.004704         0.012574         0.031253         0.015620   \n3198         0.010071         0.009639         0.030514         0.010773   \n3199         0.001208         0.004971         0.014260         0.016625   \n\n      imf_1_feature_5  imf_1_feature_6  imf_1_feature_7  imf_1_feature_8  \\\n0            0.015363         0.086067         0.043169         0.036946   \n1            0.047071         0.070620         0.070484         0.022026   \n2            0.102190         0.129517         0.075366         0.056435   \n3            0.045849         0.075612         0.094904         0.053560   \n4            0.012088         0.076470         0.126046         0.088670   \n...               ...              ...              ...              ...   \n3195         0.016047         0.008095         0.028272         0.018574   \n3196         0.009250         0.016283         0.013973         0.009530   \n3197         0.002208         0.029783         0.025996         0.007936   \n3198         0.001604         0.018141         0.030366         0.017224   \n3199         0.025723         0.026920         0.017984         0.008127   \n\n      imf_1_feature_9  imf_1_feature_10  ...  imf_1_feature_248  \\\n0            0.072398          0.100767  ...           0.054025   \n1            0.093699          0.023186  ...           0.100857   \n2            0.094701          0.037966  ...           0.071534   \n3            0.086233          0.072574  ...           0.041053   \n4            0.066795          0.046619  ...           0.053259   \n...               ...               ...  ...                ...   \n3195         0.017942          0.043779  ...           0.041790   \n3196         0.015082          0.017309  ...           0.016522   \n3197         0.008834          0.008706  ...           0.008310   \n3198         0.009127          0.020327  ...           0.019404   \n3199         0.006057          0.015341  ...           0.014644   \n\n      imf_1_feature_249  imf_1_feature_250  imf_1_feature_251  \\\n0              0.091116           0.050495           0.020504   \n1              0.098248           0.071655           0.070450   \n2              0.099206           0.082002           0.077314   \n3              0.060956           0.083080           0.046290   \n4              0.037194           0.044701           0.059681   \n...                 ...                ...                ...   \n3195           0.020034           0.017063           0.027498   \n3196           0.016841           0.008755           0.013591   \n3197           0.009864           0.007290           0.025284   \n3198           0.010192           0.015823           0.029535   \n3199           0.006763           0.007466           0.017491   \n\n      imf_1_feature_252  imf_1_feature_253  imf_1_feature_254  \\\n0              0.040957           0.022595           0.082013   \n1              0.078421           0.041970           0.039761   \n2              0.037580           0.078382           0.105992   \n3              0.019449           0.126770           0.075964   \n4              0.041496           0.084235           0.060550   \n...                 ...                ...                ...   \n3195           0.007705           0.015313           0.033260   \n3196           0.015497           0.008827           0.022630   \n3197           0.028346           0.002107           0.016373   \n3198           0.017265           0.001531           0.011292   \n3199           0.025621           0.024547           0.017426   \n\n      imf_1_feature_255  imf_1_feature_256  drone_present  \n0              0.097442           0.005685            yes  \n1              0.063975           0.037318            yes  \n2              0.176923           0.034027            yes  \n3              0.072927           0.073517            yes  \n4              0.082656           0.058736            yes  \n...                 ...                ...            ...  \n3195           0.010302           0.018391             no  \n3196           0.014762           0.023727             no  \n3197           0.032724           0.012556             no  \n3198           0.031961           0.009573             no  \n3199           0.015196           0.004827             no  \n\n[3200 rows x 257 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imf_1_feature_1</th>\n      <th>imf_1_feature_2</th>\n      <th>imf_1_feature_3</th>\n      <th>imf_1_feature_4</th>\n      <th>imf_1_feature_5</th>\n      <th>imf_1_feature_6</th>\n      <th>imf_1_feature_7</th>\n      <th>imf_1_feature_8</th>\n      <th>imf_1_feature_9</th>\n      <th>imf_1_feature_10</th>\n      <th>...</th>\n      <th>imf_1_feature_248</th>\n      <th>imf_1_feature_249</th>\n      <th>imf_1_feature_250</th>\n      <th>imf_1_feature_251</th>\n      <th>imf_1_feature_252</th>\n      <th>imf_1_feature_253</th>\n      <th>imf_1_feature_254</th>\n      <th>imf_1_feature_255</th>\n      <th>imf_1_feature_256</th>\n      <th>drone_present</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.048540</td>\n      <td>0.104127</td>\n      <td>0.036859</td>\n      <td>0.035103</td>\n      <td>0.015363</td>\n      <td>0.086067</td>\n      <td>0.043169</td>\n      <td>0.036946</td>\n      <td>0.072398</td>\n      <td>0.100767</td>\n      <td>...</td>\n      <td>0.054025</td>\n      <td>0.091116</td>\n      <td>0.050495</td>\n      <td>0.020504</td>\n      <td>0.040957</td>\n      <td>0.022595</td>\n      <td>0.082013</td>\n      <td>0.097442</td>\n      <td>0.005685</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.019087</td>\n      <td>0.015032</td>\n      <td>0.016774</td>\n      <td>0.098352</td>\n      <td>0.047071</td>\n      <td>0.070620</td>\n      <td>0.070484</td>\n      <td>0.022026</td>\n      <td>0.093699</td>\n      <td>0.023186</td>\n      <td>...</td>\n      <td>0.100857</td>\n      <td>0.098248</td>\n      <td>0.071655</td>\n      <td>0.070450</td>\n      <td>0.078421</td>\n      <td>0.041970</td>\n      <td>0.039761</td>\n      <td>0.063975</td>\n      <td>0.037318</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.105886</td>\n      <td>0.028141</td>\n      <td>0.012870</td>\n      <td>0.016689</td>\n      <td>0.102190</td>\n      <td>0.129517</td>\n      <td>0.075366</td>\n      <td>0.056435</td>\n      <td>0.094701</td>\n      <td>0.037966</td>\n      <td>...</td>\n      <td>0.071534</td>\n      <td>0.099206</td>\n      <td>0.082002</td>\n      <td>0.077314</td>\n      <td>0.037580</td>\n      <td>0.078382</td>\n      <td>0.105992</td>\n      <td>0.176923</td>\n      <td>0.034027</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.055652</td>\n      <td>0.020754</td>\n      <td>0.084426</td>\n      <td>0.068263</td>\n      <td>0.045849</td>\n      <td>0.075612</td>\n      <td>0.094904</td>\n      <td>0.053560</td>\n      <td>0.086233</td>\n      <td>0.072574</td>\n      <td>...</td>\n      <td>0.041053</td>\n      <td>0.060956</td>\n      <td>0.083080</td>\n      <td>0.046290</td>\n      <td>0.019449</td>\n      <td>0.126770</td>\n      <td>0.075964</td>\n      <td>0.072927</td>\n      <td>0.073517</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.055740</td>\n      <td>0.048381</td>\n      <td>0.059448</td>\n      <td>0.049503</td>\n      <td>0.012088</td>\n      <td>0.076470</td>\n      <td>0.126046</td>\n      <td>0.088670</td>\n      <td>0.066795</td>\n      <td>0.046619</td>\n      <td>...</td>\n      <td>0.053259</td>\n      <td>0.037194</td>\n      <td>0.044701</td>\n      <td>0.059681</td>\n      <td>0.041496</td>\n      <td>0.084235</td>\n      <td>0.060550</td>\n      <td>0.082656</td>\n      <td>0.058736</td>\n      <td>yes</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3195</th>\n      <td>0.032114</td>\n      <td>0.018314</td>\n      <td>0.009516</td>\n      <td>0.031732</td>\n      <td>0.016047</td>\n      <td>0.008095</td>\n      <td>0.028272</td>\n      <td>0.018574</td>\n      <td>0.017942</td>\n      <td>0.043779</td>\n      <td>...</td>\n      <td>0.041790</td>\n      <td>0.020034</td>\n      <td>0.017063</td>\n      <td>0.027498</td>\n      <td>0.007705</td>\n      <td>0.015313</td>\n      <td>0.033260</td>\n      <td>0.010302</td>\n      <td>0.018391</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>3196</th>\n      <td>0.007821</td>\n      <td>0.023563</td>\n      <td>0.013839</td>\n      <td>0.021590</td>\n      <td>0.009250</td>\n      <td>0.016283</td>\n      <td>0.013973</td>\n      <td>0.009530</td>\n      <td>0.015082</td>\n      <td>0.017309</td>\n      <td>...</td>\n      <td>0.016522</td>\n      <td>0.016841</td>\n      <td>0.008755</td>\n      <td>0.013591</td>\n      <td>0.015497</td>\n      <td>0.008827</td>\n      <td>0.022630</td>\n      <td>0.014762</td>\n      <td>0.023727</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>3197</th>\n      <td>0.004704</td>\n      <td>0.012574</td>\n      <td>0.031253</td>\n      <td>0.015620</td>\n      <td>0.002208</td>\n      <td>0.029783</td>\n      <td>0.025996</td>\n      <td>0.007936</td>\n      <td>0.008834</td>\n      <td>0.008706</td>\n      <td>...</td>\n      <td>0.008310</td>\n      <td>0.009864</td>\n      <td>0.007290</td>\n      <td>0.025284</td>\n      <td>0.028346</td>\n      <td>0.002107</td>\n      <td>0.016373</td>\n      <td>0.032724</td>\n      <td>0.012556</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>3198</th>\n      <td>0.010071</td>\n      <td>0.009639</td>\n      <td>0.030514</td>\n      <td>0.010773</td>\n      <td>0.001604</td>\n      <td>0.018141</td>\n      <td>0.030366</td>\n      <td>0.017224</td>\n      <td>0.009127</td>\n      <td>0.020327</td>\n      <td>...</td>\n      <td>0.019404</td>\n      <td>0.010192</td>\n      <td>0.015823</td>\n      <td>0.029535</td>\n      <td>0.017265</td>\n      <td>0.001531</td>\n      <td>0.011292</td>\n      <td>0.031961</td>\n      <td>0.009573</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>3199</th>\n      <td>0.001208</td>\n      <td>0.004971</td>\n      <td>0.014260</td>\n      <td>0.016625</td>\n      <td>0.025723</td>\n      <td>0.026920</td>\n      <td>0.017984</td>\n      <td>0.008127</td>\n      <td>0.006057</td>\n      <td>0.015341</td>\n      <td>...</td>\n      <td>0.014644</td>\n      <td>0.006763</td>\n      <td>0.007466</td>\n      <td>0.017491</td>\n      <td>0.025621</td>\n      <td>0.024547</td>\n      <td>0.017426</td>\n      <td>0.015196</td>\n      <td>0.004827</td>\n      <td>no</td>\n    </tr>\n  </tbody>\n</table>\n<p>3200 rows × 257 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "small_df = df[df.columns[0:256]]\n",
    "small_df >>= bind_cols(df.drone_present)\n",
    "\n",
    "presence_labs = df_small['drone_present']\n",
    "\n",
    "values = np.array(presence_labs)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "x = df_small.drop(['drone_present'],axis=1).values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "model = keras.Sequential(name='test')\n",
    "model.add(Dense(64, activation='relu', input_shape=(256,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model = keras.Sequential(name='test')\n",
    "model.add(Dense(64, activation='relu', input_shape=(256,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "fit1 = model.fit(x = x_train,\n",
    "        y = y_train, \n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        validation_split = .2,\n",
    "        verbose=True)\n",
    "\n",
    "prediction = model.predict(x_test)\n",
    "prediction_df = pd.DataFrame(prediction, columns=['prob_present','prob_not_present']) \n",
    "\n",
    "y_test = pd.DataFrame(y_test,columns=['present','not_present'])\n",
    "\n",
    "y_pred = y_test >> bind_cols(prediction_df)\n",
    "\n",
    "y_pred >>= mutate(actual = case_when([y_pred.present == 1, 'present'],\n",
    "[y_pred.not_present == 1, 'not_present']),\n",
    "predicted = case_when([y_pred.prob_present > y_pred.prob_not_present, 'present'],\n",
    "[y_pred.prob_present < y_pred.prob_not_present, 'not_present']))\n",
    "\n",
    "y_pred >>= mutate(correct = case_when([y_pred.actual == y_pred.predicted, 'yes'],\n",
    "[y_pred.actual != y_pred.predicted, 'no']))\n",
    "y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi Riley, if you can read this, then you have done everything right... Good Job!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittfgpuconda45a37ab07269429dbc68acdce435ee9a",
   "display_name": "Python 3.7.7 64-bit ('tfgpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}