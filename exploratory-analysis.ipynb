{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.1.0\nDefault GPU Device: /device:GPU:0\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dfply import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "   print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:\\\\ProjectData\\\\ERAU-REU\\\\Project-Drone-Behavior\\\\behavior-captures\\\\behavior-signal-multiple-processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot = df.plot(legend=False)\n",
    "#plot.figure.savefig(\"E:\\\\ProjectData\\\\ERAU-REU\\\\Project-Drone-Behavior\\\\plots\\\\allplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot = videoDf.plot(legend=False)\n",
    "#plot.figure.savefig(\"E:\\\\ProjectData\\\\ERAU-REU\\\\Project-Drone-Behavior\\\\plots\\\\allVideoPlot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df >>= mutate(drone_present = case_when([df.behavior == 'surround','yes'],\n",
    "[df.behavior == 'straight','yes'],\n",
    "[df.behavior == 'noise','no']))\n",
    "presence_labs = df['drone_present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(3200, 2)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "values = np.array(presence_labs)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = df.drop(['behavior','signal','multiple','drone_present'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_large = keras.Sequential(name='test')\n",
    "model_large.add(Dense(64, activation='relu', input_shape=(1280,)))\n",
    "model_large.add(Dense(32, activation='relu'))\n",
    "model_large.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 64)                81984     \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_2 (Dense)              (None, 2)                 66        \n=================================================================\nTotal params: 84,130\nTrainable params: 84,130\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model_large.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_large.build(input_shape=(1280,))\n",
    "model_large.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 2048 samples, validate on 512 samples\nEpoch 1/100\n2048/2048 [==============================] - 1s 290us/sample - loss: 0.6873 - accuracy: 0.4902 - val_loss: 0.6789 - val_accuracy: 0.5811\nEpoch 2/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.6734 - accuracy: 0.6079 - val_loss: 0.6677 - val_accuracy: 0.6133\nEpoch 3/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.6621 - accuracy: 0.6218 - val_loss: 0.6579 - val_accuracy: 0.6152\nEpoch 4/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.6523 - accuracy: 0.6255 - val_loss: 0.6492 - val_accuracy: 0.6162\nEpoch 5/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.6436 - accuracy: 0.6274 - val_loss: 0.6413 - val_accuracy: 0.6152\nEpoch 6/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.6358 - accuracy: 0.6274 - val_loss: 0.6341 - val_accuracy: 0.6152\nEpoch 7/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.6286 - accuracy: 0.6274 - val_loss: 0.6274 - val_accuracy: 0.6152\nEpoch 8/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.6221 - accuracy: 0.6274 - val_loss: 0.6212 - val_accuracy: 0.6152\nEpoch 9/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.6163 - accuracy: 0.6274 - val_loss: 0.6155 - val_accuracy: 0.6152\nEpoch 10/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.6107 - accuracy: 0.6274 - val_loss: 0.6098 - val_accuracy: 0.6152\nEpoch 11/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.6054 - accuracy: 0.6274 - val_loss: 0.6044 - val_accuracy: 0.6152\nEpoch 12/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.6005 - accuracy: 0.6274 - val_loss: 0.5992 - val_accuracy: 0.6152\nEpoch 13/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5958 - accuracy: 0.6274 - val_loss: 0.5942 - val_accuracy: 0.6152\nEpoch 14/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.5914 - accuracy: 0.6274 - val_loss: 0.5893 - val_accuracy: 0.6152\nEpoch 15/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5870 - accuracy: 0.6274 - val_loss: 0.5845 - val_accuracy: 0.6152\nEpoch 16/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5828 - accuracy: 0.6274 - val_loss: 0.5797 - val_accuracy: 0.6152\nEpoch 17/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.5786 - accuracy: 0.6279 - val_loss: 0.5751 - val_accuracy: 0.6182\nEpoch 18/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.5745 - accuracy: 0.6438 - val_loss: 0.5703 - val_accuracy: 0.6465\nEpoch 19/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.5705 - accuracy: 0.6814 - val_loss: 0.5657 - val_accuracy: 0.6895\nEpoch 20/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.5665 - accuracy: 0.7075 - val_loss: 0.5613 - val_accuracy: 0.7197\nEpoch 21/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5626 - accuracy: 0.7380 - val_loss: 0.5569 - val_accuracy: 0.7383\nEpoch 22/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5587 - accuracy: 0.7734 - val_loss: 0.5525 - val_accuracy: 0.7666\nEpoch 23/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.5553 - accuracy: 0.7949 - val_loss: 0.5486 - val_accuracy: 0.8154\nEpoch 24/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5519 - accuracy: 0.8286 - val_loss: 0.5452 - val_accuracy: 0.8408\nEpoch 25/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5489 - accuracy: 0.8508 - val_loss: 0.5417 - val_accuracy: 0.8516\nEpoch 26/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5458 - accuracy: 0.8594 - val_loss: 0.5385 - val_accuracy: 0.8613\nEpoch 27/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.5429 - accuracy: 0.8684 - val_loss: 0.5352 - val_accuracy: 0.8633\nEpoch 28/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5399 - accuracy: 0.8706 - val_loss: 0.5320 - val_accuracy: 0.8652\nEpoch 29/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.5367 - accuracy: 0.8726 - val_loss: 0.5291 - val_accuracy: 0.8691\nEpoch 30/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.5340 - accuracy: 0.8738 - val_loss: 0.5259 - val_accuracy: 0.8701\nEpoch 31/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5309 - accuracy: 0.8748 - val_loss: 0.5229 - val_accuracy: 0.8701\nEpoch 32/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5280 - accuracy: 0.8750 - val_loss: 0.5199 - val_accuracy: 0.8701\nEpoch 33/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.5250 - accuracy: 0.8750 - val_loss: 0.5170 - val_accuracy: 0.8711\nEpoch 34/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.5220 - accuracy: 0.8755 - val_loss: 0.5144 - val_accuracy: 0.8711\nEpoch 35/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5190 - accuracy: 0.8755 - val_loss: 0.5112 - val_accuracy: 0.8711\nEpoch 36/100\n2048/2048 [==============================] - 0s 58us/sample - loss: 0.5159 - accuracy: 0.8757 - val_loss: 0.5081 - val_accuracy: 0.8711\nEpoch 37/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.5129 - accuracy: 0.8760 - val_loss: 0.5053 - val_accuracy: 0.8711\nEpoch 38/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.5097 - accuracy: 0.8757 - val_loss: 0.5024 - val_accuracy: 0.8711\nEpoch 39/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.5063 - accuracy: 0.8760 - val_loss: 0.4996 - val_accuracy: 0.8711\nEpoch 40/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5032 - accuracy: 0.8760 - val_loss: 0.4965 - val_accuracy: 0.8711\nEpoch 41/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4997 - accuracy: 0.8760 - val_loss: 0.4938 - val_accuracy: 0.8711\nEpoch 42/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.4963 - accuracy: 0.8757 - val_loss: 0.4908 - val_accuracy: 0.8711\nEpoch 43/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4928 - accuracy: 0.8757 - val_loss: 0.4870 - val_accuracy: 0.8711\nEpoch 44/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4894 - accuracy: 0.8760 - val_loss: 0.4838 - val_accuracy: 0.8711\nEpoch 45/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.4857 - accuracy: 0.8757 - val_loss: 0.4806 - val_accuracy: 0.8711\nEpoch 46/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.4819 - accuracy: 0.8760 - val_loss: 0.4774 - val_accuracy: 0.8711\nEpoch 47/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4781 - accuracy: 0.8762 - val_loss: 0.4741 - val_accuracy: 0.8711\nEpoch 48/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4742 - accuracy: 0.8762 - val_loss: 0.4707 - val_accuracy: 0.8711\nEpoch 49/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4698 - accuracy: 0.8762 - val_loss: 0.4680 - val_accuracy: 0.8711\nEpoch 50/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.4660 - accuracy: 0.8760 - val_loss: 0.4636 - val_accuracy: 0.8711\nEpoch 51/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.4617 - accuracy: 0.8760 - val_loss: 0.4601 - val_accuracy: 0.8711\nEpoch 52/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4577 - accuracy: 0.8762 - val_loss: 0.4569 - val_accuracy: 0.8711\nEpoch 53/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4533 - accuracy: 0.8762 - val_loss: 0.4533 - val_accuracy: 0.8711\nEpoch 54/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.4489 - accuracy: 0.8760 - val_loss: 0.4503 - val_accuracy: 0.8711\nEpoch 55/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.4445 - accuracy: 0.8762 - val_loss: 0.4475 - val_accuracy: 0.8701\nEpoch 56/100\n2048/2048 [==============================] - 0s 58us/sample - loss: 0.4404 - accuracy: 0.8760 - val_loss: 0.4435 - val_accuracy: 0.8701\nEpoch 57/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4356 - accuracy: 0.8760 - val_loss: 0.4404 - val_accuracy: 0.8701\nEpoch 58/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4313 - accuracy: 0.8760 - val_loss: 0.4376 - val_accuracy: 0.8701\nEpoch 59/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4271 - accuracy: 0.8762 - val_loss: 0.4335 - val_accuracy: 0.8701\nEpoch 60/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.4231 - accuracy: 0.8762 - val_loss: 0.4311 - val_accuracy: 0.8701\nEpoch 61/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4188 - accuracy: 0.8762 - val_loss: 0.4278 - val_accuracy: 0.8701\nEpoch 62/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.4149 - accuracy: 0.8760 - val_loss: 0.4246 - val_accuracy: 0.8691\nEpoch 63/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.4112 - accuracy: 0.8760 - val_loss: 0.4210 - val_accuracy: 0.8701\nEpoch 64/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.4073 - accuracy: 0.8762 - val_loss: 0.4185 - val_accuracy: 0.8691\nEpoch 65/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4036 - accuracy: 0.8760 - val_loss: 0.4161 - val_accuracy: 0.8672\nEpoch 66/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4000 - accuracy: 0.8755 - val_loss: 0.4124 - val_accuracy: 0.8701\nEpoch 67/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3967 - accuracy: 0.8752 - val_loss: 0.4095 - val_accuracy: 0.8701\nEpoch 68/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3932 - accuracy: 0.8760 - val_loss: 0.4074 - val_accuracy: 0.8682\nEpoch 69/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3900 - accuracy: 0.8762 - val_loss: 0.4051 - val_accuracy: 0.8682\nEpoch 70/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3863 - accuracy: 0.8757 - val_loss: 0.4051 - val_accuracy: 0.8633\nEpoch 71/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3841 - accuracy: 0.8750 - val_loss: 0.4003 - val_accuracy: 0.8662\nEpoch 72/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3798 - accuracy: 0.8752 - val_loss: 0.3977 - val_accuracy: 0.8701\nEpoch 73/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3781 - accuracy: 0.8752 - val_loss: 0.3958 - val_accuracy: 0.8652\nEpoch 74/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3751 - accuracy: 0.8762 - val_loss: 0.3946 - val_accuracy: 0.8643\nEpoch 75/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3725 - accuracy: 0.8760 - val_loss: 0.3921 - val_accuracy: 0.8643\nEpoch 76/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3697 - accuracy: 0.8752 - val_loss: 0.3885 - val_accuracy: 0.8701\nEpoch 77/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3678 - accuracy: 0.8760 - val_loss: 0.3866 - val_accuracy: 0.8672\nEpoch 78/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3648 - accuracy: 0.8762 - val_loss: 0.3849 - val_accuracy: 0.8672\nEpoch 79/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3627 - accuracy: 0.8770 - val_loss: 0.3838 - val_accuracy: 0.8643\nEpoch 80/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3606 - accuracy: 0.8755 - val_loss: 0.3814 - val_accuracy: 0.8711\nEpoch 81/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3590 - accuracy: 0.8765 - val_loss: 0.3797 - val_accuracy: 0.8691\nEpoch 82/100\n2048/2048 [==============================] - 0s 65us/sample - loss: 0.3561 - accuracy: 0.8760 - val_loss: 0.3788 - val_accuracy: 0.8643\nEpoch 83/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3542 - accuracy: 0.8767 - val_loss: 0.3785 - val_accuracy: 0.8643\nEpoch 84/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3526 - accuracy: 0.8760 - val_loss: 0.3754 - val_accuracy: 0.8662\nEpoch 85/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3513 - accuracy: 0.8760 - val_loss: 0.3743 - val_accuracy: 0.8652\nEpoch 86/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3496 - accuracy: 0.8772 - val_loss: 0.3726 - val_accuracy: 0.8691\nEpoch 87/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3476 - accuracy: 0.8765 - val_loss: 0.3714 - val_accuracy: 0.8672\nEpoch 88/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3463 - accuracy: 0.8767 - val_loss: 0.3706 - val_accuracy: 0.8652\nEpoch 89/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3448 - accuracy: 0.8782 - val_loss: 0.3689 - val_accuracy: 0.8701\nEpoch 90/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3432 - accuracy: 0.8777 - val_loss: 0.3692 - val_accuracy: 0.8652\nEpoch 91/100\n2048/2048 [==============================] - 0s 58us/sample - loss: 0.3416 - accuracy: 0.8772 - val_loss: 0.3671 - val_accuracy: 0.8701\nEpoch 92/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3406 - accuracy: 0.8772 - val_loss: 0.3657 - val_accuracy: 0.8691\nEpoch 93/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3379 - accuracy: 0.8787 - val_loss: 0.3710 - val_accuracy: 0.8662\nEpoch 94/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3388 - accuracy: 0.8784 - val_loss: 0.3639 - val_accuracy: 0.8682\nEpoch 95/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3363 - accuracy: 0.8794 - val_loss: 0.3630 - val_accuracy: 0.8691\nEpoch 96/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3357 - accuracy: 0.8787 - val_loss: 0.3628 - val_accuracy: 0.8662\nEpoch 97/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3341 - accuracy: 0.8794 - val_loss: 0.3619 - val_accuracy: 0.8672\nEpoch 98/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3328 - accuracy: 0.8789 - val_loss: 0.3606 - val_accuracy: 0.8691\nEpoch 99/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3314 - accuracy: 0.8811 - val_loss: 0.3605 - val_accuracy: 0.8711\nEpoch 100/100\n2048/2048 [==============================] - 0s 66us/sample - loss: 0.3313 - accuracy: 0.8799 - val_loss: 0.3593 - val_accuracy: 0.8691\n"
    }
   ],
   "source": [
    "fit_all = model_large.fit(x = x_train,\n",
    "        y = y_train, \n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        validation_split = .2,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_large.predict(x_test)\n",
    "prediction_df = pd.DataFrame(prediction, columns=['prob_present','prob_not_present']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     present  not_present  prob_present  prob_not_present       actual  \\\n0        0.0          1.0      0.136889          0.864184  not_present   \n1        0.0          1.0      0.198516          0.795845  not_present   \n2        0.0          1.0      0.162797          0.835352  not_present   \n3        1.0          0.0      0.149262          0.844731      present   \n4        0.0          1.0      0.174247          0.813663  not_present   \n..       ...          ...           ...               ...          ...   \n635      0.0          1.0      0.168411          0.835854  not_present   \n636      0.0          1.0      0.152016          0.835837  not_present   \n637      0.0          1.0      0.157270          0.847382  not_present   \n638      0.0          1.0      0.035570          0.972761  not_present   \n639      0.0          1.0      0.155034          0.860091  not_present   \n\n       predicted correct feature  \n0    not_present     yes     all  \n1    not_present     yes     all  \n2    not_present     yes     all  \n3    not_present      no     all  \n4    not_present     yes     all  \n..           ...     ...     ...  \n635  not_present     yes     all  \n636  not_present     yes     all  \n637  not_present     yes     all  \n638  not_present     yes     all  \n639  not_present     yes     all  \n\n[640 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>present</th>\n      <th>not_present</th>\n      <th>prob_present</th>\n      <th>prob_not_present</th>\n      <th>actual</th>\n      <th>predicted</th>\n      <th>correct</th>\n      <th>feature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.136889</td>\n      <td>0.864184</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>all</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.198516</td>\n      <td>0.795845</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>all</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.162797</td>\n      <td>0.835352</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>all</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.149262</td>\n      <td>0.844731</td>\n      <td>present</td>\n      <td>not_present</td>\n      <td>no</td>\n      <td>all</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.174247</td>\n      <td>0.813663</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>all</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>635</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.168411</td>\n      <td>0.835854</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>all</td>\n    </tr>\n    <tr>\n      <th>636</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.152016</td>\n      <td>0.835837</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>all</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.157270</td>\n      <td>0.847382</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>all</td>\n    </tr>\n    <tr>\n      <th>638</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.035570</td>\n      <td>0.972761</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>all</td>\n    </tr>\n    <tr>\n      <th>639</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.155034</td>\n      <td>0.860091</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>all</td>\n    </tr>\n  </tbody>\n</table>\n<p>640 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "y_test = pd.DataFrame(y_test,columns=['present','not_present'])\n",
    "\n",
    "y_pred = y_test >> bind_cols(prediction_df)\n",
    "\n",
    "y_pred >>= mutate(actual = case_when([y_pred.present == 1, 'present'],\n",
    "[y_pred.not_present == 1, 'not_present']),\n",
    "predicted = case_when([y_pred.prob_present > y_pred.prob_not_present, 'present'],\n",
    "[y_pred.prob_present < y_pred.prob_not_present, 'not_present']))\n",
    "\n",
    "y_pred >>= mutate(correct = case_when([y_pred.actual == y_pred.predicted, 'yes'],\n",
    "[y_pred.actual != y_pred.predicted, 'no']),\n",
    "feature = 'all')\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_3 (Dense)              (None, 64)                16448     \n_________________________________________________________________\ndense_4 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_5 (Dense)              (None, 2)                 66        \n=================================================================\nTotal params: 18,594\nTrainable params: 18,594\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 2048 samples, validate on 512 samples\nEpoch 1/100\n2048/2048 [==============================] - 0s 179us/sample - loss: 0.6875 - accuracy: 0.5537 - val_loss: 0.6797 - val_accuracy: 0.5996\nEpoch 2/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.6761 - accuracy: 0.5913 - val_loss: 0.6720 - val_accuracy: 0.5996\nEpoch 3/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.6695 - accuracy: 0.5913 - val_loss: 0.6665 - val_accuracy: 0.5996\nEpoch 4/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.6646 - accuracy: 0.5913 - val_loss: 0.6624 - val_accuracy: 0.5996\nEpoch 5/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.6609 - accuracy: 0.5913 - val_loss: 0.6593 - val_accuracy: 0.5996\nEpoch 6/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.6580 - accuracy: 0.5913 - val_loss: 0.6568 - val_accuracy: 0.5996\nEpoch 7/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.6555 - accuracy: 0.5913 - val_loss: 0.6546 - val_accuracy: 0.5996\nEpoch 8/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.6534 - accuracy: 0.5913 - val_loss: 0.6527 - val_accuracy: 0.5996\nEpoch 9/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.6514 - accuracy: 0.5913 - val_loss: 0.6510 - val_accuracy: 0.5996\nEpoch 10/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.6495 - accuracy: 0.5913 - val_loss: 0.6493 - val_accuracy: 0.5996\nEpoch 11/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.6477 - accuracy: 0.5913 - val_loss: 0.6477 - val_accuracy: 0.5996\nEpoch 12/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.6460 - accuracy: 0.5913 - val_loss: 0.6461 - val_accuracy: 0.5996\nEpoch 13/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.6442 - accuracy: 0.5913 - val_loss: 0.6445 - val_accuracy: 0.5996\nEpoch 14/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.6424 - accuracy: 0.5913 - val_loss: 0.6429 - val_accuracy: 0.5996\nEpoch 15/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.6406 - accuracy: 0.5913 - val_loss: 0.6413 - val_accuracy: 0.5996\nEpoch 16/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.6388 - accuracy: 0.5913 - val_loss: 0.6397 - val_accuracy: 0.5996\nEpoch 17/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.6370 - accuracy: 0.5913 - val_loss: 0.6381 - val_accuracy: 0.5996\nEpoch 18/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.6351 - accuracy: 0.5913 - val_loss: 0.6364 - val_accuracy: 0.5996\nEpoch 19/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.6332 - accuracy: 0.5913 - val_loss: 0.6348 - val_accuracy: 0.5996\nEpoch 20/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.6312 - accuracy: 0.5913 - val_loss: 0.6330 - val_accuracy: 0.5996\nEpoch 21/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.6293 - accuracy: 0.5913 - val_loss: 0.6313 - val_accuracy: 0.5996\nEpoch 22/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.6273 - accuracy: 0.5913 - val_loss: 0.6296 - val_accuracy: 0.5996\nEpoch 23/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.6253 - accuracy: 0.5913 - val_loss: 0.6279 - val_accuracy: 0.5996\nEpoch 24/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.6232 - accuracy: 0.5913 - val_loss: 0.6262 - val_accuracy: 0.5996\nEpoch 25/100\n1088/2048 [==============>...............] - ETA: 0s - loss: 0.6237 - accuracy: 0.592048/2048 [==============================] - 0s 55us/sample - loss: 0.6212 - accuracy: 0.5925 - val_loss: 0.6245 - val_accuracy: 0.6045\nEpoch 26/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.6191 - accuracy: 0.6016 - val_loss: 0.6228 - val_accuracy: 0.6367\nEpoch 27/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.6171 - accuracy: 0.6335 - val_loss: 0.6212 - val_accuracy: 0.6963\nEpoch 28/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.6151 - accuracy: 0.6809 - val_loss: 0.6196 - val_accuracy: 0.7256\nEpoch 29/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.6132 - accuracy: 0.7080 - val_loss: 0.6181 - val_accuracy: 0.7314\nEpoch 30/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.6113 - accuracy: 0.7161 - val_loss: 0.6167 - val_accuracy: 0.7363\nEpoch 31/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.6095 - accuracy: 0.7227 - val_loss: 0.6154 - val_accuracy: 0.7383\nEpoch 32/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.6079 - accuracy: 0.7283 - val_loss: 0.6142 - val_accuracy: 0.7578\nEpoch 33/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.6062 - accuracy: 0.7422 - val_loss: 0.6131 - val_accuracy: 0.8066\nEpoch 34/100\n2048/2048 [==============================] - 0s 61us/sample - loss: 0.6048 - accuracy: 0.7827 - val_loss: 0.6119 - val_accuracy: 0.8076\nEpoch 35/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.6033 - accuracy: 0.8066 - val_loss: 0.6108 - val_accuracy: 0.8252\nEpoch 36/100\n2048/2048 [==============================] - 0s 66us/sample - loss: 0.6019 - accuracy: 0.8262 - val_loss: 0.6098 - val_accuracy: 0.8477\nEpoch 37/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.6005 - accuracy: 0.8354 - val_loss: 0.6088 - val_accuracy: 0.8682\nEpoch 38/100\n2048/2048 [==============================] - 0s 66us/sample - loss: 0.5992 - accuracy: 0.8435 - val_loss: 0.6078 - val_accuracy: 0.8760\nEpoch 39/100\n2048/2048 [==============================] - 0s 68us/sample - loss: 0.5978 - accuracy: 0.8523 - val_loss: 0.6069 - val_accuracy: 0.8818\nEpoch 40/100\n2048/2048 [==============================] - 0s 63us/sample - loss: 0.5966 - accuracy: 0.8572 - val_loss: 0.6058 - val_accuracy: 0.8818\nEpoch 41/100\n2048/2048 [==============================] - 0s 63us/sample - loss: 0.5954 - accuracy: 0.8608 - val_loss: 0.6049 - val_accuracy: 0.8838\nEpoch 42/100\n2048/2048 [==============================] - 0s 68us/sample - loss: 0.5941 - accuracy: 0.8625 - val_loss: 0.6039 - val_accuracy: 0.8867\nEpoch 43/100\n2048/2048 [==============================] - 0s 64us/sample - loss: 0.5929 - accuracy: 0.8660 - val_loss: 0.6029 - val_accuracy: 0.8867\nEpoch 44/100\n2048/2048 [==============================] - 0s 63us/sample - loss: 0.5917 - accuracy: 0.8674 - val_loss: 0.6019 - val_accuracy: 0.8877\nEpoch 45/100\n2048/2048 [==============================] - 0s 66us/sample - loss: 0.5905 - accuracy: 0.8669 - val_loss: 0.6009 - val_accuracy: 0.8877\nEpoch 46/100\n2048/2048 [==============================] - 0s 66us/sample - loss: 0.5892 - accuracy: 0.8679 - val_loss: 0.5999 - val_accuracy: 0.8877\nEpoch 47/100\n2048/2048 [==============================] - 0s 73us/sample - loss: 0.5880 - accuracy: 0.8679 - val_loss: 0.5988 - val_accuracy: 0.8887\nEpoch 48/100\n2048/2048 [==============================] - 0s 69us/sample - loss: 0.5868 - accuracy: 0.8682 - val_loss: 0.5978 - val_accuracy: 0.8887\nEpoch 49/100\n2048/2048 [==============================] - 0s 73us/sample - loss: 0.5855 - accuracy: 0.8682 - val_loss: 0.5967 - val_accuracy: 0.8887\nEpoch 50/100\n2048/2048 [==============================] - 0s 63us/sample - loss: 0.5843 - accuracy: 0.8682 - val_loss: 0.5956 - val_accuracy: 0.8887\nEpoch 51/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5830 - accuracy: 0.8682 - val_loss: 0.5947 - val_accuracy: 0.8887\nEpoch 52/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.5820 - accuracy: 0.8682 - val_loss: 0.5933 - val_accuracy: 0.8887\nEpoch 53/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.5806 - accuracy: 0.8682 - val_loss: 0.5922 - val_accuracy: 0.8887\nEpoch 54/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.5794 - accuracy: 0.8682 - val_loss: 0.5910 - val_accuracy: 0.8887\nEpoch 55/100\n2048/2048 [==============================] - 0s 58us/sample - loss: 0.5781 - accuracy: 0.8682 - val_loss: 0.5898 - val_accuracy: 0.8887\nEpoch 56/100\n2048/2048 [==============================] - 0s 63us/sample - loss: 0.5768 - accuracy: 0.8682 - val_loss: 0.5884 - val_accuracy: 0.8887\nEpoch 57/100\n2048/2048 [==============================] - 0s 64us/sample - loss: 0.5755 - accuracy: 0.8682 - val_loss: 0.5871 - val_accuracy: 0.8887\nEpoch 58/100\n2048/2048 [==============================] - 0s 63us/sample - loss: 0.5742 - accuracy: 0.8682 - val_loss: 0.5859 - val_accuracy: 0.8887\nEpoch 59/100\n2048/2048 [==============================] - 0s 63us/sample - loss: 0.5729 - accuracy: 0.8682 - val_loss: 0.5844 - val_accuracy: 0.8887\nEpoch 60/100\n2048/2048 [==============================] - 0s 63us/sample - loss: 0.5715 - accuracy: 0.8682 - val_loss: 0.5831 - val_accuracy: 0.8887\nEpoch 61/100\n2048/2048 [==============================] - 0s 71us/sample - loss: 0.5701 - accuracy: 0.8682 - val_loss: 0.5815 - val_accuracy: 0.8887\nEpoch 62/100\n2048/2048 [==============================] - 0s 71us/sample - loss: 0.5687 - accuracy: 0.8682 - val_loss: 0.5800 - val_accuracy: 0.8887\nEpoch 63/100\n2048/2048 [==============================] - 0s 73us/sample - loss: 0.5673 - accuracy: 0.8682 - val_loss: 0.5784 - val_accuracy: 0.8887\nEpoch 64/100\n2048/2048 [==============================] - 0s 82us/sample - loss: 0.5658 - accuracy: 0.8682 - val_loss: 0.5768 - val_accuracy: 0.8887\nEpoch 65/100\n2048/2048 [==============================] - 0s 76us/sample - loss: 0.5642 - accuracy: 0.8682 - val_loss: 0.5750 - val_accuracy: 0.8887\nEpoch 66/100\n2048/2048 [==============================] - 0s 69us/sample - loss: 0.5628 - accuracy: 0.8682 - val_loss: 0.5733 - val_accuracy: 0.8887\nEpoch 67/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.5612 - accuracy: 0.8682 - val_loss: 0.5714 - val_accuracy: 0.8887\nEpoch 68/100\n2048/2048 [==============================] - 0s 68us/sample - loss: 0.5595 - accuracy: 0.8682 - val_loss: 0.5695 - val_accuracy: 0.8887\nEpoch 69/100\n2048/2048 [==============================] - 0s 74us/sample - loss: 0.5578 - accuracy: 0.8682 - val_loss: 0.5676 - val_accuracy: 0.8887\nEpoch 70/100\n2048/2048 [==============================] - 0s 67us/sample - loss: 0.5562 - accuracy: 0.8682 - val_loss: 0.5655 - val_accuracy: 0.8887\nEpoch 71/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.5545 - accuracy: 0.8682 - val_loss: 0.5635 - val_accuracy: 0.8887\nEpoch 72/100\n2048/2048 [==============================] - 0s 61us/sample - loss: 0.5526 - accuracy: 0.8682 - val_loss: 0.5614 - val_accuracy: 0.8887\nEpoch 73/100\n2048/2048 [==============================] - 0s 74us/sample - loss: 0.5508 - accuracy: 0.8682 - val_loss: 0.5590 - val_accuracy: 0.8887\nEpoch 74/100\n2048/2048 [==============================] - 0s 72us/sample - loss: 0.5489 - accuracy: 0.8682 - val_loss: 0.5567 - val_accuracy: 0.8887\nEpoch 75/100\n2048/2048 [==============================] - 0s 68us/sample - loss: 0.5470 - accuracy: 0.8682 - val_loss: 0.5543 - val_accuracy: 0.8887\nEpoch 76/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.5449 - accuracy: 0.8682 - val_loss: 0.5518 - val_accuracy: 0.8887\nEpoch 77/100\n2048/2048 [==============================] - 0s 75us/sample - loss: 0.5429 - accuracy: 0.8682 - val_loss: 0.5492 - val_accuracy: 0.8887\nEpoch 78/100\n2048/2048 [==============================] - 0s 69us/sample - loss: 0.5407 - accuracy: 0.8682 - val_loss: 0.5465 - val_accuracy: 0.8887\nEpoch 79/100\n2048/2048 [==============================] - 0s 71us/sample - loss: 0.5386 - accuracy: 0.8682 - val_loss: 0.5438 - val_accuracy: 0.8887\nEpoch 80/100\n2048/2048 [==============================] - 0s 70us/sample - loss: 0.5365 - accuracy: 0.8682 - val_loss: 0.5410 - val_accuracy: 0.8887\nEpoch 81/100\n2048/2048 [==============================] - 0s 69us/sample - loss: 0.5341 - accuracy: 0.8682 - val_loss: 0.5380 - val_accuracy: 0.8887\nEpoch 82/100\n2048/2048 [==============================] - 0s 67us/sample - loss: 0.5316 - accuracy: 0.8682 - val_loss: 0.5351 - val_accuracy: 0.8887\nEpoch 83/100\n2048/2048 [==============================] - 0s 65us/sample - loss: 0.5293 - accuracy: 0.8682 - val_loss: 0.5319 - val_accuracy: 0.8887\nEpoch 84/100\n2048/2048 [==============================] - 0s 74us/sample - loss: 0.5268 - accuracy: 0.8682 - val_loss: 0.5287 - val_accuracy: 0.8887\nEpoch 85/100\n2048/2048 [==============================] - 0s 65us/sample - loss: 0.5243 - accuracy: 0.8682 - val_loss: 0.5252 - val_accuracy: 0.8887\nEpoch 86/100\n2048/2048 [==============================] - 0s 68us/sample - loss: 0.5216 - accuracy: 0.8682 - val_loss: 0.5217 - val_accuracy: 0.8887\nEpoch 87/100\n2048/2048 [==============================] - 0s 66us/sample - loss: 0.5190 - accuracy: 0.8682 - val_loss: 0.5181 - val_accuracy: 0.8887\nEpoch 88/100\n2048/2048 [==============================] - 0s 68us/sample - loss: 0.5162 - accuracy: 0.8682 - val_loss: 0.5146 - val_accuracy: 0.8887\nEpoch 89/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.5135 - accuracy: 0.8682 - val_loss: 0.5108 - val_accuracy: 0.8887\nEpoch 90/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5105 - accuracy: 0.8682 - val_loss: 0.5071 - val_accuracy: 0.8887\nEpoch 91/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.5076 - accuracy: 0.8682 - val_loss: 0.5030 - val_accuracy: 0.8887\nEpoch 92/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.5044 - accuracy: 0.8682 - val_loss: 0.4991 - val_accuracy: 0.8887\nEpoch 93/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.5017 - accuracy: 0.8682 - val_loss: 0.4949 - val_accuracy: 0.8887\nEpoch 94/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4984 - accuracy: 0.8682 - val_loss: 0.4910 - val_accuracy: 0.8887\nEpoch 95/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4953 - accuracy: 0.8682 - val_loss: 0.4869 - val_accuracy: 0.8887\nEpoch 96/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4921 - accuracy: 0.8682 - val_loss: 0.4830 - val_accuracy: 0.8887\nEpoch 97/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.4891 - accuracy: 0.8682 - val_loss: 0.4787 - val_accuracy: 0.8887\nEpoch 98/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.4858 - accuracy: 0.8682 - val_loss: 0.4749 - val_accuracy: 0.8887\nEpoch 99/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.4829 - accuracy: 0.8682 - val_loss: 0.4709 - val_accuracy: 0.8887\nEpoch 100/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.4795 - accuracy: 0.8682 - val_loss: 0.4667 - val_accuracy: 0.8887\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     present  not_present  prob_present  prob_not_present       actual  \\\n0        0.0          1.0      0.237451          0.739642  not_present   \n1        1.0          0.0      0.716398          0.334916      present   \n2        0.0          1.0      0.399759          0.590905  not_present   \n3        0.0          1.0      0.264118          0.708033  not_present   \n4        0.0          1.0      0.212785          0.776766  not_present   \n..       ...          ...           ...               ...          ...   \n635      1.0          0.0      0.278924          0.807757      present   \n636      1.0          0.0      0.731530          0.322220      present   \n637      1.0          0.0      0.663550          0.380602      present   \n638      0.0          1.0      0.284572          0.688631  not_present   \n639      0.0          1.0      0.228096          0.762693  not_present   \n\n       predicted correct feature  \n0    not_present     yes   imf_1  \n1        present     yes   imf_1  \n2    not_present     yes   imf_1  \n3    not_present     yes   imf_1  \n4    not_present     yes   imf_1  \n..           ...     ...     ...  \n635  not_present      no   imf_1  \n636      present     yes   imf_1  \n637      present     yes   imf_1  \n638  not_present     yes   imf_1  \n639  not_present     yes   imf_1  \n\n[640 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>present</th>\n      <th>not_present</th>\n      <th>prob_present</th>\n      <th>prob_not_present</th>\n      <th>actual</th>\n      <th>predicted</th>\n      <th>correct</th>\n      <th>feature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.237451</td>\n      <td>0.739642</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.716398</td>\n      <td>0.334916</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.399759</td>\n      <td>0.590905</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.264118</td>\n      <td>0.708033</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.212785</td>\n      <td>0.776766</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>635</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.278924</td>\n      <td>0.807757</td>\n      <td>present</td>\n      <td>not_present</td>\n      <td>no</td>\n      <td>imf_1</td>\n    </tr>\n    <tr>\n      <th>636</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.731530</td>\n      <td>0.322220</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_1</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.663550</td>\n      <td>0.380602</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_1</td>\n    </tr>\n    <tr>\n      <th>638</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.284572</td>\n      <td>0.688631</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_1</td>\n    </tr>\n    <tr>\n      <th>639</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.228096</td>\n      <td>0.762693</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_1</td>\n    </tr>\n  </tbody>\n</table>\n<p>640 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "small_df = df[df.columns[0:256]]\n",
    "small_df >>= bind_cols(df.drone_present)\n",
    "\n",
    "presence_labs = small_df['drone_present']\n",
    "\n",
    "values = np.array(presence_labs)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "x = small_df.drop(['drone_present'],axis=1).values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "model_small = keras.Sequential(name='test')\n",
    "model_small.add(Dense(64, activation='relu', input_shape=(256,)))\n",
    "model_small.add(Dense(32, activation='relu'))\n",
    "model_small.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model_small.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_small.build(input_shape=(256,))\n",
    "model_small.summary()\n",
    "\n",
    "fit1 = model_small.fit(x = x_train,\n",
    "        y = y_train, \n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        validation_split = .2,\n",
    "        verbose=True)\n",
    "\n",
    "prediction = model_small.predict(x_test)\n",
    "prediction_df = pd.DataFrame(prediction, columns=['prob_present','prob_not_present']) \n",
    "\n",
    "y_test = pd.DataFrame(y_test,columns=['present','not_present'])\n",
    "\n",
    "y_pred = y_test >> bind_cols(prediction_df)\n",
    "\n",
    "y_pred >>= mutate(actual = case_when([y_pred.present == 1, 'present'],\n",
    "[y_pred.not_present == 1, 'not_present']),\n",
    "predicted = case_when([y_pred.prob_present > y_pred.prob_not_present, 'present'],\n",
    "[y_pred.prob_present < y_pred.prob_not_present, 'not_present']))\n",
    "\n",
    "y_pred >>= mutate(correct = case_when([y_pred.actual == y_pred.predicted, 'yes'],\n",
    "[y_pred.actual != y_pred.predicted, 'no']),\n",
    "feature = 'imf_1')\n",
    "y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 2048 samples, validate on 512 samples\nEpoch 1/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.5279 - accuracy: 0.8411 - val_loss: 0.5069 - val_accuracy: 0.8271\nEpoch 2/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4993 - accuracy: 0.8406 - val_loss: 0.4939 - val_accuracy: 0.8281\nEpoch 3/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4836 - accuracy: 0.8467 - val_loss: 0.4858 - val_accuracy: 0.8359\nEpoch 4/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.4730 - accuracy: 0.8586 - val_loss: 0.4784 - val_accuracy: 0.8457\nEpoch 5/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4648 - accuracy: 0.8623 - val_loss: 0.4733 - val_accuracy: 0.8496\nEpoch 6/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4582 - accuracy: 0.8669 - val_loss: 0.4688 - val_accuracy: 0.8496\nEpoch 7/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4527 - accuracy: 0.8696 - val_loss: 0.4654 - val_accuracy: 0.8496\nEpoch 8/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4482 - accuracy: 0.8708 - val_loss: 0.4618 - val_accuracy: 0.8516\nEpoch 9/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4440 - accuracy: 0.8711 - val_loss: 0.4591 - val_accuracy: 0.8525\nEpoch 10/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.4405 - accuracy: 0.8716 - val_loss: 0.4563 - val_accuracy: 0.8525\nEpoch 11/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4369 - accuracy: 0.8721 - val_loss: 0.4536 - val_accuracy: 0.8525\nEpoch 12/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4338 - accuracy: 0.8718 - val_loss: 0.4511 - val_accuracy: 0.8525\nEpoch 13/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4312 - accuracy: 0.8723 - val_loss: 0.4491 - val_accuracy: 0.8516\nEpoch 14/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4282 - accuracy: 0.8718 - val_loss: 0.4474 - val_accuracy: 0.8506\nEpoch 15/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4257 - accuracy: 0.8728 - val_loss: 0.4453 - val_accuracy: 0.8506\nEpoch 16/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.4229 - accuracy: 0.8721 - val_loss: 0.4432 - val_accuracy: 0.8535\nEpoch 17/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4210 - accuracy: 0.8726 - val_loss: 0.4416 - val_accuracy: 0.8516\nEpoch 18/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4188 - accuracy: 0.8726 - val_loss: 0.4402 - val_accuracy: 0.8516\nEpoch 19/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4166 - accuracy: 0.8723 - val_loss: 0.4385 - val_accuracy: 0.8516\nEpoch 20/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4146 - accuracy: 0.8726 - val_loss: 0.4360 - val_accuracy: 0.8516\nEpoch 21/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4126 - accuracy: 0.8728 - val_loss: 0.4345 - val_accuracy: 0.8516\nEpoch 22/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4100 - accuracy: 0.8726 - val_loss: 0.4333 - val_accuracy: 0.8516\nEpoch 23/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.4086 - accuracy: 0.8728 - val_loss: 0.4319 - val_accuracy: 0.8516\nEpoch 24/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.4066 - accuracy: 0.8726 - val_loss: 0.4305 - val_accuracy: 0.8516\nEpoch 25/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4049 - accuracy: 0.8718 - val_loss: 0.4285 - val_accuracy: 0.8516\nEpoch 26/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4028 - accuracy: 0.8728 - val_loss: 0.4274 - val_accuracy: 0.8516\nEpoch 27/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.4015 - accuracy: 0.8723 - val_loss: 0.4257 - val_accuracy: 0.8516\nEpoch 28/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3991 - accuracy: 0.8730 - val_loss: 0.4250 - val_accuracy: 0.8516\nEpoch 29/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3978 - accuracy: 0.8718 - val_loss: 0.4232 - val_accuracy: 0.8516\nEpoch 30/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3962 - accuracy: 0.8721 - val_loss: 0.4219 - val_accuracy: 0.8516\nEpoch 31/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3945 - accuracy: 0.8726 - val_loss: 0.4207 - val_accuracy: 0.8516\nEpoch 32/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3932 - accuracy: 0.8721 - val_loss: 0.4193 - val_accuracy: 0.8516\nEpoch 33/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3916 - accuracy: 0.8723 - val_loss: 0.4181 - val_accuracy: 0.8516\nEpoch 34/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3901 - accuracy: 0.8713 - val_loss: 0.4168 - val_accuracy: 0.8516\nEpoch 35/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3878 - accuracy: 0.8728 - val_loss: 0.4170 - val_accuracy: 0.8525\nEpoch 36/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3866 - accuracy: 0.8728 - val_loss: 0.4154 - val_accuracy: 0.8506\nEpoch 37/100\n2048/2048 [==============================] - 0s 79us/sample - loss: 0.3859 - accuracy: 0.8723 - val_loss: 0.4137 - val_accuracy: 0.8516\nEpoch 38/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3841 - accuracy: 0.8728 - val_loss: 0.4147 - val_accuracy: 0.8496\nEpoch 39/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3832 - accuracy: 0.8723 - val_loss: 0.4124 - val_accuracy: 0.8506\nEpoch 40/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3807 - accuracy: 0.8716 - val_loss: 0.4112 - val_accuracy: 0.8516\nEpoch 41/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3807 - accuracy: 0.8723 - val_loss: 0.4101 - val_accuracy: 0.8506\nEpoch 42/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3793 - accuracy: 0.8721 - val_loss: 0.4085 - val_accuracy: 0.8516\nEpoch 43/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3785 - accuracy: 0.8721 - val_loss: 0.4075 - val_accuracy: 0.8516\nEpoch 44/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3768 - accuracy: 0.8723 - val_loss: 0.4075 - val_accuracy: 0.8496\nEpoch 45/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3761 - accuracy: 0.8718 - val_loss: 0.4061 - val_accuracy: 0.8516\nEpoch 46/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3746 - accuracy: 0.8716 - val_loss: 0.4053 - val_accuracy: 0.8516\nEpoch 47/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3734 - accuracy: 0.8721 - val_loss: 0.4045 - val_accuracy: 0.8506\nEpoch 48/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3725 - accuracy: 0.8721 - val_loss: 0.4037 - val_accuracy: 0.8506\nEpoch 49/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3718 - accuracy: 0.8721 - val_loss: 0.4027 - val_accuracy: 0.8516\nEpoch 50/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3703 - accuracy: 0.8721 - val_loss: 0.4036 - val_accuracy: 0.8496\nEpoch 51/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3693 - accuracy: 0.8726 - val_loss: 0.4037 - val_accuracy: 0.8486\nEpoch 52/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3688 - accuracy: 0.8723 - val_loss: 0.4014 - val_accuracy: 0.8496\nEpoch 53/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3679 - accuracy: 0.8721 - val_loss: 0.4007 - val_accuracy: 0.8496\nEpoch 54/100\n2048/2048 [==============================] - 0s 67us/sample - loss: 0.3668 - accuracy: 0.8726 - val_loss: 0.3994 - val_accuracy: 0.8506\nEpoch 55/100\n2048/2048 [==============================] - 0s 74us/sample - loss: 0.3660 - accuracy: 0.8718 - val_loss: 0.3996 - val_accuracy: 0.8496\nEpoch 56/100\n2048/2048 [==============================] - 0s 58us/sample - loss: 0.3651 - accuracy: 0.8711 - val_loss: 0.3982 - val_accuracy: 0.8516\nEpoch 57/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3643 - accuracy: 0.8730 - val_loss: 0.3977 - val_accuracy: 0.8506\nEpoch 58/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3635 - accuracy: 0.8721 - val_loss: 0.3975 - val_accuracy: 0.8496\nEpoch 59/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3627 - accuracy: 0.8718 - val_loss: 0.3967 - val_accuracy: 0.8516\nEpoch 60/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3622 - accuracy: 0.8721 - val_loss: 0.3960 - val_accuracy: 0.8506\nEpoch 61/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3617 - accuracy: 0.8721 - val_loss: 0.3966 - val_accuracy: 0.8486\nEpoch 62/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3605 - accuracy: 0.8713 - val_loss: 0.3948 - val_accuracy: 0.8496\nEpoch 63/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3598 - accuracy: 0.8721 - val_loss: 0.3943 - val_accuracy: 0.8496\nEpoch 64/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3593 - accuracy: 0.8726 - val_loss: 0.3942 - val_accuracy: 0.8496\nEpoch 65/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3587 - accuracy: 0.8721 - val_loss: 0.3931 - val_accuracy: 0.8516\nEpoch 66/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3576 - accuracy: 0.8718 - val_loss: 0.3929 - val_accuracy: 0.8496\nEpoch 67/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3564 - accuracy: 0.8723 - val_loss: 0.3945 - val_accuracy: 0.8496\nEpoch 68/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3567 - accuracy: 0.8718 - val_loss: 0.3925 - val_accuracy: 0.8486\nEpoch 69/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3557 - accuracy: 0.8728 - val_loss: 0.3916 - val_accuracy: 0.8516\nEpoch 70/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3550 - accuracy: 0.8728 - val_loss: 0.3940 - val_accuracy: 0.8486\nEpoch 71/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3547 - accuracy: 0.8723 - val_loss: 0.3909 - val_accuracy: 0.8516\nEpoch 72/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3539 - accuracy: 0.8713 - val_loss: 0.3900 - val_accuracy: 0.8496\nEpoch 73/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3538 - accuracy: 0.8733 - val_loss: 0.3898 - val_accuracy: 0.8496\nEpoch 74/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3528 - accuracy: 0.8721 - val_loss: 0.3906 - val_accuracy: 0.8506\nEpoch 75/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3527 - accuracy: 0.8728 - val_loss: 0.3893 - val_accuracy: 0.8496\nEpoch 76/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3519 - accuracy: 0.8728 - val_loss: 0.3891 - val_accuracy: 0.8496\nEpoch 77/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3516 - accuracy: 0.8735 - val_loss: 0.3884 - val_accuracy: 0.8496\nEpoch 78/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3509 - accuracy: 0.8735 - val_loss: 0.3897 - val_accuracy: 0.8496\nEpoch 79/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3505 - accuracy: 0.8723 - val_loss: 0.3879 - val_accuracy: 0.8496\nEpoch 80/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3506 - accuracy: 0.8730 - val_loss: 0.3878 - val_accuracy: 0.8496\nEpoch 81/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3496 - accuracy: 0.8735 - val_loss: 0.3882 - val_accuracy: 0.8506\nEpoch 82/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3493 - accuracy: 0.8726 - val_loss: 0.3886 - val_accuracy: 0.8496\nEpoch 83/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3490 - accuracy: 0.8730 - val_loss: 0.3879 - val_accuracy: 0.8496\nEpoch 84/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3482 - accuracy: 0.8730 - val_loss: 0.3889 - val_accuracy: 0.8496\nEpoch 85/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3479 - accuracy: 0.8733 - val_loss: 0.3865 - val_accuracy: 0.8496\nEpoch 86/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3475 - accuracy: 0.8728 - val_loss: 0.3861 - val_accuracy: 0.8496\nEpoch 87/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3466 - accuracy: 0.8728 - val_loss: 0.3858 - val_accuracy: 0.8496\nEpoch 88/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3467 - accuracy: 0.8733 - val_loss: 0.3857 - val_accuracy: 0.8496\nEpoch 89/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3463 - accuracy: 0.8730 - val_loss: 0.3853 - val_accuracy: 0.8496\nEpoch 90/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3462 - accuracy: 0.8730 - val_loss: 0.3854 - val_accuracy: 0.8506\nEpoch 91/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3458 - accuracy: 0.8723 - val_loss: 0.3847 - val_accuracy: 0.8496\nEpoch 92/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3453 - accuracy: 0.8740 - val_loss: 0.3846 - val_accuracy: 0.8506\nEpoch 93/100\n2048/2048 [==============================] - 0s 61us/sample - loss: 0.3441 - accuracy: 0.8743 - val_loss: 0.3843 - val_accuracy: 0.8506\nEpoch 94/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3442 - accuracy: 0.8748 - val_loss: 0.3841 - val_accuracy: 0.8496\nEpoch 95/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3439 - accuracy: 0.8743 - val_loss: 0.3841 - val_accuracy: 0.8506\nEpoch 96/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3434 - accuracy: 0.8743 - val_loss: 0.3836 - val_accuracy: 0.8506\nEpoch 97/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3432 - accuracy: 0.8728 - val_loss: 0.3844 - val_accuracy: 0.8496\nEpoch 98/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3428 - accuracy: 0.8738 - val_loss: 0.3841 - val_accuracy: 0.8496\nEpoch 99/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3419 - accuracy: 0.8735 - val_loss: 0.3835 - val_accuracy: 0.8496\nEpoch 100/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3421 - accuracy: 0.8730 - val_loss: 0.3852 - val_accuracy: 0.8496\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     present  not_present  prob_present  prob_not_present       actual  \\\n0        0.0          1.0      0.168916          0.824937  not_present   \n1        0.0          1.0      0.146301          0.850568  not_present   \n2        1.0          0.0      0.976649          0.035136      present   \n3        0.0          1.0      0.196313          0.811291  not_present   \n4        1.0          0.0      0.972681          0.039702      present   \n..       ...          ...           ...               ...          ...   \n635      0.0          1.0      0.975785          0.051501  not_present   \n636      0.0          1.0      0.167533          0.820990  not_present   \n637      0.0          1.0      0.160948          0.821174  not_present   \n638      1.0          0.0      0.168828          0.822900      present   \n639      0.0          1.0      0.201541          0.788963  not_present   \n\n       predicted correct feature  \n0    not_present     yes   imf_2  \n1    not_present     yes   imf_2  \n2        present     yes   imf_2  \n3    not_present     yes   imf_2  \n4        present     yes   imf_2  \n..           ...     ...     ...  \n635      present      no   imf_2  \n636  not_present     yes   imf_2  \n637  not_present     yes   imf_2  \n638  not_present      no   imf_2  \n639  not_present     yes   imf_2  \n\n[640 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>present</th>\n      <th>not_present</th>\n      <th>prob_present</th>\n      <th>prob_not_present</th>\n      <th>actual</th>\n      <th>predicted</th>\n      <th>correct</th>\n      <th>feature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.168916</td>\n      <td>0.824937</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.146301</td>\n      <td>0.850568</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.976649</td>\n      <td>0.035136</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.196313</td>\n      <td>0.811291</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.972681</td>\n      <td>0.039702</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>635</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.975785</td>\n      <td>0.051501</td>\n      <td>not_present</td>\n      <td>present</td>\n      <td>no</td>\n      <td>imf_2</td>\n    </tr>\n    <tr>\n      <th>636</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.167533</td>\n      <td>0.820990</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_2</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.160948</td>\n      <td>0.821174</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_2</td>\n    </tr>\n    <tr>\n      <th>638</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.168828</td>\n      <td>0.822900</td>\n      <td>present</td>\n      <td>not_present</td>\n      <td>no</td>\n      <td>imf_2</td>\n    </tr>\n    <tr>\n      <th>639</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.201541</td>\n      <td>0.788963</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_2</td>\n    </tr>\n  </tbody>\n</table>\n<p>640 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "small_df = df[df.columns[256:512]]\n",
    "small_df >>= bind_cols(df.drone_present)\n",
    "\n",
    "presence_labs = small_df['drone_present']\n",
    "\n",
    "values = np.array(presence_labs)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "x = small_df.drop(['drone_present'],axis=1).values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "fit2 = model_small.fit(x = x_train,\n",
    "        y = y_train, \n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        validation_split = .2,\n",
    "        verbose=True)\n",
    "\n",
    "prediction = model_small.predict(x_test)\n",
    "prediction_df = pd.DataFrame(prediction, columns=['prob_present','prob_not_present']) \n",
    "\n",
    "y_test = pd.DataFrame(y_test,columns=['present','not_present'])\n",
    "\n",
    "y_pred = y_test >> bind_cols(prediction_df)\n",
    "\n",
    "y_pred >>= mutate(actual = case_when([y_pred.present == 1, 'present'],\n",
    "[y_pred.not_present == 1, 'not_present']),\n",
    "predicted = case_when([y_pred.prob_present > y_pred.prob_not_present, 'present'],\n",
    "[y_pred.prob_present < y_pred.prob_not_present, 'not_present']))\n",
    "\n",
    "y_pred >>= mutate(correct = case_when([y_pred.actual == y_pred.predicted, 'yes'],\n",
    "[y_pred.actual != y_pred.predicted, 'no']),\n",
    "feature = 'imf_2')\n",
    "y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 2048 samples, validate on 512 samples\nEpoch 1/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4351 - accuracy: 0.8137 - val_loss: 0.4021 - val_accuracy: 0.8242\nEpoch 2/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.4024 - accuracy: 0.8416 - val_loss: 0.3965 - val_accuracy: 0.8428\nEpoch 3/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3978 - accuracy: 0.8486 - val_loss: 0.3938 - val_accuracy: 0.8428\nEpoch 4/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3964 - accuracy: 0.8481 - val_loss: 0.3922 - val_accuracy: 0.8438\nEpoch 5/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3936 - accuracy: 0.8491 - val_loss: 0.3904 - val_accuracy: 0.8438\nEpoch 6/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3927 - accuracy: 0.8499 - val_loss: 0.3892 - val_accuracy: 0.8447\nEpoch 7/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3903 - accuracy: 0.8516 - val_loss: 0.3941 - val_accuracy: 0.8389\nEpoch 8/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3885 - accuracy: 0.8499 - val_loss: 0.3884 - val_accuracy: 0.8438\nEpoch 9/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3869 - accuracy: 0.8518 - val_loss: 0.3852 - val_accuracy: 0.8486\nEpoch 10/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3854 - accuracy: 0.8528 - val_loss: 0.3836 - val_accuracy: 0.8477\nEpoch 11/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3836 - accuracy: 0.8535 - val_loss: 0.3827 - val_accuracy: 0.8486\nEpoch 12/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3823 - accuracy: 0.8545 - val_loss: 0.3816 - val_accuracy: 0.8506\nEpoch 13/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3814 - accuracy: 0.8540 - val_loss: 0.3820 - val_accuracy: 0.8467\nEpoch 14/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3793 - accuracy: 0.8550 - val_loss: 0.3811 - val_accuracy: 0.8486\nEpoch 15/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3790 - accuracy: 0.8550 - val_loss: 0.3791 - val_accuracy: 0.8525\nEpoch 16/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3773 - accuracy: 0.8547 - val_loss: 0.3780 - val_accuracy: 0.8516\nEpoch 17/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3767 - accuracy: 0.8552 - val_loss: 0.3793 - val_accuracy: 0.8516\nEpoch 18/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3754 - accuracy: 0.8569 - val_loss: 0.3766 - val_accuracy: 0.8516\nEpoch 19/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3750 - accuracy: 0.8567 - val_loss: 0.3756 - val_accuracy: 0.8555\nEpoch 20/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3738 - accuracy: 0.8579 - val_loss: 0.3765 - val_accuracy: 0.8525\nEpoch 21/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3725 - accuracy: 0.8579 - val_loss: 0.3765 - val_accuracy: 0.8525\nEpoch 22/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3717 - accuracy: 0.8574 - val_loss: 0.3795 - val_accuracy: 0.8525\nEpoch 23/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3708 - accuracy: 0.8582 - val_loss: 0.3755 - val_accuracy: 0.8535\nEpoch 24/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3695 - accuracy: 0.8582 - val_loss: 0.3732 - val_accuracy: 0.8545\nEpoch 25/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3692 - accuracy: 0.8577 - val_loss: 0.3798 - val_accuracy: 0.8525\nEpoch 26/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3697 - accuracy: 0.8579 - val_loss: 0.3748 - val_accuracy: 0.8545\nEpoch 27/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3681 - accuracy: 0.8579 - val_loss: 0.3739 - val_accuracy: 0.8535\nEpoch 28/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3668 - accuracy: 0.8574 - val_loss: 0.3709 - val_accuracy: 0.8564\nEpoch 29/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3661 - accuracy: 0.8579 - val_loss: 0.3750 - val_accuracy: 0.8555\nEpoch 30/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3653 - accuracy: 0.8582 - val_loss: 0.3700 - val_accuracy: 0.8555\nEpoch 31/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3651 - accuracy: 0.8582 - val_loss: 0.3762 - val_accuracy: 0.8535\nEpoch 32/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3647 - accuracy: 0.8589 - val_loss: 0.3722 - val_accuracy: 0.8555\nEpoch 33/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3642 - accuracy: 0.8596 - val_loss: 0.3690 - val_accuracy: 0.8574\nEpoch 34/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3631 - accuracy: 0.8591 - val_loss: 0.3684 - val_accuracy: 0.8564\nEpoch 35/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3629 - accuracy: 0.8584 - val_loss: 0.3702 - val_accuracy: 0.8564\nEpoch 36/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3629 - accuracy: 0.8589 - val_loss: 0.3693 - val_accuracy: 0.8574\nEpoch 37/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3621 - accuracy: 0.8582 - val_loss: 0.3707 - val_accuracy: 0.8564\nEpoch 38/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3614 - accuracy: 0.8599 - val_loss: 0.3681 - val_accuracy: 0.8574\nEpoch 39/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3603 - accuracy: 0.8589 - val_loss: 0.3764 - val_accuracy: 0.8545\nEpoch 40/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3598 - accuracy: 0.8601 - val_loss: 0.3718 - val_accuracy: 0.8564\nEpoch 41/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3605 - accuracy: 0.8611 - val_loss: 0.3664 - val_accuracy: 0.8574\nEpoch 42/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3590 - accuracy: 0.8606 - val_loss: 0.3648 - val_accuracy: 0.8564\nEpoch 43/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3588 - accuracy: 0.8594 - val_loss: 0.3658 - val_accuracy: 0.8584\nEpoch 44/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3586 - accuracy: 0.8604 - val_loss: 0.3662 - val_accuracy: 0.8584\nEpoch 45/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3582 - accuracy: 0.8606 - val_loss: 0.3653 - val_accuracy: 0.8584\nEpoch 46/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3574 - accuracy: 0.8613 - val_loss: 0.3644 - val_accuracy: 0.8584\nEpoch 47/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3565 - accuracy: 0.8633 - val_loss: 0.3664 - val_accuracy: 0.8584\nEpoch 48/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3567 - accuracy: 0.8616 - val_loss: 0.3716 - val_accuracy: 0.8564\nEpoch 49/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3569 - accuracy: 0.8630 - val_loss: 0.3675 - val_accuracy: 0.8584\nEpoch 50/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3556 - accuracy: 0.8616 - val_loss: 0.3672 - val_accuracy: 0.8584\nEpoch 51/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3555 - accuracy: 0.8621 - val_loss: 0.3624 - val_accuracy: 0.8584\nEpoch 52/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3545 - accuracy: 0.8628 - val_loss: 0.3619 - val_accuracy: 0.8584\nEpoch 53/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3547 - accuracy: 0.8621 - val_loss: 0.3613 - val_accuracy: 0.8584\nEpoch 54/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3545 - accuracy: 0.8630 - val_loss: 0.3644 - val_accuracy: 0.8584\nEpoch 55/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3541 - accuracy: 0.8638 - val_loss: 0.3640 - val_accuracy: 0.8584\nEpoch 56/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3536 - accuracy: 0.8643 - val_loss: 0.3606 - val_accuracy: 0.8594\nEpoch 57/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3534 - accuracy: 0.8638 - val_loss: 0.3606 - val_accuracy: 0.8584\nEpoch 58/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3534 - accuracy: 0.8638 - val_loss: 0.3605 - val_accuracy: 0.8594\nEpoch 59/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3523 - accuracy: 0.8633 - val_loss: 0.3725 - val_accuracy: 0.8574\nEpoch 60/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3524 - accuracy: 0.8647 - val_loss: 0.3666 - val_accuracy: 0.8594\nEpoch 61/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3520 - accuracy: 0.8650 - val_loss: 0.3608 - val_accuracy: 0.8604\nEpoch 62/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3519 - accuracy: 0.8647 - val_loss: 0.3611 - val_accuracy: 0.8594\nEpoch 63/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3517 - accuracy: 0.8650 - val_loss: 0.3609 - val_accuracy: 0.8594\nEpoch 64/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3511 - accuracy: 0.8652 - val_loss: 0.3624 - val_accuracy: 0.8604\nEpoch 65/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3501 - accuracy: 0.8652 - val_loss: 0.3587 - val_accuracy: 0.8613\nEpoch 66/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3504 - accuracy: 0.8650 - val_loss: 0.3596 - val_accuracy: 0.8613\nEpoch 67/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3500 - accuracy: 0.8652 - val_loss: 0.3594 - val_accuracy: 0.8604\nEpoch 68/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3497 - accuracy: 0.8660 - val_loss: 0.3684 - val_accuracy: 0.8594\nEpoch 69/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3491 - accuracy: 0.8652 - val_loss: 0.3639 - val_accuracy: 0.8613\nEpoch 70/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3485 - accuracy: 0.8652 - val_loss: 0.3589 - val_accuracy: 0.8633\nEpoch 71/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3479 - accuracy: 0.8660 - val_loss: 0.3575 - val_accuracy: 0.8623\nEpoch 72/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3486 - accuracy: 0.8662 - val_loss: 0.3602 - val_accuracy: 0.8613\nEpoch 73/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3486 - accuracy: 0.8652 - val_loss: 0.3611 - val_accuracy: 0.8613\nEpoch 74/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3482 - accuracy: 0.8662 - val_loss: 0.3585 - val_accuracy: 0.8633\nEpoch 75/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3476 - accuracy: 0.8657 - val_loss: 0.3599 - val_accuracy: 0.8623\nEpoch 76/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3478 - accuracy: 0.8657 - val_loss: 0.3608 - val_accuracy: 0.8623\nEpoch 77/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3476 - accuracy: 0.8662 - val_loss: 0.3577 - val_accuracy: 0.8633\nEpoch 78/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3467 - accuracy: 0.8655 - val_loss: 0.3563 - val_accuracy: 0.8633\nEpoch 79/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3465 - accuracy: 0.8667 - val_loss: 0.3576 - val_accuracy: 0.8633\nEpoch 80/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3463 - accuracy: 0.8669 - val_loss: 0.3568 - val_accuracy: 0.8633\nEpoch 81/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3457 - accuracy: 0.8672 - val_loss: 0.3551 - val_accuracy: 0.8633\nEpoch 82/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3458 - accuracy: 0.8669 - val_loss: 0.3563 - val_accuracy: 0.8613\nEpoch 83/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3466 - accuracy: 0.8660 - val_loss: 0.3566 - val_accuracy: 0.8633\nEpoch 84/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3449 - accuracy: 0.8674 - val_loss: 0.3550 - val_accuracy: 0.8652\nEpoch 85/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3456 - accuracy: 0.8669 - val_loss: 0.3595 - val_accuracy: 0.8623\nEpoch 86/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3443 - accuracy: 0.8674 - val_loss: 0.3663 - val_accuracy: 0.8613\nEpoch 87/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3446 - accuracy: 0.8679 - val_loss: 0.3557 - val_accuracy: 0.8643\nEpoch 88/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3436 - accuracy: 0.8684 - val_loss: 0.3695 - val_accuracy: 0.8613\nEpoch 89/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3436 - accuracy: 0.8684 - val_loss: 0.3544 - val_accuracy: 0.8613\nEpoch 90/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3432 - accuracy: 0.8672 - val_loss: 0.3542 - val_accuracy: 0.8613\nEpoch 91/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3433 - accuracy: 0.8669 - val_loss: 0.3656 - val_accuracy: 0.8613\nEpoch 92/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3437 - accuracy: 0.8672 - val_loss: 0.3616 - val_accuracy: 0.8643\nEpoch 93/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3436 - accuracy: 0.8672 - val_loss: 0.3555 - val_accuracy: 0.8643\nEpoch 94/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3425 - accuracy: 0.8682 - val_loss: 0.3537 - val_accuracy: 0.8643\nEpoch 95/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3427 - accuracy: 0.8687 - val_loss: 0.3581 - val_accuracy: 0.8643\nEpoch 96/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3427 - accuracy: 0.8677 - val_loss: 0.3604 - val_accuracy: 0.8643\nEpoch 97/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3422 - accuracy: 0.8674 - val_loss: 0.3544 - val_accuracy: 0.8643\nEpoch 98/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3424 - accuracy: 0.8687 - val_loss: 0.3612 - val_accuracy: 0.8643\nEpoch 99/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3413 - accuracy: 0.8687 - val_loss: 0.3557 - val_accuracy: 0.8643\nEpoch 100/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3417 - accuracy: 0.8677 - val_loss: 0.3597 - val_accuracy: 0.8643\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     present  not_present  prob_present  prob_not_present       actual  \\\n0        0.0          1.0      0.135586          0.863920  not_present   \n1        0.0          1.0      0.190671          0.803014  not_present   \n2        0.0          1.0      0.383367          0.631783  not_present   \n3        0.0          1.0      0.136931          0.856289  not_present   \n4        0.0          1.0      0.153871          0.850087  not_present   \n..       ...          ...           ...               ...          ...   \n635      1.0          0.0      0.993823          0.008410      present   \n636      0.0          1.0      0.216730          0.781095  not_present   \n637      0.0          1.0      0.150244          0.846555  not_present   \n638      1.0          0.0      0.145910          0.854962      present   \n639      1.0          0.0      0.971320          0.035874      present   \n\n       predicted correct feature  \n0    not_present     yes   imf_3  \n1    not_present     yes   imf_3  \n2    not_present     yes   imf_3  \n3    not_present     yes   imf_3  \n4    not_present     yes   imf_3  \n..           ...     ...     ...  \n635      present     yes   imf_3  \n636  not_present     yes   imf_3  \n637  not_present     yes   imf_3  \n638  not_present      no   imf_3  \n639      present     yes   imf_3  \n\n[640 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>present</th>\n      <th>not_present</th>\n      <th>prob_present</th>\n      <th>prob_not_present</th>\n      <th>actual</th>\n      <th>predicted</th>\n      <th>correct</th>\n      <th>feature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.135586</td>\n      <td>0.863920</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.190671</td>\n      <td>0.803014</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.383367</td>\n      <td>0.631783</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.136931</td>\n      <td>0.856289</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.153871</td>\n      <td>0.850087</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>635</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.993823</td>\n      <td>0.008410</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_3</td>\n    </tr>\n    <tr>\n      <th>636</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.216730</td>\n      <td>0.781095</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_3</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.150244</td>\n      <td>0.846555</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_3</td>\n    </tr>\n    <tr>\n      <th>638</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.145910</td>\n      <td>0.854962</td>\n      <td>present</td>\n      <td>not_present</td>\n      <td>no</td>\n      <td>imf_3</td>\n    </tr>\n    <tr>\n      <th>639</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.971320</td>\n      <td>0.035874</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_3</td>\n    </tr>\n  </tbody>\n</table>\n<p>640 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "small_df = df[df.columns[512:768]]\n",
    "small_df >>= bind_cols(df.drone_present)\n",
    "\n",
    "presence_labs = small_df['drone_present']\n",
    "\n",
    "values = np.array(presence_labs)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "x = small_df.drop(['drone_present'],axis=1).values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "fit3 = model_small.fit(x = x_train,\n",
    "        y = y_train, \n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        validation_split = .2,\n",
    "        verbose=True)\n",
    "\n",
    "prediction = model_small.predict(x_test)\n",
    "prediction_df = pd.DataFrame(prediction, columns=['prob_present','prob_not_present']) \n",
    "\n",
    "y_test = pd.DataFrame(y_test,columns=['present','not_present'])\n",
    "\n",
    "y_pred = y_test >> bind_cols(prediction_df)\n",
    "\n",
    "y_pred >>= mutate(actual = case_when([y_pred.present == 1, 'present'],\n",
    "[y_pred.not_present == 1, 'not_present']),\n",
    "predicted = case_when([y_pred.prob_present > y_pred.prob_not_present, 'present'],\n",
    "[y_pred.prob_present < y_pred.prob_not_present, 'not_present']))\n",
    "\n",
    "y_pred >>= mutate(correct = case_when([y_pred.actual == y_pred.predicted, 'yes'],\n",
    "[y_pred.actual != y_pred.predicted, 'no']),\n",
    "feature = 'imf_3')\n",
    "y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 2048 samples, validate on 512 samples\nEpoch 1/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.5015 - accuracy: 0.7756 - val_loss: 0.3757 - val_accuracy: 0.8477\nEpoch 2/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.4183 - accuracy: 0.8262 - val_loss: 0.3638 - val_accuracy: 0.8535\nEpoch 3/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.4137 - accuracy: 0.8281 - val_loss: 0.3626 - val_accuracy: 0.8525\nEpoch 4/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4112 - accuracy: 0.8315 - val_loss: 0.3628 - val_accuracy: 0.8555\nEpoch 5/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4098 - accuracy: 0.8318 - val_loss: 0.3606 - val_accuracy: 0.8623\nEpoch 6/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.4091 - accuracy: 0.8320 - val_loss: 0.3600 - val_accuracy: 0.8613\nEpoch 7/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.4081 - accuracy: 0.8325 - val_loss: 0.3585 - val_accuracy: 0.8652\nEpoch 8/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.4066 - accuracy: 0.8318 - val_loss: 0.3579 - val_accuracy: 0.8643\nEpoch 9/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.4053 - accuracy: 0.8335 - val_loss: 0.3570 - val_accuracy: 0.8662\nEpoch 10/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.4048 - accuracy: 0.8345 - val_loss: 0.3561 - val_accuracy: 0.8662\nEpoch 11/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.4037 - accuracy: 0.8350 - val_loss: 0.3573 - val_accuracy: 0.8623\nEpoch 12/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.4024 - accuracy: 0.8350 - val_loss: 0.3543 - val_accuracy: 0.8672\nEpoch 13/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.4006 - accuracy: 0.8342 - val_loss: 0.3548 - val_accuracy: 0.8672\nEpoch 14/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4006 - accuracy: 0.8354 - val_loss: 0.3568 - val_accuracy: 0.8613\nEpoch 15/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.4000 - accuracy: 0.8357 - val_loss: 0.3536 - val_accuracy: 0.8672\nEpoch 16/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3994 - accuracy: 0.8376 - val_loss: 0.3516 - val_accuracy: 0.8682\nEpoch 17/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3981 - accuracy: 0.8359 - val_loss: 0.3549 - val_accuracy: 0.8623\nEpoch 18/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3977 - accuracy: 0.8367 - val_loss: 0.3492 - val_accuracy: 0.8672\nEpoch 19/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3964 - accuracy: 0.8367 - val_loss: 0.3565 - val_accuracy: 0.8584\nEpoch 20/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3973 - accuracy: 0.8376 - val_loss: 0.3487 - val_accuracy: 0.8672\nEpoch 21/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3957 - accuracy: 0.8374 - val_loss: 0.3482 - val_accuracy: 0.8682\nEpoch 22/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3951 - accuracy: 0.8374 - val_loss: 0.3487 - val_accuracy: 0.8672\nEpoch 23/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3945 - accuracy: 0.8386 - val_loss: 0.3479 - val_accuracy: 0.8682\nEpoch 24/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3934 - accuracy: 0.8406 - val_loss: 0.3474 - val_accuracy: 0.8672\nEpoch 25/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3932 - accuracy: 0.8398 - val_loss: 0.3459 - val_accuracy: 0.8682\nEpoch 26/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3928 - accuracy: 0.8394 - val_loss: 0.3450 - val_accuracy: 0.8691\nEpoch 27/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3920 - accuracy: 0.8411 - val_loss: 0.3455 - val_accuracy: 0.8682\nEpoch 28/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3908 - accuracy: 0.8398 - val_loss: 0.3454 - val_accuracy: 0.8682\nEpoch 29/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3908 - accuracy: 0.8406 - val_loss: 0.3444 - val_accuracy: 0.8682\nEpoch 30/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3902 - accuracy: 0.8403 - val_loss: 0.3438 - val_accuracy: 0.8691\nEpoch 31/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3882 - accuracy: 0.8416 - val_loss: 0.3431 - val_accuracy: 0.8691\nEpoch 32/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3892 - accuracy: 0.8423 - val_loss: 0.3442 - val_accuracy: 0.8682\nEpoch 33/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3885 - accuracy: 0.8418 - val_loss: 0.3442 - val_accuracy: 0.8662\nEpoch 34/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3884 - accuracy: 0.8418 - val_loss: 0.3422 - val_accuracy: 0.8691\nEpoch 35/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3880 - accuracy: 0.8428 - val_loss: 0.3422 - val_accuracy: 0.8691\nEpoch 36/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3874 - accuracy: 0.8423 - val_loss: 0.3449 - val_accuracy: 0.8643\nEpoch 37/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3870 - accuracy: 0.8418 - val_loss: 0.3420 - val_accuracy: 0.8691\nEpoch 38/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3860 - accuracy: 0.8430 - val_loss: 0.3408 - val_accuracy: 0.8691\nEpoch 39/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3858 - accuracy: 0.8430 - val_loss: 0.3402 - val_accuracy: 0.8682\nEpoch 40/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3848 - accuracy: 0.8438 - val_loss: 0.3420 - val_accuracy: 0.8662\nEpoch 41/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3840 - accuracy: 0.8425 - val_loss: 0.3392 - val_accuracy: 0.8691\nEpoch 42/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3841 - accuracy: 0.8433 - val_loss: 0.3417 - val_accuracy: 0.8652\nEpoch 43/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3842 - accuracy: 0.8445 - val_loss: 0.3374 - val_accuracy: 0.8691\nEpoch 44/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3818 - accuracy: 0.8433 - val_loss: 0.3379 - val_accuracy: 0.8691\nEpoch 45/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3830 - accuracy: 0.8445 - val_loss: 0.3375 - val_accuracy: 0.8691\nEpoch 46/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3820 - accuracy: 0.8435 - val_loss: 0.3374 - val_accuracy: 0.8691\nEpoch 47/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3816 - accuracy: 0.8433 - val_loss: 0.3364 - val_accuracy: 0.8682\nEpoch 48/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3812 - accuracy: 0.8440 - val_loss: 0.3369 - val_accuracy: 0.8682\nEpoch 49/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3811 - accuracy: 0.8450 - val_loss: 0.3368 - val_accuracy: 0.8682\nEpoch 50/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3806 - accuracy: 0.8447 - val_loss: 0.3378 - val_accuracy: 0.8662\nEpoch 51/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3803 - accuracy: 0.8440 - val_loss: 0.3361 - val_accuracy: 0.8682\nEpoch 52/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3793 - accuracy: 0.8450 - val_loss: 0.3404 - val_accuracy: 0.8613\nEpoch 53/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3789 - accuracy: 0.8459 - val_loss: 0.3391 - val_accuracy: 0.8613\nEpoch 54/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3791 - accuracy: 0.8457 - val_loss: 0.3369 - val_accuracy: 0.8652\nEpoch 55/100\n2048/2048 [==============================] - 0s 68us/sample - loss: 0.3788 - accuracy: 0.8452 - val_loss: 0.3340 - val_accuracy: 0.8682\nEpoch 56/100\n2048/2048 [==============================] - 0s 61us/sample - loss: 0.3779 - accuracy: 0.8462 - val_loss: 0.3348 - val_accuracy: 0.8691\nEpoch 57/100\n2048/2048 [==============================] - 0s 75us/sample - loss: 0.3770 - accuracy: 0.8469 - val_loss: 0.3351 - val_accuracy: 0.8701\nEpoch 58/100\n2048/2048 [==============================] - 0s 71us/sample - loss: 0.3771 - accuracy: 0.8472 - val_loss: 0.3352 - val_accuracy: 0.8701\nEpoch 59/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3769 - accuracy: 0.8477 - val_loss: 0.3350 - val_accuracy: 0.8701\nEpoch 60/100\n2048/2048 [==============================] - 0s 72us/sample - loss: 0.3760 - accuracy: 0.8469 - val_loss: 0.3320 - val_accuracy: 0.8691\nEpoch 61/100\n2048/2048 [==============================] - 0s 109us/sample - loss: 0.3757 - accuracy: 0.8477 - val_loss: 0.3345 - val_accuracy: 0.8662\nEpoch 62/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3755 - accuracy: 0.8481 - val_loss: 0.3349 - val_accuracy: 0.8633\nEpoch 63/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3760 - accuracy: 0.8472 - val_loss: 0.3338 - val_accuracy: 0.8662\nEpoch 64/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3746 - accuracy: 0.8479 - val_loss: 0.3313 - val_accuracy: 0.8701\nEpoch 65/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3748 - accuracy: 0.8491 - val_loss: 0.3334 - val_accuracy: 0.8691\nEpoch 66/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3740 - accuracy: 0.8489 - val_loss: 0.3343 - val_accuracy: 0.8643\nEpoch 67/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3737 - accuracy: 0.8486 - val_loss: 0.3309 - val_accuracy: 0.8721\nEpoch 68/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3735 - accuracy: 0.8496 - val_loss: 0.3365 - val_accuracy: 0.8633\nEpoch 69/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3727 - accuracy: 0.8491 - val_loss: 0.3311 - val_accuracy: 0.8721\nEpoch 70/100\n2048/2048 [==============================] - 0s 69us/sample - loss: 0.3731 - accuracy: 0.8499 - val_loss: 0.3296 - val_accuracy: 0.8760\nEpoch 71/100\n2048/2048 [==============================] - 0s 86us/sample - loss: 0.3723 - accuracy: 0.8503 - val_loss: 0.3335 - val_accuracy: 0.8662\nEpoch 72/100\n2048/2048 [==============================] - 0s 80us/sample - loss: 0.3717 - accuracy: 0.8516 - val_loss: 0.3315 - val_accuracy: 0.8682\nEpoch 73/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3720 - accuracy: 0.8499 - val_loss: 0.3296 - val_accuracy: 0.8721\nEpoch 74/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3715 - accuracy: 0.8499 - val_loss: 0.3316 - val_accuracy: 0.8711\nEpoch 75/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3711 - accuracy: 0.8508 - val_loss: 0.3284 - val_accuracy: 0.8760\nEpoch 76/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3703 - accuracy: 0.8513 - val_loss: 0.3317 - val_accuracy: 0.8691\nEpoch 77/100\n2048/2048 [==============================] - 0s 65us/sample - loss: 0.3701 - accuracy: 0.8506 - val_loss: 0.3288 - val_accuracy: 0.8711\nEpoch 78/100\n2048/2048 [==============================] - 0s 67us/sample - loss: 0.3703 - accuracy: 0.8513 - val_loss: 0.3280 - val_accuracy: 0.8750\nEpoch 79/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3695 - accuracy: 0.8521 - val_loss: 0.3274 - val_accuracy: 0.8760\nEpoch 80/100\n2048/2048 [==============================] - 0s 65us/sample - loss: 0.3695 - accuracy: 0.8511 - val_loss: 0.3270 - val_accuracy: 0.8730\nEpoch 81/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3683 - accuracy: 0.8506 - val_loss: 0.3277 - val_accuracy: 0.8701\nEpoch 82/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3685 - accuracy: 0.8525 - val_loss: 0.3294 - val_accuracy: 0.8711\nEpoch 83/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3679 - accuracy: 0.8513 - val_loss: 0.3262 - val_accuracy: 0.8760\nEpoch 84/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3678 - accuracy: 0.8511 - val_loss: 0.3266 - val_accuracy: 0.8750\nEpoch 85/100\n2048/2048 [==============================] - 0s 73us/sample - loss: 0.3683 - accuracy: 0.8513 - val_loss: 0.3262 - val_accuracy: 0.8750\nEpoch 86/100\n2048/2048 [==============================] - 0s 83us/sample - loss: 0.3675 - accuracy: 0.8528 - val_loss: 0.3267 - val_accuracy: 0.8711\nEpoch 87/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3675 - accuracy: 0.8523 - val_loss: 0.3274 - val_accuracy: 0.8711\nEpoch 88/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3668 - accuracy: 0.8523 - val_loss: 0.3276 - val_accuracy: 0.8711\nEpoch 89/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3666 - accuracy: 0.8542 - val_loss: 0.3265 - val_accuracy: 0.8730\nEpoch 90/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3669 - accuracy: 0.8513 - val_loss: 0.3281 - val_accuracy: 0.8711\nEpoch 91/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3659 - accuracy: 0.8545 - val_loss: 0.3301 - val_accuracy: 0.8701\nEpoch 92/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3661 - accuracy: 0.8540 - val_loss: 0.3260 - val_accuracy: 0.8740\nEpoch 93/100\n2048/2048 [==============================] - 0s 77us/sample - loss: 0.3650 - accuracy: 0.8533 - val_loss: 0.3238 - val_accuracy: 0.8799\nEpoch 94/100\n2048/2048 [==============================] - 0s 102us/sample - loss: 0.3651 - accuracy: 0.8533 - val_loss: 0.3262 - val_accuracy: 0.8750\nEpoch 95/100\n2048/2048 [==============================] - 0s 103us/sample - loss: 0.3648 - accuracy: 0.8530 - val_loss: 0.3286 - val_accuracy: 0.8730\nEpoch 96/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3649 - accuracy: 0.8528 - val_loss: 0.3248 - val_accuracy: 0.8750\nEpoch 97/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3639 - accuracy: 0.8550 - val_loss: 0.3261 - val_accuracy: 0.8730\nEpoch 98/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3642 - accuracy: 0.8540 - val_loss: 0.3282 - val_accuracy: 0.8740\nEpoch 99/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3634 - accuracy: 0.8540 - val_loss: 0.3268 - val_accuracy: 0.8750\nEpoch 100/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3637 - accuracy: 0.8555 - val_loss: 0.3237 - val_accuracy: 0.8789\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     present  not_present  prob_present  prob_not_present       actual  \\\n0        0.0          1.0      0.163446          0.838162  not_present   \n1        1.0          0.0      0.163396          0.839083      present   \n2        0.0          1.0      0.065234          0.959768  not_present   \n3        1.0          0.0      0.216368          0.786270      present   \n4        0.0          1.0      0.131921          0.867549  not_present   \n..       ...          ...           ...               ...          ...   \n635      1.0          0.0      0.993379          0.007508      present   \n636      1.0          0.0      0.185032          0.815659      present   \n637      1.0          0.0      0.998716          0.000873      present   \n638      0.0          1.0      0.388044          0.597987  not_present   \n639      0.0          1.0      0.117551          0.884954  not_present   \n\n       predicted correct feature  \n0    not_present     yes   imf_4  \n1    not_present      no   imf_4  \n2    not_present     yes   imf_4  \n3    not_present      no   imf_4  \n4    not_present     yes   imf_4  \n..           ...     ...     ...  \n635      present     yes   imf_4  \n636  not_present      no   imf_4  \n637      present     yes   imf_4  \n638  not_present     yes   imf_4  \n639  not_present     yes   imf_4  \n\n[640 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>present</th>\n      <th>not_present</th>\n      <th>prob_present</th>\n      <th>prob_not_present</th>\n      <th>actual</th>\n      <th>predicted</th>\n      <th>correct</th>\n      <th>feature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.163446</td>\n      <td>0.838162</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.163396</td>\n      <td>0.839083</td>\n      <td>present</td>\n      <td>not_present</td>\n      <td>no</td>\n      <td>imf_4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.065234</td>\n      <td>0.959768</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.216368</td>\n      <td>0.786270</td>\n      <td>present</td>\n      <td>not_present</td>\n      <td>no</td>\n      <td>imf_4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.131921</td>\n      <td>0.867549</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>635</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.993379</td>\n      <td>0.007508</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_4</td>\n    </tr>\n    <tr>\n      <th>636</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.185032</td>\n      <td>0.815659</td>\n      <td>present</td>\n      <td>not_present</td>\n      <td>no</td>\n      <td>imf_4</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.998716</td>\n      <td>0.000873</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_4</td>\n    </tr>\n    <tr>\n      <th>638</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.388044</td>\n      <td>0.597987</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_4</td>\n    </tr>\n    <tr>\n      <th>639</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.117551</td>\n      <td>0.884954</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_4</td>\n    </tr>\n  </tbody>\n</table>\n<p>640 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "small_df = df[df.columns[768:1024]]\n",
    "small_df >>= bind_cols(df.drone_present)\n",
    "\n",
    "presence_labs = small_df['drone_present']\n",
    "\n",
    "values = np.array(presence_labs)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "x = small_df.drop(['drone_present'],axis=1).values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "fit4 = model_small.fit(x = x_train,\n",
    "        y = y_train, \n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        validation_split = .2,\n",
    "        verbose=True)\n",
    "\n",
    "prediction = model_small.predict(x_test)\n",
    "prediction_df = pd.DataFrame(prediction, columns=['prob_present','prob_not_present']) \n",
    "\n",
    "y_test = pd.DataFrame(y_test,columns=['present','not_present'])\n",
    "\n",
    "y_pred = y_test >> bind_cols(prediction_df)\n",
    "\n",
    "y_pred >>= mutate(actual = case_when([y_pred.present == 1, 'present'],\n",
    "[y_pred.not_present == 1, 'not_present']),\n",
    "predicted = case_when([y_pred.prob_present > y_pred.prob_not_present, 'present'],\n",
    "[y_pred.prob_present < y_pred.prob_not_present, 'not_present']))\n",
    "\n",
    "y_pred >>= mutate(correct = case_when([y_pred.actual == y_pred.predicted, 'yes'],\n",
    "[y_pred.actual != y_pred.predicted, 'no']),\n",
    "feature = 'imf_4')\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 2048 samples, validate on 512 samples\nEpoch 1/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3897 - accuracy: 0.8398 - val_loss: 0.4057 - val_accuracy: 0.8301\nEpoch 2/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3737 - accuracy: 0.8523 - val_loss: 0.4020 - val_accuracy: 0.8320\nEpoch 3/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3717 - accuracy: 0.8535 - val_loss: 0.3989 - val_accuracy: 0.8320\nEpoch 4/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3717 - accuracy: 0.8545 - val_loss: 0.4019 - val_accuracy: 0.8340\nEpoch 5/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3713 - accuracy: 0.8555 - val_loss: 0.3977 - val_accuracy: 0.8389\nEpoch 6/100\n2048/2048 [==============================] - 0s 66us/sample - loss: 0.3710 - accuracy: 0.8560 - val_loss: 0.3970 - val_accuracy: 0.8389\nEpoch 7/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3700 - accuracy: 0.8562 - val_loss: 0.3968 - val_accuracy: 0.8359\nEpoch 8/100\n2048/2048 [==============================] - 0s 64us/sample - loss: 0.3688 - accuracy: 0.8555 - val_loss: 0.3946 - val_accuracy: 0.8398\nEpoch 9/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.3689 - accuracy: 0.8545 - val_loss: 0.3969 - val_accuracy: 0.8369\nEpoch 10/100\n2048/2048 [==============================] - 0s 61us/sample - loss: 0.3691 - accuracy: 0.8584 - val_loss: 0.3955 - val_accuracy: 0.8398\nEpoch 11/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3679 - accuracy: 0.8582 - val_loss: 0.3935 - val_accuracy: 0.8398\nEpoch 12/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3674 - accuracy: 0.8596 - val_loss: 0.3933 - val_accuracy: 0.8389\nEpoch 13/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3678 - accuracy: 0.8577 - val_loss: 0.3934 - val_accuracy: 0.8398\nEpoch 14/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3658 - accuracy: 0.8572 - val_loss: 0.3945 - val_accuracy: 0.8408\nEpoch 15/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3665 - accuracy: 0.8591 - val_loss: 0.3905 - val_accuracy: 0.8398\nEpoch 16/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3660 - accuracy: 0.8569 - val_loss: 0.3925 - val_accuracy: 0.8428\nEpoch 17/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3645 - accuracy: 0.8577 - val_loss: 0.3893 - val_accuracy: 0.8398\nEpoch 18/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3646 - accuracy: 0.8584 - val_loss: 0.3886 - val_accuracy: 0.8418\nEpoch 19/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3645 - accuracy: 0.8599 - val_loss: 0.3905 - val_accuracy: 0.8438\nEpoch 20/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3642 - accuracy: 0.8584 - val_loss: 0.3885 - val_accuracy: 0.8398\nEpoch 21/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3634 - accuracy: 0.8601 - val_loss: 0.3880 - val_accuracy: 0.8398\nEpoch 22/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3634 - accuracy: 0.8591 - val_loss: 0.3866 - val_accuracy: 0.8408\nEpoch 23/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3627 - accuracy: 0.8613 - val_loss: 0.3877 - val_accuracy: 0.8447\nEpoch 24/100\n2048/2048 [==============================] - 0s 61us/sample - loss: 0.3626 - accuracy: 0.8616 - val_loss: 0.3857 - val_accuracy: 0.8447\nEpoch 25/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3607 - accuracy: 0.8623 - val_loss: 0.3856 - val_accuracy: 0.8398\nEpoch 26/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3622 - accuracy: 0.8594 - val_loss: 0.3871 - val_accuracy: 0.8398\nEpoch 27/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3611 - accuracy: 0.8608 - val_loss: 0.3859 - val_accuracy: 0.8457\nEpoch 28/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.3608 - accuracy: 0.8594 - val_loss: 0.3840 - val_accuracy: 0.8457\nEpoch 29/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3607 - accuracy: 0.8582 - val_loss: 0.3841 - val_accuracy: 0.8457\nEpoch 30/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3593 - accuracy: 0.8613 - val_loss: 0.3876 - val_accuracy: 0.8457\nEpoch 31/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3595 - accuracy: 0.8606 - val_loss: 0.3856 - val_accuracy: 0.8457\nEpoch 32/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3592 - accuracy: 0.8616 - val_loss: 0.3860 - val_accuracy: 0.8418\nEpoch 33/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3587 - accuracy: 0.8638 - val_loss: 0.3822 - val_accuracy: 0.8408\nEpoch 34/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3589 - accuracy: 0.8625 - val_loss: 0.3834 - val_accuracy: 0.8457\nEpoch 35/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3575 - accuracy: 0.8621 - val_loss: 0.3805 - val_accuracy: 0.8457\nEpoch 36/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3578 - accuracy: 0.8628 - val_loss: 0.3839 - val_accuracy: 0.8438\nEpoch 37/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3568 - accuracy: 0.8640 - val_loss: 0.3796 - val_accuracy: 0.8457\nEpoch 38/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3573 - accuracy: 0.8638 - val_loss: 0.3812 - val_accuracy: 0.8457\nEpoch 39/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3560 - accuracy: 0.8655 - val_loss: 0.3787 - val_accuracy: 0.8457\nEpoch 40/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3565 - accuracy: 0.8643 - val_loss: 0.3790 - val_accuracy: 0.8457\nEpoch 41/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3555 - accuracy: 0.8630 - val_loss: 0.3764 - val_accuracy: 0.8506\nEpoch 42/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3556 - accuracy: 0.8623 - val_loss: 0.3787 - val_accuracy: 0.8457\nEpoch 43/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3562 - accuracy: 0.8647 - val_loss: 0.3777 - val_accuracy: 0.8457\nEpoch 44/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3557 - accuracy: 0.8645 - val_loss: 0.3784 - val_accuracy: 0.8457\nEpoch 45/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3548 - accuracy: 0.8638 - val_loss: 0.3787 - val_accuracy: 0.8467\nEpoch 46/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3540 - accuracy: 0.8643 - val_loss: 0.3791 - val_accuracy: 0.8467\nEpoch 47/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3542 - accuracy: 0.8650 - val_loss: 0.3781 - val_accuracy: 0.8457\nEpoch 48/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3536 - accuracy: 0.8652 - val_loss: 0.3776 - val_accuracy: 0.8477\nEpoch 49/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3527 - accuracy: 0.8665 - val_loss: 0.3749 - val_accuracy: 0.8496\nEpoch 50/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3530 - accuracy: 0.8640 - val_loss: 0.3730 - val_accuracy: 0.8477\nEpoch 51/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3533 - accuracy: 0.8628 - val_loss: 0.3734 - val_accuracy: 0.8496\nEpoch 52/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3528 - accuracy: 0.8640 - val_loss: 0.3755 - val_accuracy: 0.8457\nEpoch 53/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3530 - accuracy: 0.8655 - val_loss: 0.3732 - val_accuracy: 0.8486\nEpoch 54/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3527 - accuracy: 0.8647 - val_loss: 0.3745 - val_accuracy: 0.8496\nEpoch 55/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3526 - accuracy: 0.8650 - val_loss: 0.3753 - val_accuracy: 0.8496\nEpoch 56/100\n2048/2048 [==============================] - 0s 64us/sample - loss: 0.3520 - accuracy: 0.8652 - val_loss: 0.3739 - val_accuracy: 0.8477\nEpoch 57/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3509 - accuracy: 0.8645 - val_loss: 0.3732 - val_accuracy: 0.8486\nEpoch 58/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3512 - accuracy: 0.8643 - val_loss: 0.3708 - val_accuracy: 0.8496\nEpoch 59/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3511 - accuracy: 0.8645 - val_loss: 0.3736 - val_accuracy: 0.8506\nEpoch 60/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3510 - accuracy: 0.8650 - val_loss: 0.3728 - val_accuracy: 0.8496\nEpoch 61/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3503 - accuracy: 0.8657 - val_loss: 0.3714 - val_accuracy: 0.8477\nEpoch 62/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3505 - accuracy: 0.8660 - val_loss: 0.3704 - val_accuracy: 0.8516\nEpoch 63/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3500 - accuracy: 0.8650 - val_loss: 0.3720 - val_accuracy: 0.8525\nEpoch 64/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3494 - accuracy: 0.8662 - val_loss: 0.3742 - val_accuracy: 0.8535\nEpoch 65/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3501 - accuracy: 0.8655 - val_loss: 0.3710 - val_accuracy: 0.8535\nEpoch 66/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3495 - accuracy: 0.8655 - val_loss: 0.3694 - val_accuracy: 0.8516\nEpoch 67/100\n2048/2048 [==============================] - 0s 58us/sample - loss: 0.3494 - accuracy: 0.8657 - val_loss: 0.3694 - val_accuracy: 0.8535\nEpoch 68/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3483 - accuracy: 0.8645 - val_loss: 0.3682 - val_accuracy: 0.8516\nEpoch 69/100\n2048/2048 [==============================] - 0s 47us/sample - loss: 0.3487 - accuracy: 0.8655 - val_loss: 0.3668 - val_accuracy: 0.8535\nEpoch 70/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3482 - accuracy: 0.8638 - val_loss: 0.3665 - val_accuracy: 0.8535\nEpoch 71/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3476 - accuracy: 0.8652 - val_loss: 0.3679 - val_accuracy: 0.8535\nEpoch 72/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3481 - accuracy: 0.8662 - val_loss: 0.3697 - val_accuracy: 0.8535\nEpoch 73/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3477 - accuracy: 0.8650 - val_loss: 0.3673 - val_accuracy: 0.8535\nEpoch 74/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3479 - accuracy: 0.8645 - val_loss: 0.3688 - val_accuracy: 0.8535\nEpoch 75/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3468 - accuracy: 0.8657 - val_loss: 0.3683 - val_accuracy: 0.8525\nEpoch 76/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3464 - accuracy: 0.8650 - val_loss: 0.3694 - val_accuracy: 0.8535\nEpoch 77/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3469 - accuracy: 0.8657 - val_loss: 0.3646 - val_accuracy: 0.8535\nEpoch 78/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3471 - accuracy: 0.8657 - val_loss: 0.3649 - val_accuracy: 0.8535\nEpoch 79/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3466 - accuracy: 0.8652 - val_loss: 0.3662 - val_accuracy: 0.8574\nEpoch 80/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3458 - accuracy: 0.8650 - val_loss: 0.3644 - val_accuracy: 0.8545\nEpoch 81/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3462 - accuracy: 0.8647 - val_loss: 0.3656 - val_accuracy: 0.8535\nEpoch 82/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3456 - accuracy: 0.8652 - val_loss: 0.3627 - val_accuracy: 0.8555\nEpoch 83/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3457 - accuracy: 0.8662 - val_loss: 0.3637 - val_accuracy: 0.8574\nEpoch 84/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3457 - accuracy: 0.8650 - val_loss: 0.3630 - val_accuracy: 0.8574\nEpoch 85/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3454 - accuracy: 0.8662 - val_loss: 0.3617 - val_accuracy: 0.8584\nEpoch 86/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3461 - accuracy: 0.8628 - val_loss: 0.3635 - val_accuracy: 0.8574\nEpoch 87/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3448 - accuracy: 0.8655 - val_loss: 0.3621 - val_accuracy: 0.8574\nEpoch 88/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3443 - accuracy: 0.8655 - val_loss: 0.3642 - val_accuracy: 0.8555\nEpoch 89/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3441 - accuracy: 0.8665 - val_loss: 0.3641 - val_accuracy: 0.8574\nEpoch 90/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3449 - accuracy: 0.8650 - val_loss: 0.3631 - val_accuracy: 0.8564\nEpoch 91/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3445 - accuracy: 0.8660 - val_loss: 0.3633 - val_accuracy: 0.8574\nEpoch 92/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3436 - accuracy: 0.8679 - val_loss: 0.3631 - val_accuracy: 0.8574\nEpoch 93/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3438 - accuracy: 0.8660 - val_loss: 0.3645 - val_accuracy: 0.8594\nEpoch 94/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3434 - accuracy: 0.8669 - val_loss: 0.3612 - val_accuracy: 0.8584\nEpoch 95/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3431 - accuracy: 0.8662 - val_loss: 0.3602 - val_accuracy: 0.8594\nEpoch 96/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3434 - accuracy: 0.8674 - val_loss: 0.3602 - val_accuracy: 0.8574\nEpoch 97/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3439 - accuracy: 0.8655 - val_loss: 0.3597 - val_accuracy: 0.8594\nEpoch 98/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3428 - accuracy: 0.8672 - val_loss: 0.3630 - val_accuracy: 0.8594\nEpoch 99/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.3429 - accuracy: 0.8674 - val_loss: 0.3613 - val_accuracy: 0.8574\nEpoch 100/100\n2048/2048 [==============================] - 0s 65us/sample - loss: 0.3421 - accuracy: 0.8684 - val_loss: 0.3598 - val_accuracy: 0.8574\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     present  not_present  prob_present  prob_not_present       actual  \\\n0        0.0          1.0      0.121820          0.878875  not_present   \n1        1.0          0.0      0.995807          0.004531      present   \n2        0.0          1.0      0.157295          0.843426  not_present   \n3        0.0          1.0      0.170885          0.830069  not_present   \n4        0.0          1.0      0.162006          0.838424  not_present   \n..       ...          ...           ...               ...          ...   \n635      1.0          0.0      0.993923          0.006557      present   \n636      0.0          1.0      0.139798          0.854986  not_present   \n637      1.0          0.0      0.995972          0.004365      present   \n638      1.0          0.0      0.979417          0.021836      present   \n639      1.0          0.0      0.960562          0.041378      present   \n\n       predicted correct feature  \n0    not_present     yes   imf_5  \n1        present     yes   imf_5  \n2    not_present     yes   imf_5  \n3    not_present     yes   imf_5  \n4    not_present     yes   imf_5  \n..           ...     ...     ...  \n635      present     yes   imf_5  \n636  not_present     yes   imf_5  \n637      present     yes   imf_5  \n638      present     yes   imf_5  \n639      present     yes   imf_5  \n\n[640 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>present</th>\n      <th>not_present</th>\n      <th>prob_present</th>\n      <th>prob_not_present</th>\n      <th>actual</th>\n      <th>predicted</th>\n      <th>correct</th>\n      <th>feature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.121820</td>\n      <td>0.878875</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.995807</td>\n      <td>0.004531</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.157295</td>\n      <td>0.843426</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.170885</td>\n      <td>0.830069</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.162006</td>\n      <td>0.838424</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>635</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.993923</td>\n      <td>0.006557</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_5</td>\n    </tr>\n    <tr>\n      <th>636</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.139798</td>\n      <td>0.854986</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>imf_5</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.995972</td>\n      <td>0.004365</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_5</td>\n    </tr>\n    <tr>\n      <th>638</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.979417</td>\n      <td>0.021836</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_5</td>\n    </tr>\n    <tr>\n      <th>639</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.960562</td>\n      <td>0.041378</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>imf_5</td>\n    </tr>\n  </tbody>\n</table>\n<p>640 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "small_df = df[df.columns[1024:1280]]\n",
    "small_df >>= bind_cols(df.drone_present)\n",
    "\n",
    "presence_labs = small_df['drone_present']\n",
    "\n",
    "values = np.array(presence_labs)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "x = small_df.drop(['drone_present'],axis=1).values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "fit5 = model_small.fit(x = x_train,\n",
    "        y = y_train, \n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        validation_split = .2,\n",
    "        verbose=True)\n",
    "\n",
    "prediction = model_small.predict(x_test)\n",
    "prediction_df = pd.DataFrame(prediction, columns=['prob_present','prob_not_present']) \n",
    "\n",
    "y_test = pd.DataFrame(y_test,columns=['present','not_present'])\n",
    "\n",
    "y_pred = y_test >> bind_cols(prediction_df)\n",
    "\n",
    "y_pred >>= mutate(actual = case_when([y_pred.present == 1, 'present'],\n",
    "[y_pred.not_present == 1, 'not_present']),\n",
    "predicted = case_when([y_pred.prob_present > y_pred.prob_not_present, 'present'],\n",
    "[y_pred.prob_present < y_pred.prob_not_present, 'not_present']))\n",
    "\n",
    "y_pred >>= mutate(correct = case_when([y_pred.actual == y_pred.predicted, 'yes'],\n",
    "[y_pred.actual != y_pred.predicted, 'no']),\n",
    "feature = 'imf_5')\n",
    "y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x1e6eb969308>"
     },
     "metadata": {},
     "execution_count": 19
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"me9b3e9d710\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#me9b3e9d710\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"92.915308\" xlink:href=\"#me9b3e9d710\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(83.371558 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"140.509309\" xlink:href=\"#me9b3e9d710\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(130.965559 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"188.10331\" xlink:href=\"#me9b3e9d710\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(178.55956 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.697311\" xlink:href=\"#me9b3e9d710\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(226.153561 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"283.291312\" xlink:href=\"#me9b3e9d710\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(270.566312 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.885313\" xlink:href=\"#me9b3e9d710\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 1200 -->\r\n      <g transform=\"translate(318.160313 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m919e2ed0c2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m919e2ed0c2\" y=\"214.756364\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 218.555582)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m919e2ed0c2\" y=\"175.221818\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 179.021037)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m919e2ed0c2\" y=\"135.687273\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 139.486491)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m919e2ed0c2\" y=\"96.152727\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 99.951946)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m919e2ed0c2\" y=\"56.618182\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(7.2 60.417401)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m919e2ed0c2\" y=\"17.083636\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 20.882855)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p2dd226ccd6)\" d=\"M 45.321307 214.756364 \r\nL 45.559277 113.508733 \r\nL 45.797247 79.388497 \r\nL 46.273187 71.92962 \r\nL 46.987097 66.237567 \r\nL 47.938977 62.255097 \r\nL 49.366797 57.206053 \r\nL 50.794617 52.92477 \r\nL 52.460407 48.675048 \r\nL 53.888227 45.580635 \r\nL 55.078077 43.332008 \r\nL 56.505897 40.989603 \r\nL 58.171687 38.674201 \r\nL 60.075447 36.50869 \r\nL 61.979207 34.711286 \r\nL 63.644997 33.424294 \r\nL 66.024697 31.889906 \r\nL 68.642367 30.51 \r\nL 71.260037 29.390483 \r\nL 74.591617 28.212807 \r\nL 78.637108 27.039868 \r\nL 83.158538 25.973822 \r\nL 88.869818 24.855155 \r\nL 96.008918 23.699791 \r\nL 104.099898 22.621194 \r\nL 113.856668 21.548011 \r\nL 125.517198 20.502238 \r\nL 138.367579 19.587302 \r\nL 152.645779 18.800246 \r\nL 168.589769 18.155472 \r\nL 186.43752 17.665932 \r\nL 207.14091 17.330019 \r\nL 233.079641 17.149185 \r\nL 279.007852 17.084911 \r\nL 349.684943 17.083636 \r\nL 349.684943 17.083636 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 271.35 219.64 \r\nL 357.903125 219.64 \r\nQ 359.903125 219.64 359.903125 217.64 \r\nL 359.903125 203.68375 \r\nQ 359.903125 201.68375 357.903125 201.68375 \r\nL 271.35 201.68375 \r\nQ 269.35 201.68375 269.35 203.68375 \r\nL 269.35 217.64 \r\nQ 269.35 219.64 271.35 219.64 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\">\r\n     <path d=\"M 273.35 209.782187 \r\nL 293.35 209.782187 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\"/>\r\n    <g id=\"text_14\">\r\n     <!-- ratio_sums -->\r\n     <defs>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n     </defs>\r\n     <g transform=\"translate(301.35 213.282187)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"41.113281\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"102.392578\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"230.566406\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"280.566406\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"332.666016\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"396.044922\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"493.457031\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p2dd226ccd6\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "x = df.drop(['behavior','signal','multiple','drone_present'],axis=1).values\n",
    "\n",
    "pca = decomposition.PCA(n_components=1280)\n",
    "pca.fit(x)\n",
    "pca_x = pca.transform(x)\n",
    "\n",
    "variance = []\n",
    "for i in range(1280):\n",
    "    variance.append(sum(pca.explained_variance_ratio_[0:i]))\n",
    "\n",
    "variance = pd.DataFrame(variance,columns=['ratio_sums'])\n",
    "variance.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"test\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_6 (Dense)              (None, 64)                12864     \n_________________________________________________________________\ndense_7 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_8 (Dense)              (None, 2)                 66        \n=================================================================\nTotal params: 15,010\nTrainable params: 15,010\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 2048 samples, validate on 512 samples\nEpoch 1/100\n2048/2048 [==============================] - 1s 247us/sample - loss: 0.6899 - accuracy: 0.6589 - val_loss: 0.6827 - val_accuracy: 0.7051\nEpoch 2/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.6754 - accuracy: 0.7097 - val_loss: 0.6688 - val_accuracy: 0.7324\nEpoch 3/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.6632 - accuracy: 0.7273 - val_loss: 0.6563 - val_accuracy: 0.7568\nEpoch 4/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.6516 - accuracy: 0.7593 - val_loss: 0.6441 - val_accuracy: 0.7705\nEpoch 5/100\n2048/2048 [==============================] - 0s 54us/sample - loss: 0.6401 - accuracy: 0.7822 - val_loss: 0.6321 - val_accuracy: 0.8320\nEpoch 6/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.6281 - accuracy: 0.8474 - val_loss: 0.6197 - val_accuracy: 0.8691\nEpoch 7/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.6155 - accuracy: 0.8694 - val_loss: 0.6070 - val_accuracy: 0.8730\nEpoch 8/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.6023 - accuracy: 0.8740 - val_loss: 0.5940 - val_accuracy: 0.8740\nEpoch 9/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.5883 - accuracy: 0.8757 - val_loss: 0.5804 - val_accuracy: 0.8740\nEpoch 10/100\n2048/2048 [==============================] - 0s 73us/sample - loss: 0.5737 - accuracy: 0.8760 - val_loss: 0.5668 - val_accuracy: 0.8750\nEpoch 11/100\n2048/2048 [==============================] - 0s 82us/sample - loss: 0.5587 - accuracy: 0.8762 - val_loss: 0.5531 - val_accuracy: 0.8750\nEpoch 12/100\n2048/2048 [==============================] - 0s 82us/sample - loss: 0.5437 - accuracy: 0.8765 - val_loss: 0.5396 - val_accuracy: 0.8750\nEpoch 13/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.5283 - accuracy: 0.8767 - val_loss: 0.5259 - val_accuracy: 0.8750\nEpoch 14/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.5131 - accuracy: 0.8767 - val_loss: 0.5131 - val_accuracy: 0.8750\nEpoch 15/100\n2048/2048 [==============================] - 0s 58us/sample - loss: 0.4987 - accuracy: 0.8765 - val_loss: 0.5011 - val_accuracy: 0.8750\nEpoch 16/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4853 - accuracy: 0.8762 - val_loss: 0.4899 - val_accuracy: 0.8750\nEpoch 17/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.4728 - accuracy: 0.8755 - val_loss: 0.4797 - val_accuracy: 0.8740\nEpoch 18/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.4615 - accuracy: 0.8748 - val_loss: 0.4704 - val_accuracy: 0.8740\nEpoch 19/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.4515 - accuracy: 0.8745 - val_loss: 0.4621 - val_accuracy: 0.8740\nEpoch 20/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4424 - accuracy: 0.8745 - val_loss: 0.4547 - val_accuracy: 0.8740\nEpoch 21/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.4344 - accuracy: 0.8738 - val_loss: 0.4477 - val_accuracy: 0.8740\nEpoch 22/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.4273 - accuracy: 0.8735 - val_loss: 0.4414 - val_accuracy: 0.8740\nEpoch 23/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.4210 - accuracy: 0.8740 - val_loss: 0.4360 - val_accuracy: 0.8740\nEpoch 24/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.4153 - accuracy: 0.8743 - val_loss: 0.4308 - val_accuracy: 0.8740\nEpoch 25/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.4100 - accuracy: 0.8735 - val_loss: 0.4257 - val_accuracy: 0.8740\nEpoch 26/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4053 - accuracy: 0.8748 - val_loss: 0.4210 - val_accuracy: 0.8740\nEpoch 27/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.4008 - accuracy: 0.8745 - val_loss: 0.4167 - val_accuracy: 0.8740\nEpoch 28/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3967 - accuracy: 0.8748 - val_loss: 0.4126 - val_accuracy: 0.8740\nEpoch 29/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3929 - accuracy: 0.8743 - val_loss: 0.4087 - val_accuracy: 0.8740\nEpoch 30/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3893 - accuracy: 0.8748 - val_loss: 0.4050 - val_accuracy: 0.8740\nEpoch 31/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3860 - accuracy: 0.8748 - val_loss: 0.4016 - val_accuracy: 0.8750\nEpoch 32/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3829 - accuracy: 0.8748 - val_loss: 0.3984 - val_accuracy: 0.8750\nEpoch 33/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3800 - accuracy: 0.8752 - val_loss: 0.3955 - val_accuracy: 0.8750\nEpoch 34/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3773 - accuracy: 0.8752 - val_loss: 0.3927 - val_accuracy: 0.8750\nEpoch 35/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3748 - accuracy: 0.8748 - val_loss: 0.3900 - val_accuracy: 0.8750\nEpoch 36/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3725 - accuracy: 0.8752 - val_loss: 0.3874 - val_accuracy: 0.8750\nEpoch 37/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3703 - accuracy: 0.8752 - val_loss: 0.3851 - val_accuracy: 0.8750\nEpoch 38/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3682 - accuracy: 0.8752 - val_loss: 0.3828 - val_accuracy: 0.8750\nEpoch 39/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.3663 - accuracy: 0.8760 - val_loss: 0.3809 - val_accuracy: 0.8750\nEpoch 40/100\n2048/2048 [==============================] - 0s 61us/sample - loss: 0.3645 - accuracy: 0.8757 - val_loss: 0.3789 - val_accuracy: 0.8750\nEpoch 41/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3627 - accuracy: 0.8757 - val_loss: 0.3769 - val_accuracy: 0.8750\nEpoch 42/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3611 - accuracy: 0.8760 - val_loss: 0.3752 - val_accuracy: 0.8750\nEpoch 43/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3596 - accuracy: 0.8760 - val_loss: 0.3735 - val_accuracy: 0.8750\nEpoch 44/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3581 - accuracy: 0.8760 - val_loss: 0.3718 - val_accuracy: 0.8740\nEpoch 45/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3567 - accuracy: 0.8762 - val_loss: 0.3702 - val_accuracy: 0.8740\nEpoch 46/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3554 - accuracy: 0.8760 - val_loss: 0.3688 - val_accuracy: 0.8740\nEpoch 47/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3541 - accuracy: 0.8760 - val_loss: 0.3673 - val_accuracy: 0.8740\nEpoch 48/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3529 - accuracy: 0.8762 - val_loss: 0.3659 - val_accuracy: 0.8740\nEpoch 49/100\n2048/2048 [==============================] - 0s 72us/sample - loss: 0.3517 - accuracy: 0.8765 - val_loss: 0.3647 - val_accuracy: 0.8740\nEpoch 50/100\n2048/2048 [==============================] - 0s 65us/sample - loss: 0.3506 - accuracy: 0.8765 - val_loss: 0.3634 - val_accuracy: 0.8740\nEpoch 51/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3496 - accuracy: 0.8765 - val_loss: 0.3623 - val_accuracy: 0.8740\nEpoch 52/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3486 - accuracy: 0.8765 - val_loss: 0.3611 - val_accuracy: 0.8740\nEpoch 53/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3476 - accuracy: 0.8765 - val_loss: 0.3600 - val_accuracy: 0.8740\nEpoch 54/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3467 - accuracy: 0.8765 - val_loss: 0.3590 - val_accuracy: 0.8740\nEpoch 55/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3458 - accuracy: 0.8765 - val_loss: 0.3580 - val_accuracy: 0.8740\nEpoch 56/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3449 - accuracy: 0.8767 - val_loss: 0.3571 - val_accuracy: 0.8740\nEpoch 57/100\n2048/2048 [==============================] - 0s 53us/sample - loss: 0.3441 - accuracy: 0.8767 - val_loss: 0.3561 - val_accuracy: 0.8740\nEpoch 58/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3434 - accuracy: 0.8767 - val_loss: 0.3553 - val_accuracy: 0.8740\nEpoch 59/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3427 - accuracy: 0.8767 - val_loss: 0.3544 - val_accuracy: 0.8740\nEpoch 60/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3419 - accuracy: 0.8767 - val_loss: 0.3536 - val_accuracy: 0.8740\nEpoch 61/100\n2048/2048 [==============================] - 0s 55us/sample - loss: 0.3412 - accuracy: 0.8767 - val_loss: 0.3528 - val_accuracy: 0.8740\nEpoch 62/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3406 - accuracy: 0.8770 - val_loss: 0.3520 - val_accuracy: 0.8740\nEpoch 63/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3399 - accuracy: 0.8770 - val_loss: 0.3513 - val_accuracy: 0.8740\nEpoch 64/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3393 - accuracy: 0.8770 - val_loss: 0.3507 - val_accuracy: 0.8740\nEpoch 65/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3388 - accuracy: 0.8770 - val_loss: 0.3501 - val_accuracy: 0.8740\nEpoch 66/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3382 - accuracy: 0.8770 - val_loss: 0.3495 - val_accuracy: 0.8740\nEpoch 67/100\n2048/2048 [==============================] - 0s 51us/sample - loss: 0.3377 - accuracy: 0.8770 - val_loss: 0.3490 - val_accuracy: 0.8740\nEpoch 68/100\n2048/2048 [==============================] - 0s 50us/sample - loss: 0.3372 - accuracy: 0.8770 - val_loss: 0.3484 - val_accuracy: 0.8740\nEpoch 69/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3367 - accuracy: 0.8770 - val_loss: 0.3479 - val_accuracy: 0.8740\nEpoch 70/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3362 - accuracy: 0.8770 - val_loss: 0.3474 - val_accuracy: 0.8740\nEpoch 71/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3357 - accuracy: 0.8770 - val_loss: 0.3470 - val_accuracy: 0.8740\nEpoch 72/100\n2048/2048 [==============================] - 0s 66us/sample - loss: 0.3353 - accuracy: 0.8770 - val_loss: 0.3465 - val_accuracy: 0.8740\nEpoch 73/100\n2048/2048 [==============================] - 0s 52us/sample - loss: 0.3348 - accuracy: 0.8770 - val_loss: 0.3460 - val_accuracy: 0.8740\nEpoch 74/100\n2048/2048 [==============================] - 0s 49us/sample - loss: 0.3344 - accuracy: 0.8770 - val_loss: 0.3456 - val_accuracy: 0.8740\nEpoch 75/100\n2048/2048 [==============================] - 0s 48us/sample - loss: 0.3340 - accuracy: 0.8770 - val_loss: 0.3452 - val_accuracy: 0.8740\nEpoch 76/100\n2048/2048 [==============================] - 0s 58us/sample - loss: 0.3336 - accuracy: 0.8770 - val_loss: 0.3447 - val_accuracy: 0.8740\nEpoch 77/100\n2048/2048 [==============================] - 0s 64us/sample - loss: 0.3333 - accuracy: 0.8770 - val_loss: 0.3444 - val_accuracy: 0.8740\nEpoch 78/100\n2048/2048 [==============================] - 0s 66us/sample - loss: 0.3329 - accuracy: 0.8770 - val_loss: 0.3440 - val_accuracy: 0.8740\nEpoch 79/100\n2048/2048 [==============================] - 0s 59us/sample - loss: 0.3325 - accuracy: 0.8770 - val_loss: 0.3436 - val_accuracy: 0.8740\nEpoch 80/100\n2048/2048 [==============================] - 0s 77us/sample - loss: 0.3322 - accuracy: 0.8770 - val_loss: 0.3433 - val_accuracy: 0.8740\nEpoch 81/100\n2048/2048 [==============================] - 0s 65us/sample - loss: 0.3318 - accuracy: 0.8770 - val_loss: 0.3429 - val_accuracy: 0.8740\nEpoch 82/100\n2048/2048 [==============================] - 0s 72us/sample - loss: 0.3315 - accuracy: 0.8770 - val_loss: 0.3426 - val_accuracy: 0.8740\nEpoch 83/100\n2048/2048 [==============================] - 0s 99us/sample - loss: 0.3311 - accuracy: 0.8770 - val_loss: 0.3422 - val_accuracy: 0.8740\nEpoch 84/100\n2048/2048 [==============================] - 0s 107us/sample - loss: 0.3309 - accuracy: 0.8770 - val_loss: 0.3419 - val_accuracy: 0.8740\nEpoch 85/100\n2048/2048 [==============================] - 0s 62us/sample - loss: 0.3305 - accuracy: 0.8770 - val_loss: 0.3416 - val_accuracy: 0.8740\nEpoch 86/100\n2048/2048 [==============================] - 0s 70us/sample - loss: 0.3302 - accuracy: 0.8770 - val_loss: 0.3413 - val_accuracy: 0.8740\nEpoch 87/100\n2048/2048 [==============================] - 0s 64us/sample - loss: 0.3299 - accuracy: 0.8770 - val_loss: 0.3410 - val_accuracy: 0.8740\nEpoch 88/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3296 - accuracy: 0.8770 - val_loss: 0.3407 - val_accuracy: 0.8740\nEpoch 89/100\n2048/2048 [==============================] - 0s 66us/sample - loss: 0.3293 - accuracy: 0.8770 - val_loss: 0.3404 - val_accuracy: 0.8740\nEpoch 90/100\n2048/2048 [==============================] - 0s 70us/sample - loss: 0.3290 - accuracy: 0.8770 - val_loss: 0.3401 - val_accuracy: 0.8740\nEpoch 91/100\n2048/2048 [==============================] - 0s 61us/sample - loss: 0.3288 - accuracy: 0.8770 - val_loss: 0.3399 - val_accuracy: 0.8740\nEpoch 92/100\n2048/2048 [==============================] - 0s 57us/sample - loss: 0.3285 - accuracy: 0.8770 - val_loss: 0.3395 - val_accuracy: 0.8740\nEpoch 93/100\n2048/2048 [==============================] - 0s 69us/sample - loss: 0.3282 - accuracy: 0.8770 - val_loss: 0.3392 - val_accuracy: 0.8740\nEpoch 94/100\n2048/2048 [==============================] - 0s 60us/sample - loss: 0.3280 - accuracy: 0.8770 - val_loss: 0.3390 - val_accuracy: 0.8740\nEpoch 95/100\n2048/2048 [==============================] - 0s 67us/sample - loss: 0.3277 - accuracy: 0.8770 - val_loss: 0.3387 - val_accuracy: 0.8740\nEpoch 96/100\n2048/2048 [==============================] - 0s 56us/sample - loss: 0.3274 - accuracy: 0.8770 - val_loss: 0.3384 - val_accuracy: 0.8740\nEpoch 97/100\n2048/2048 [==============================] - 0s 68us/sample - loss: 0.3272 - accuracy: 0.8770 - val_loss: 0.3382 - val_accuracy: 0.8740\nEpoch 98/100\n2048/2048 [==============================] - 0s 65us/sample - loss: 0.3269 - accuracy: 0.8770 - val_loss: 0.3380 - val_accuracy: 0.8740\nEpoch 99/100\n2048/2048 [==============================] - 0s 58us/sample - loss: 0.3267 - accuracy: 0.8770 - val_loss: 0.3377 - val_accuracy: 0.8740\nEpoch 100/100\n2048/2048 [==============================] - 0s 119us/sample - loss: 0.3265 - accuracy: 0.8770 - val_loss: 0.3374 - val_accuracy: 0.8740\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     present  not_present  prob_present  prob_not_present       actual  \\\n0        1.0          0.0      0.163459          0.827430      present   \n1        1.0          0.0      0.989765          0.010213      present   \n2        1.0          0.0      0.166846          0.833190      present   \n3        0.0          1.0      0.203723          0.804960  not_present   \n4        1.0          0.0      0.155582          0.848806      present   \n..       ...          ...           ...               ...          ...   \n635      0.0          1.0      0.212918          0.774682  not_present   \n636      1.0          0.0      0.998865          0.001022      present   \n637      0.0          1.0      0.185018          0.822350  not_present   \n638      0.0          1.0      0.189441          0.822144  not_present   \n639      0.0          1.0      0.252489          0.741521  not_present   \n\n       predicted correct feature  \n0    not_present      no     pca  \n1        present     yes     pca  \n2    not_present      no     pca  \n3    not_present     yes     pca  \n4    not_present      no     pca  \n..           ...     ...     ...  \n635  not_present     yes     pca  \n636      present     yes     pca  \n637  not_present     yes     pca  \n638  not_present     yes     pca  \n639  not_present     yes     pca  \n\n[640 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>present</th>\n      <th>not_present</th>\n      <th>prob_present</th>\n      <th>prob_not_present</th>\n      <th>actual</th>\n      <th>predicted</th>\n      <th>correct</th>\n      <th>feature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.163459</td>\n      <td>0.827430</td>\n      <td>present</td>\n      <td>not_present</td>\n      <td>no</td>\n      <td>pca</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.989765</td>\n      <td>0.010213</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>pca</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.166846</td>\n      <td>0.833190</td>\n      <td>present</td>\n      <td>not_present</td>\n      <td>no</td>\n      <td>pca</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.203723</td>\n      <td>0.804960</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>pca</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.155582</td>\n      <td>0.848806</td>\n      <td>present</td>\n      <td>not_present</td>\n      <td>no</td>\n      <td>pca</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>635</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.212918</td>\n      <td>0.774682</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>pca</td>\n    </tr>\n    <tr>\n      <th>636</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.998865</td>\n      <td>0.001022</td>\n      <td>present</td>\n      <td>present</td>\n      <td>yes</td>\n      <td>pca</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.185018</td>\n      <td>0.822350</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>pca</td>\n    </tr>\n    <tr>\n      <th>638</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.189441</td>\n      <td>0.822144</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>pca</td>\n    </tr>\n    <tr>\n      <th>639</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.252489</td>\n      <td>0.741521</td>\n      <td>not_present</td>\n      <td>not_present</td>\n      <td>yes</td>\n      <td>pca</td>\n    </tr>\n  </tbody>\n</table>\n<p>640 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "presence_labs = df['drone_present']\n",
    "\n",
    "values = np.array(presence_labs)\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "x = df.drop(['behavior','signal','multiple','drone_present'],axis=1).values\n",
    "\n",
    "pca = decomposition.PCA(n_components=200)\n",
    "pca.fit(x)\n",
    "x = pca.transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "model_pca = keras.Sequential(name='test')\n",
    "model_pca.add(Dense(64, activation='relu', input_shape=(200,)))\n",
    "model_pca.add(Dense(32, activation='relu'))\n",
    "model_pca.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model_pca.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_pca.build(input_shape=(200,))\n",
    "model_pca.summary()\n",
    "\n",
    "fit_pca = model_pca.fit(x = x_train,\n",
    "        y = y_train, \n",
    "        epochs = 100,\n",
    "        batch_size = 32,\n",
    "        validation_split = .2,\n",
    "        verbose=True)\n",
    "\n",
    "prediction = model_pca.predict(x_test)\n",
    "prediction_df = pd.DataFrame(prediction, columns=['prob_present','prob_not_present']) \n",
    "\n",
    "y_test = pd.DataFrame(y_test,columns=['present','not_present'])\n",
    "\n",
    "y_pred = y_test >> bind_cols(prediction_df)\n",
    "\n",
    "y_pred >>= mutate(actual = case_when([y_pred.present == 1, 'present'],\n",
    "[y_pred.not_present == 1, 'not_present']),\n",
    "predicted = case_when([y_pred.prob_present > y_pred.prob_not_present, 'present'],\n",
    "[y_pred.prob_present < y_pred.prob_not_present, 'not_present']))\n",
    "\n",
    "y_pred >>= mutate(correct = case_when([y_pred.actual == y_pred.predicted, 'yes'],\n",
    "[y_pred.actual != y_pred.predicted, 'no']),\n",
    "feature = 'pca')\n",
    "y_pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('tfgpu': conda)",
   "language": "python",
   "name": "python_defaultSpec_1594743212694"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}